#! /bin/sh -e

# DP: Updates from the cell-4_3-branch up to 20100216

dir=
if [ $# -eq 3 -a "$2" = '-d' ]; then
    pdir="-d $3"
    dir="$3/"
elif [ $# -ne 1 ]; then
    echo >&2 "`basename $0`: script expects -patch|-unpatch as argument"
    exit 1
fi
case "$1" in
    -patch)
        patch $pdir -f --no-backup-if-mismatch -p0 < $0
        #cd ${dir}gcc && autoconf
        ;;
    -unpatch)
        patch $pdir -f --no-backup-if-mismatch -R -p0 < $0
        #rm ${dir}gcc/configure
        ;;
    *)
        echo >&2 "`basename $0`: script expects -patch|-unpatch as argument"
        exit 1
esac
exit 0

Index: gcc/targhooks.c
===================================================================
--- gcc/targhooks.c	(.../gcc-4_3-branch)	(revision 156795)
+++ gcc/targhooks.c	(.../cell-4_3-branch)	(revision 156810)
@@ -691,6 +691,22 @@
    return id;
 }
 
+const char *
+default_addr_space_name (int addrspace ATTRIBUTE_UNUSED)
+{
+  gcc_unreachable ();
+}
+
+rtx (* default_addr_space_conversion_rtl (int from ATTRIBUTE_UNUSED, int to ATTRIBUTE_UNUSED)) (rtx, rtx)
+{
+  gcc_unreachable ();
+}
+
+unsigned char default_addr_space_number (const tree ident ATTRIBUTE_UNUSED)
+{
+  gcc_unreachable ();
+}
+
 bool
 default_builtin_vector_alignment_reachable (const_tree type, bool is_packed)
 {
Index: gcc/targhooks.h
===================================================================
--- gcc/targhooks.h	(.../gcc-4_3-branch)	(revision 156795)
+++ gcc/targhooks.h	(.../cell-4_3-branch)	(revision 156810)
@@ -95,3 +95,6 @@
 extern bool default_handle_c_option (size_t, const char *, int);
 extern int default_reloc_rw_mask (void);
 extern tree default_mangle_decl_assembler_name (tree, tree);
+extern const char *default_addr_space_name (int);
+extern unsigned char default_addr_space_number (const tree);
+extern rtx (*default_addr_space_conversion_rtl (int, int)) (rtx, rtx);
Index: gcc/tree-pretty-print.c
===================================================================
--- gcc/tree-pretty-print.c	(.../gcc-4_3-branch)	(revision 156795)
+++ gcc/tree-pretty-print.c	(.../cell-4_3-branch)	(revision 156810)
@@ -35,6 +35,8 @@
 #include "tree-pass.h"
 #include "fixed-value.h"
 #include "value-prof.h"
+#include "target.h"
+#include "target-def.h"
 
 /* Local functions, macros and variables.  */
 static int op_prio (const_tree);
@@ -550,6 +552,13 @@
 	else if (quals & TYPE_QUAL_RESTRICT)
 	  pp_string (buffer, "restrict ");
 
+ 	if (TYPE_ADDR_SPACE (node))
+ 	  {
+ 	    const char *as = targetm.addr_space_name (TYPE_ADDR_SPACE (node));
+	    pp_string (buffer, as);
+	    pp_space (buffer);
+ 	  }
+
 	class = TREE_CODE_CLASS (TREE_CODE (node));
 
 	if (class == tcc_declaration)
@@ -626,6 +635,13 @@
 	  if (quals & TYPE_QUAL_RESTRICT)
 	    pp_string (buffer, " restrict");
 
+	  if (TYPE_ADDR_SPACE (node))
+	    {
+	      const char *as = targetm.addr_space_name (TYPE_ADDR_SPACE (node));
+	      pp_string (buffer, as);
+	      pp_space (buffer);
+	    }
+	  
 	  if (TYPE_REF_CAN_ALIAS_ALL (node))
 	    pp_string (buffer, " {ref-all}");
 	}
Index: gcc/tree.c
===================================================================
--- gcc/tree.c	(.../gcc-4_3-branch)	(revision 156795)
+++ gcc/tree.c	(.../cell-4_3-branch)	(revision 156810)
@@ -1415,8 +1415,13 @@
   if (TREE_CODE (expr) != INTEGER_CST)
     return 0;
 
-  prec = (POINTER_TYPE_P (TREE_TYPE (expr))
-	  ? POINTER_SIZE : TYPE_PRECISION (TREE_TYPE (expr)));
+  if (OTHER_ADDR_SPACE_POINTER_TYPE_P (TREE_TYPE (expr)))
+    prec = GET_MODE_BITSIZE (targetm.addr_space_pointer_mode (TYPE_ADDR_SPACE (TREE_TYPE (expr))));
+  else if (POINTER_TYPE_P (TREE_TYPE (expr)))
+    prec = POINTER_SIZE;
+  else
+    prec = TYPE_PRECISION (TREE_TYPE (expr));
+
   high = TREE_INT_CST_HIGH (expr);
   low = TREE_INT_CST_LOW (expr);
 
@@ -1480,8 +1485,12 @@
   if (TREE_CODE (expr) == COMPLEX_CST)
     return tree_log2 (TREE_REALPART (expr));
 
-  prec = (POINTER_TYPE_P (TREE_TYPE (expr))
-	  ? POINTER_SIZE : TYPE_PRECISION (TREE_TYPE (expr)));
+  if (OTHER_ADDR_SPACE_POINTER_TYPE_P (TREE_TYPE (expr)))
+    prec = GET_MODE_BITSIZE (targetm.addr_space_pointer_mode (TYPE_ADDR_SPACE (TREE_TYPE (expr))));
+  else if (POINTER_TYPE_P (TREE_TYPE (expr)))
+    prec = POINTER_SIZE;
+  else
+    prec = TYPE_PRECISION (TREE_TYPE (expr));
 
   high = TREE_INT_CST_HIGH (expr);
   low = TREE_INT_CST_LOW (expr);
@@ -1518,8 +1527,12 @@
   if (TREE_CODE (expr) == COMPLEX_CST)
     return tree_log2 (TREE_REALPART (expr));
 
-  prec = (POINTER_TYPE_P (TREE_TYPE (expr))
-	  ? POINTER_SIZE : TYPE_PRECISION (TREE_TYPE (expr)));
+  if (OTHER_ADDR_SPACE_POINTER_TYPE_P (TREE_TYPE (expr)))
+    prec = GET_MODE_BITSIZE (targetm.addr_space_pointer_mode (TYPE_ADDR_SPACE (TREE_TYPE (expr))));
+  else if (POINTER_TYPE_P (TREE_TYPE (expr)))
+    prec = POINTER_SIZE;
+  else
+    prec = TYPE_PRECISION (TREE_TYPE (expr));
 
   high = TREE_INT_CST_HIGH (expr);
   low = TREE_INT_CST_LOW (expr);
@@ -4172,6 +4185,7 @@
   TYPE_READONLY (type) = (type_quals & TYPE_QUAL_CONST) != 0;
   TYPE_VOLATILE (type) = (type_quals & TYPE_QUAL_VOLATILE) != 0;
   TYPE_RESTRICT (type) = (type_quals & TYPE_QUAL_RESTRICT) != 0;
+  TYPE_ADDR_SPACE (type) = DECODE_QUAL_ADDR_SPACE (type_quals);
 }
 
 /* Returns true iff CAND is equivalent to BASE with TYPE_QUALS.  */
@@ -4179,7 +4193,7 @@
 bool
 check_qualified_type (const_tree cand, const_tree base, int type_quals)
 {
-  return (TYPE_QUALS (cand) == type_quals
+  return (TYPE_QUALS (CONST_CAST_TREE (cand)) == type_quals
 	  && TYPE_NAME (cand) == TYPE_NAME (base)
 	  /* Apparently this is needed for Objective-C.  */
 	  && TYPE_CONTEXT (cand) == TYPE_CONTEXT (base)
@@ -5478,7 +5492,9 @@
 tree
 build_pointer_type (tree to_type)
 {
-  return build_pointer_type_for_mode (to_type, ptr_mode, false);
+  enum machine_mode mode = targetm.addr_space_pointer_mode
+    (TYPE_ADDR_SPACE (strip_array_types (to_type)));
+  return build_pointer_type_for_mode (to_type, mode, false);
 }
 
 /* Same as build_pointer_type_for_mode, but for REFERENCE_TYPE.  */
@@ -5814,6 +5830,18 @@
   return argtypes;
 }
 
+/* Recursively examines the array elements of TYPE, until a non-array
+   element type is found.  */
+
+tree
+strip_array_types (tree type)
+{
+  while (TREE_CODE (type) == ARRAY_TYPE)
+    type = TREE_TYPE (type);
+
+  return type;
+}
+
 /* Construct, lay out and return
    the type of functions returning type VALUE_TYPE
    given arguments of types ARG_TYPES.
Index: gcc/tree.h
===================================================================
--- gcc/tree.h	(.../gcc-4_3-branch)	(revision 156795)
+++ gcc/tree.h	(.../cell-4_3-branch)	(revision 156810)
@@ -1115,6 +1115,17 @@
 #define POINTER_TYPE_P(TYPE) \
   (TREE_CODE (TYPE) == POINTER_TYPE || TREE_CODE (TYPE) == REFERENCE_TYPE)
 
+/* Nonzero if TYPE is a pointer or reference type qualified as
+   belonging to an address space that is not the generic address
+   space.  */
+#define OTHER_ADDR_SPACE_POINTER_TYPE_P(TYPE) \
+  (POINTER_TYPE_P (TYPE) && TYPE_ADDR_SPACE (strip_array_types (TREE_TYPE (TYPE))))
+
+/* Nonzero if TYPE is a pointer or reference type, but does not belong
+   to an address space outside the generic address space.  */
+#define GENERIC_ADDR_SPACE_POINTER_TYPE_P(TYPE) \
+  (POINTER_TYPE_P (TYPE) && !TYPE_ADDR_SPACE (strip_array_types (TREE_TYPE (TYPE))))
+
 /* Nonzero if this type is a complete type.  */
 #define COMPLETE_TYPE_P(NODE) (TYPE_SIZE (NODE) != NULL_TREE)
 
@@ -2222,6 +2233,9 @@
    the term.  */
 #define TYPE_RESTRICT(NODE) (TYPE_CHECK (NODE)->type.restrict_flag)
 
+/* If nonzero, this type is in the extended address space.  */
+#define TYPE_ADDR_SPACE(NODE) (TYPE_CHECK (NODE)->type.address_space)
+
 /* There is a TYPE_QUAL value for each type qualifier.  They can be
    combined by bitwise-or to form the complete set of qualifiers for a
    type.  */
@@ -2231,11 +2245,15 @@
 #define TYPE_QUAL_VOLATILE 0x2
 #define TYPE_QUAL_RESTRICT 0x4
 
+#define ENCODE_QUAL_ADDR_SPACE(NUM) ((NUM & 0xFF) << 8)
+#define DECODE_QUAL_ADDR_SPACE(X) (((X) >> 8) && 0xFF)
+
 /* The set of type qualifiers for this type.  */
 #define TYPE_QUALS(NODE)					\
   ((TYPE_READONLY (NODE) * TYPE_QUAL_CONST)			\
    | (TYPE_VOLATILE (NODE) * TYPE_QUAL_VOLATILE)		\
-   | (TYPE_RESTRICT (NODE) * TYPE_QUAL_RESTRICT))
+   | (TYPE_RESTRICT (NODE) * TYPE_QUAL_RESTRICT)		\
+   | (ENCODE_QUAL_ADDR_SPACE (TYPE_ADDR_SPACE (strip_array_types (NODE)))))
 
 /* These flags are available for each language front end to use internally.  */
 #define TYPE_LANG_FLAG_0(NODE) (TYPE_CHECK (NODE)->type.lang_flag_0)
@@ -2327,6 +2345,8 @@
   unsigned user_align : 1;
 
   unsigned int align;
+  unsigned char address_space;
+
   tree pointer_to;
   tree reference_to;
   union tree_type_symtab {
@@ -4062,6 +4082,7 @@
 extern bool tree_expr_nonnegative_warnv_p (tree, bool *);
 extern bool may_negate_without_overflow_p (const_tree);
 extern tree get_inner_array_type (const_tree);
+extern tree strip_array_types (tree);
 
 /* Construct various nodes representing fract or accum data types.  */
 
Index: gcc/tree-pass.h
===================================================================
--- gcc/tree-pass.h	(.../gcc-4_3-branch)	(revision 156795)
+++ gcc/tree-pass.h	(.../cell-4_3-branch)	(revision 156810)
@@ -385,6 +385,7 @@
 extern struct tree_opt_pass pass_rtl_loop_done;
 
 extern struct tree_opt_pass pass_web;
+extern struct tree_opt_pass pass_split_before_cse2;
 extern struct tree_opt_pass pass_cse2;
 extern struct tree_opt_pass pass_df_initialize_opt;
 extern struct tree_opt_pass pass_df_initialize_no_opt;
Index: gcc/target.h
===================================================================
--- gcc/target.h	(.../gcc-4_3-branch)	(revision 156795)
+++ gcc/target.h	(.../cell-4_3-branch)	(revision 156810)
@@ -627,6 +627,21 @@
   /* True if MODE is valid for a pointer in __attribute__((mode("MODE"))).  */
   bool (* valid_pointer_mode) (enum machine_mode mode);
 
+  /* MODE to use for a pointer into another address space.  */
+  enum machine_mode (* addr_space_pointer_mode) (int);
+
+  /* Function to map an address space to a descriptive string.  */
+  const char * (* addr_space_name) (int);
+
+  /* Function to map an address space to a descriptive string.  */
+  unsigned char (* addr_space_number) (const tree);
+
+  /* Function to return a gen function for the pointer conversion.  */
+  rtx (* (* addr_space_conversion_rtl) (int, int)) (rtx, rtx);
+
+  /* True if an identifier that is a valid address space.  */
+  bool (* valid_addr_space) (const_tree);
+
   /* True if MODE is valid for the target.  By "valid", we mean able to
      be manipulated in non-trivial ways.  In particular, this means all
      the arithmetic is supported.  */
Index: gcc/rtlanal.c
===================================================================
--- gcc/rtlanal.c	(.../gcc-4_3-branch)	(revision 156795)
+++ gcc/rtlanal.c	(.../cell-4_3-branch)	(revision 156810)
@@ -266,6 +266,10 @@
 rtx_addr_can_trap_p_1 (const_rtx x, HOST_WIDE_INT offset, HOST_WIDE_INT size,
 		       enum machine_mode mode, bool unaligned_mems)
 {
+#ifdef ADDRESSES_NEVER_TRAP
+  /* On some processors, like the SPU, memory accesses never trap.  */
+  return 0;
+#else
   enum rtx_code code = GET_CODE (x);
 
   if (STRICT_ALIGNMENT
@@ -382,6 +386,7 @@
 
   /* If it isn't one of the case above, it can cause a trap.  */
   return 1;
+#endif
 }
 
 /* Return nonzero if the use of X as an address in a MEM can cause a trap.  */
Index: gcc/final.c
===================================================================
--- gcc/final.c	(.../gcc-4_3-branch)	(revision 156795)
+++ gcc/final.c	(.../cell-4_3-branch)	(revision 156810)
@@ -4242,6 +4242,7 @@
 #ifdef STACK_REGS
   regstack_completed = 0;
 #endif
+  split0_completed = 0;
 
   /* Clear out the insn_length contents now that they are no
      longer valid.  */
Index: gcc/builtins.c
===================================================================
--- gcc/builtins.c	(.../gcc-4_3-branch)	(revision 156795)
+++ gcc/builtins.c	(.../cell-4_3-branch)	(revision 156810)
@@ -5153,17 +5153,35 @@
 expand_builtin_expect (tree exp, rtx target)
 {
   tree arg, c;
+  rtx new_target;
 
   if (call_expr_nargs (exp) < 2)
     return const0_rtx;
   arg = CALL_EXPR_ARG (exp, 0);
   c = CALL_EXPR_ARG (exp, 1);
 
-  target = expand_expr (arg, target, VOIDmode, EXPAND_NORMAL);
+  new_target = expand_expr (arg, target, VOIDmode, EXPAND_NORMAL);
+
+#ifdef HAVE_builtin_expect
+  if (HAVE_builtin_expect)
+    {
+      int icode = CODE_FOR_builtin_expect;
+      enum machine_mode mode1 = insn_data[icode].operand[1].mode;
+      rtx op1 = expand_expr (c, NULL_RTX, VOIDmode, EXPAND_NORMAL);
+      if (GET_MODE (op1) != mode1 && mode1 != VOIDmode
+	  && GET_MODE (op1) != VOIDmode)
+	op1 = convert_modes (mode1, GET_MODE (op1), op1, 0);
+      if (!insn_data[icode].operand[1].predicate (op1, mode1)
+	  && mode1 != VOIDmode)
+	op1 = copy_to_mode_reg (mode1, op1);
+      emit_insn (gen_builtin_expect (target, op1));
+    }
+#else
   /* When guessing was done, the hints should be already stripped away.  */
   gcc_assert (!flag_guess_branch_prob
 	      || optimize == 0 || errorcount || sorrycount);
-  return target;
+#endif
+  return new_target;
 }
 
 void
Index: gcc/fold-const.c
===================================================================
--- gcc/fold-const.c	(.../gcc-4_3-branch)	(revision 156795)
+++ gcc/fold-const.c	(.../cell-4_3-branch)	(revision 156810)
@@ -64,6 +64,7 @@
 #include "hashtab.h"
 #include "langhooks.h"
 #include "md5.h"
+#include "target.h"
 
 /* Nonzero if we are folding constants inside an initializer; zero
    otherwise.  */
@@ -205,8 +206,10 @@
   unsigned int prec;
   int sign_extended_type;
 
-  if (POINTER_TYPE_P (type)
-      || TREE_CODE (type) == OFFSET_TYPE)
+  if (OTHER_ADDR_SPACE_POINTER_TYPE_P (type))
+    prec = GET_MODE_BITSIZE (targetm.addr_space_pointer_mode (TYPE_ADDR_SPACE (type)));
+  else if (POINTER_TYPE_P (type)
+	   || TREE_CODE (type) == OFFSET_TYPE)
     prec = POINTER_SIZE;
   else
     prec = TYPE_PRECISION (type);
@@ -2399,7 +2402,9 @@
   if (TREE_TYPE (arg1) == type)
     return arg1;
 
-  if (POINTER_TYPE_P (type) || INTEGRAL_TYPE_P (type))
+  if (OTHER_ADDR_SPACE_POINTER_TYPE_P (type))
+    return NULL_TREE;
+  else if (POINTER_TYPE_P (type) || INTEGRAL_TYPE_P (type))
     {
       if (TREE_CODE (arg1) == INTEGER_CST)
 	return fold_convert_const_int_from_int (type, arg1);
Index: gcc/testsuite/gcc.c-torture/execute/vector-subscript-2.c
===================================================================
--- gcc/testsuite/gcc.c-torture/execute/vector-subscript-2.c	(.../gcc-4_3-branch)	(revision 0)
+++ gcc/testsuite/gcc.c-torture/execute/vector-subscript-2.c	(.../cell-4_3-branch)	(revision 156810)
@@ -0,0 +1,66 @@
+#define vector __attribute__((vector_size(sizeof(int)*4) ))
+
+/* Check to make sure that we extract and insert the vector at the same
+   location for vector subscripting (with constant indexes) and
+   that vectors layout are the same as arrays. */
+
+struct TV4
+{
+    vector int v;
+};
+
+typedef struct TV4 MYV4;
+
+static inline MYV4 myfunc2( int x, int y, int z, int w )
+{
+    MYV4 temp;
+    temp.v[0] = x;
+    temp.v[1] = y;
+    temp.v[2] = z;
+    temp.v[3] = w;
+    return temp;
+}
+MYV4 val3;
+__attribute__((noinline)) void modify (void) 
+{
+    val3 = myfunc2( 1, 2, 3, 4 );
+}
+int main( int argc, char* argv[] )
+{
+  int a[4];
+  int i;
+  
+  /* Set up the vector.  */
+  modify();
+  
+  /* Check the vector via the global variable.  */
+  if (val3.v[0] != 1)
+    __builtin_abort ();
+  if (val3.v[1] != 2)
+    __builtin_abort ();
+  if (val3.v[2] != 3)
+    __builtin_abort ();
+  if (val3.v[3] != 4)
+    __builtin_abort ();
+    
+  vector int a1 = val3.v;
+  
+   /* Check the vector via a local variable.  */
+  if (a1[0] != 1)
+    __builtin_abort ();
+  if (a1[1] != 2)
+    __builtin_abort ();
+  if (a1[2] != 3)
+    __builtin_abort ();
+  if (a1[3] != 4)
+    __builtin_abort ();
+    
+  __builtin_memcpy(a, &val3, sizeof(a));  
+   /* Check the vector via copying it to an array.  */
+  for(i = 0; i < 4; i++)
+    if (a[i] != i+1)
+      __builtin_abort ();
+  
+  
+  return 0;
+}
Index: gcc/testsuite/gcc.c-torture/execute/vector-subscript-1.c
===================================================================
--- gcc/testsuite/gcc.c-torture/execute/vector-subscript-1.c	(.../gcc-4_3-branch)	(revision 0)
+++ gcc/testsuite/gcc.c-torture/execute/vector-subscript-1.c	(.../cell-4_3-branch)	(revision 156810)
@@ -0,0 +1,58 @@
+#define vector __attribute__((vector_size(sizeof(int)*4) ))
+
+/* Check to make sure that we extract and insert the vector at the same
+   location for vector subscripting and that vectors layout are the same
+   as arrays. */
+
+struct TV4
+{
+    vector int v;
+};
+
+typedef struct TV4 MYV4;
+static inline int *f(MYV4 *a, int i)
+{
+  return &(a->v[i]);
+}
+
+static inline MYV4 myfunc2( int x, int y, int z, int w )
+{
+    MYV4 temp;
+    *f(&temp, 0 ) = x;
+    *f(&temp, 1 ) = y;
+    *f(&temp, 2 ) = z;
+    *f(&temp, 3 ) = w;
+    return temp;
+}
+
+MYV4 val3;
+
+__attribute__((noinline)) void modify (void) 
+{
+    val3 = myfunc2( 1, 2, 3, 4 );
+}
+
+int main( int argc, char* argv[] )
+{
+  int a[4];
+  int i;
+  
+  modify();
+  
+  if (*f(&val3, 0 ) != 1)
+    __builtin_abort ();
+  if (*f(&val3, 1 ) != 2)
+    __builtin_abort ();
+  if (*f(&val3, 2 ) != 3)
+    __builtin_abort ();
+  if (*f(&val3, 3 ) != 4)
+    __builtin_abort ();
+    
+  __builtin_memcpy(a, &val3, 16);
+  for(i = 0; i < 4; i++)
+    if (a[i] != i+1)
+      __builtin_abort ();
+  
+  
+  return 0;
+}
Index: gcc/testsuite/gcc.target/powerpc/altivec-macros.c
===================================================================
--- gcc/testsuite/gcc.target/powerpc/altivec-macros.c	(.../gcc-4_3-branch)	(revision 0)
+++ gcc/testsuite/gcc.target/powerpc/altivec-macros.c	(.../cell-4_3-branch)	(revision 156810)
@@ -0,0 +1,63 @@
+/* Copyright (C) 2007 Free Software Foundation, Inc.  */
+
+/* { dg-do preprocess } */
+/* { dg-options "-maltivec" } */
+
+/* Conditional macros should not be expanded by pragmas.  */
+#pragma __vector
+_Pragma ("__vector")
+
+/* Redefinition of conditional macros.  */
+/* No warning should be generated.  */
+
+#define __vector __new_vector
+#define __pixel __new_pixel
+#define __bool __new_bool
+#define vector new_vector
+#define pixel new_pixel
+#define bool new_bool
+
+/* Definition of conditional macros.  */
+/* No warning should be generated.  */
+
+#undef __vector
+#define __vector __new_vector
+
+#undef __pixel
+#define __pixel __new_pixel
+
+#undef __bool
+#define __bool __new_bool
+
+#undef vector
+#define vector new_vector
+
+#undef pixel
+#define pixel new_pixel
+
+#undef bool
+#define bool new_bool
+
+/* Re-definition of "unconditional" macros.  */
+/* Warnings should be generated as usual.  */
+
+#define __vector	__newer_vector
+#define __pixel		__newer_pixel
+#define __bool		__newer_bool
+#define vector		newer_vector
+#define pixel		newer_pixel
+#define bool		newer_bool
+
+/* { dg-warning "redefined" "__vector redefined"  { target *-*-* } 44 } */
+/* { dg-warning "redefined" "__pixel redefined"   { target *-*-* } 45 } */
+/* { dg-warning "redefined" "__bool redefined"    { target *-*-* } 46 } */
+/* { dg-warning "redefined" "vector redefined"    { target *-*-* } 47 } */
+/* { dg-warning "redefined" "pixel redefined"     { target *-*-* } 48 } */
+/* { dg-warning "redefined" "bool redefined"      { target *-*-* } 49 } */
+
+/* { dg-warning "previous"  "prev __vector defn"  { target *-*-* } 24 } */
+/* { dg-warning "previous"  "prev __pixel defn"   { target *-*-* } 27 } */
+/* { dg-warning "previous"  "prev __bool defn"    { target *-*-* } 30 } */
+/* { dg-warning "previous"  "prev vector defn"    { target *-*-* } 33 } */
+/* { dg-warning "previous"  "prev pixel defn"     { target *-*-* } 36 } */
+/* { dg-warning "previous"  "prev bool defn"      { target *-*-* } 39 } */
Index: gcc/testsuite/gcc.target/powerpc/altivec-26.c
===================================================================
--- gcc/testsuite/gcc.target/powerpc/altivec-26.c	(.../gcc-4_3-branch)	(revision 0)
+++ gcc/testsuite/gcc.target/powerpc/altivec-26.c	(.../cell-4_3-branch)	(revision 156810)
@@ -0,0 +1,11 @@
+/* { dg-do compile { target powerpc*-*-* } } */
+/* { dg-require-effective-target powerpc_altivec_ok } */
+/* { dg-options "-maltivec" } */
+
+/* A compiler implementing context-sensitive keywords must define this
+   preprocessor macro so that altivec.h does not provide the vector,
+   pixel, etc. macros.  */
+
+#ifndef __APPLE_ALTIVEC__
+#error __APPLE_ALTIVEC__ not pre-defined
+#endif
Index: gcc/testsuite/gcc.target/spu/vector.c
===================================================================
--- gcc/testsuite/gcc.target/spu/vector.c	(.../gcc-4_3-branch)	(revision 0)
+++ gcc/testsuite/gcc.target/spu/vector.c	(.../cell-4_3-branch)	(revision 156810)
@@ -0,0 +1,32 @@
+/* { dg-do compile } */
+/* { dg-options "" } */
+
+#ifndef __VECTOR_KEYWORD_SUPPORTED__
+#error __VECTOR_KEYWORD_SUPPORTED__ is not defined
+#endif
+
+/* __vector is expanded unconditionally.  */
+__vector int vi;
+__vector unsigned char vuc;
+__vector signed char vsc;
+__vector unsigned short vus;
+__vector signed short vss;
+__vector unsigned int vui;
+__vector signed int vsi;
+__vector unsigned long long ull;
+__vector signed long long sll;
+__vector float vf;
+__vector double vd;
+
+/* vector is expanded conditionally, based on the context.  */
+vector int vi;
+vector unsigned char vuc;
+vector signed char vsc;
+vector unsigned short vus;
+vector signed short vss;
+vector unsigned int vui;
+vector signed int vsi;
+vector unsigned long long ull;
+vector signed long long sll;
+vector float vf;
+vector double vd;
Index: gcc/testsuite/gcc.target/spu/cache.c
===================================================================
--- gcc/testsuite/gcc.target/spu/cache.c	(.../gcc-4_3-branch)	(revision 0)
+++ gcc/testsuite/gcc.target/spu/cache.c	(.../cell-4_3-branch)	(revision 156810)
@@ -0,0 +1,220 @@
+/* Copyright (C) 2008 Free Software Foundation, Inc.
+
+   This file is free software; you can redistribute it and/or modify it under
+   the terms of the GNU General Public License as published by the Free
+   Software Foundation; either version 2 of the License, or (at your option)
+   any later version.
+
+   This file is distributed in the hope that it will be useful, but WITHOUT
+   ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+   FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
+   for more details.
+
+   You should have received a copy of the GNU General Public License
+   along with this file; see the file COPYING.  If not, write to the Free
+   Software Foundation, 51 Franklin Street, Fifth Floor, Boston, MA
+   02110-1301, USA.  */
+
+/* { dg-do run } */
+/* { dg-options "-mcache-size=8" } */
+
+#include <stdlib.h>
+#include <string.h>
+#include <spu_cache.h>
+extern void *malloc (__SIZE_TYPE__);
+extern void *memset (void *, int, __SIZE_TYPE__);
+extern void abort (void);
+
+#ifdef __EA64__
+#define addr unsigned long long
+#else
+#define addr unsigned long
+#endif
+
+#ifdef __EA64__
+#define malloc_ea __malloc_ea64
+#define memset_ea __memset_ea64
+#define memcpy_ea __memcpy_ea64
+
+typedef unsigned long long size_ea_t;
+
+__ea void *__malloc_ea64 (size_ea_t);
+__ea void *__memset_ea64 (__ea void *, int, size_ea_t);
+__ea void *__memcpy_ea64 (__ea void *, __ea const void *, size_ea_t);
+#else
+#define malloc_ea __malloc_ea32
+#define memset_ea __memset_ea32
+#define memcpy_ea __memcpy_ea32
+
+typedef unsigned long size_ea_t;
+
+__ea void *__malloc_ea32 (size_ea_t size);
+__ea void *__memset_ea32 (__ea void *, int, size_ea_t);
+__ea void *__memcpy_ea32 (__ea void *, __ea const void *, size_ea_t);
+#endif
+
+static __ea void *bigblock;
+static __ea void *block;
+static int *ls_block;
+
+extern void __cache_tag_array_size;
+#define CACHE_SIZE (4 * (int) &__cache_tag_array_size)
+#define LINE_SIZE 128
+
+void
+init_mem ()
+{
+  bigblock = malloc_ea (CACHE_SIZE + 2 * LINE_SIZE);
+  block = malloc_ea (2 * LINE_SIZE);
+  ls_block = malloc (LINE_SIZE);
+
+  memset_ea (bigblock, 0, CACHE_SIZE + 2 * LINE_SIZE);
+  memset_ea (block, -1, 2 * LINE_SIZE);
+  memset (ls_block, -1, LINE_SIZE);
+  cache_flush ();
+}
+
+/* Test 1: Simple cache fetching.  */
+void
+test1 ()
+{
+  addr aligned = ((((addr) block) + LINE_SIZE - 1) & -LINE_SIZE);
+  int *p1 = NULL;
+  int *p2 = NULL;
+  int i = 0;
+
+  /* First, check if the same addr give the same cache ptr.  */
+  p1 = cache_fetch ((__ea void *) aligned);
+  p2 = cache_fetch ((__ea void *) aligned);
+
+  if (p1 != p2)
+    abort ();
+
+  /* Check that the data actually is in the cache. */
+  for (i = 0; i < LINE_SIZE / sizeof (int); i++)
+    {
+      if (p1[i] != -1)
+	abort ();
+    }
+
+  /* Check returning within the cache line. */
+  p2 = cache_fetch ((__ea void *) (aligned + sizeof (int)));
+
+  if (p2 - p1 != 1)
+    abort ();
+
+  /* Finally, check that fetching an LS pointer returns that pointer.  */
+  p1 = cache_fetch ((__ea char *) ls_block);
+  if (p1 != ls_block)
+    abort ();
+}
+
+/* Test 2: Eviction testing. */
+void
+test2 ()
+{
+  addr aligned = ((((addr) block) + LINE_SIZE - 1) & -LINE_SIZE);
+  int *p = NULL;
+  int i = 0;
+
+  /* First check that clean evictions don't write back.  */
+  p = cache_fetch ((__ea void *) aligned);
+  for (i = 0; i < LINE_SIZE / sizeof (int); i++)
+    p[i] = 0;
+
+  cache_evict ((__ea void *) aligned);
+  memcpy_ea ((__ea char *) ls_block, (__ea void *) aligned, LINE_SIZE);
+
+  for (i = 0; i < LINE_SIZE / sizeof (int); i++)
+    {
+      if (ls_block[i] == 0)
+	abort ();
+    }
+
+  /* Now check that dirty evictions do write back.  */
+  p = cache_fetch_dirty ((__ea void *) aligned, LINE_SIZE);
+  for (i = 0; i < LINE_SIZE / sizeof (int); i++)
+    p[i] = 0;
+
+  cache_evict ((__ea void *) aligned);
+  memcpy_ea ((__ea char *) ls_block, (__ea void *) aligned, LINE_SIZE);
+
+  for (i = 0; i < LINE_SIZE / sizeof (int); i++)
+    {
+      if (ls_block[i] != 0)
+	abort ();
+    }
+
+  /* Finally, check that non-atomic writeback only writes dirty bytes.  */
+
+  for (i = 0; i < LINE_SIZE / sizeof (int); i++)
+    {
+      p = cache_fetch_dirty ((__ea void *) (aligned + i * sizeof (int)),
+			     (i % 2) * sizeof (int));
+      p[0] = -1;
+    }
+
+  cache_evict ((__ea void *) aligned);
+  memcpy_ea ((__ea char *) ls_block, (__ea void *) aligned, LINE_SIZE);
+
+  for (i = 0; i < LINE_SIZE / sizeof (int); i++)
+    {
+      if ((ls_block[i] == -1) && (i % 2 == 0))
+	abort ();
+      if ((ls_block[i] == 0) && (i % 2 == 1))
+	abort ();
+    }
+}
+
+/* Test LS forced-eviction. */
+void
+test3 ()
+{
+  addr aligned = ((((addr) bigblock) + LINE_SIZE - 1) & -LINE_SIZE);
+  char *test = NULL;
+  char *ls = NULL;
+  int i = 0;
+
+  /* Init memory, fill the cache to capacity.  */
+  ls = cache_fetch_dirty ((__ea void *) aligned, LINE_SIZE);
+  for (i = 1; i < (CACHE_SIZE / LINE_SIZE); i++)
+    cache_fetch_dirty ((__ea void *) (aligned + i * LINE_SIZE), LINE_SIZE);
+
+  memset (ls, -1, LINE_SIZE);
+  test = cache_fetch ((__ea void *) (aligned + CACHE_SIZE));
+
+  /* test == ls indicates cache collision.  */
+  if (test != ls)
+    abort ();
+
+  /* Make sure it actually wrote the cache line.  */
+  for (i = 0; i < LINE_SIZE; i++)
+    {
+      if (ls[i] != 0)
+	abort ();
+    }
+
+  ls = cache_fetch ((__ea void *) aligned);
+
+  /* test != ls indicates another entry was evicted.  */
+  if (test == ls)
+    abort ();
+
+  /* Make sure that the previous eviction actually wrote back.  */
+  for (i = 0; i < LINE_SIZE; i++)
+    {
+      if (ls[i] != 0xFF)
+	abort ();
+    }
+}
+
+int
+main (int argc, char **argv)
+{
+  init_mem ();
+  test1 ();
+  test2 ();
+  test3 ();
+
+  return 0;
+}
Index: gcc/testsuite/gcc.target/spu/ea/ea.exp
===================================================================
--- gcc/testsuite/gcc.target/spu/ea/ea.exp	(.../gcc-4_3-branch)	(revision 0)
+++ gcc/testsuite/gcc.target/spu/ea/ea.exp	(.../cell-4_3-branch)	(revision 156810)
@@ -0,0 +1,41 @@
+#   Copyright (C) 2007 Free Software Foundation, Inc.
+
+# This program is free software; you can redistribute it and/or modify
+# it under the terms of the GNU General Public License as published by
+# the Free Software Foundation; either version 3 of the License, or
+# (at your option) any later version.
+# 
+# This program is distributed in the hope that it will be useful,
+# but WITHOUT ANY WARRANTY; without even the implied warranty of
+# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+# GNU General Public License for more details.
+# 
+# You should have received a copy of the GNU General Public License
+# along with GCC; see the file COPYING3.  If not see
+# <http://www.gnu.org/licenses/>.
+
+# GCC testsuite that uses the `dg.exp' driver.
+
+# Load support procs.
+load_lib gcc-dg.exp
+
+# Exit immediately if this isn't a SPU target.
+if { ![istarget spu-*-*] } then {
+  return
+}
+
+# If a testcase doesn't have special options, use these.
+global DEFAULT_CFLAGS
+if ![info exists DEFAULT_CFLAGS] then {
+    set DEFAULT_CFLAGS "-std=gnu89 -pedantic-errors"
+}
+
+# Initialize `dg'.
+dg-init
+
+# Main loop.
+dg-runtest [lsort [glob -nocomplain $srcdir/$subdir/*.\[cS\]]] \
+        "" $DEFAULT_CFLAGS
+
+# All done.
+dg-finish
Index: gcc/testsuite/gcc.target/spu/ea/cppdefine32.c
===================================================================
--- gcc/testsuite/gcc.target/spu/ea/cppdefine32.c	(.../gcc-4_3-branch)	(revision 0)
+++ gcc/testsuite/gcc.target/spu/ea/cppdefine32.c	(.../cell-4_3-branch)	(revision 156810)
@@ -0,0 +1,9 @@
+/* Test default __EA32__ define.  */
+/* { dg-options "-std=gnu89 -pedantic-errors -mea32" } */
+/* { dg-do compile } */
+
+#ifdef __EA32__
+int x;
+#else
+#error __EA32__ undefined
+#endif
Index: gcc/testsuite/gcc.target/spu/ea/cppdefine64.c
===================================================================
--- gcc/testsuite/gcc.target/spu/ea/cppdefine64.c	(.../gcc-4_3-branch)	(revision 0)
+++ gcc/testsuite/gcc.target/spu/ea/cppdefine64.c	(.../cell-4_3-branch)	(revision 156810)
@@ -0,0 +1,8 @@
+/* { dg-options "-std=gnu89 -mea64" } */
+/* { dg-do compile } */
+
+#ifdef __EA64__
+int x;
+#else
+#error __EA64__ undefined
+#endif
Index: gcc/testsuite/gcc.target/spu/ea/compile.c
===================================================================
--- gcc/testsuite/gcc.target/spu/ea/compile.c	(.../gcc-4_3-branch)	(revision 0)
+++ gcc/testsuite/gcc.target/spu/ea/compile.c	(.../cell-4_3-branch)	(revision 156810)
@@ -0,0 +1,85 @@
+/* Valid __ea declarations.  */
+/* { dg-do compile } */
+/* { dg-options "-std=gnu99 -pedantic-errors" } */
+
+/* Externs.  */
+
+__ea extern int i1;
+extern __ea int i2;
+extern int __ea i3;
+extern int __ea *ppu;
+
+/* Pointers.  */
+__ea int *i4p;
+
+/* Typedefs.  */
+typedef __ea int ea_int_t;
+typedef __ea int *ea_int_star_t;
+
+/* Structs.  */
+struct st {
+  __ea int *p;
+};
+
+/* Variable definitions.  */
+__ea int ii0;
+int *__ea ii1;
+static int __ea ii2;
+
+void
+f1 ()
+{
+  int *spu;
+  ppu = (ea_int_t *) spu;
+  ppu = (ea_int_star_t) spu;
+}
+
+void
+f2 ()
+{
+  int *spu;
+  spu = (int *) ppu;
+  ppu = (__ea int *) spu;
+}
+
+void
+f3 ()
+{
+  int i = sizeof (__ea int);
+}
+
+__ea int *f4 (void)
+{
+  return 0;
+}
+
+int f5 (__ea int *parm)
+{
+  static __ea int local4;
+  int tmp = local4;
+  local4 = *parm;
+  return tmp;
+}
+
+static inline __ea void *f6 (__ea void *start)
+{
+  return 0;
+}
+
+void f7 (void)
+{
+  __ea void *s1;
+  auto __ea void *s2;
+}
+
+__ea int *f8 (__ea int *x)
+{
+  register __ea int *y = x;
+  __ea int *z = y;
+  return z;
+}
+
+long long f9 (__ea long long x[2])
+{
+  return x[0] + x[1];
+}
Index: gcc/testsuite/gcc.target/spu/ea/cast1.c
===================================================================
--- gcc/testsuite/gcc.target/spu/ea/cast1.c	(.../gcc-4_3-branch)	(revision 0)
+++ gcc/testsuite/gcc.target/spu/ea/cast1.c	(.../cell-4_3-branch)	(revision 156810)
@@ -0,0 +1,23 @@
+/* { dg-do run { target spu-*-* } } */
+/* { dg-options "-std=gnu99" } */
+
+extern void abort (void);
+extern unsigned long long __ea_local_store;
+
+__ea int *ppu;
+int x, *spu = &x, *spu2;
+
+int
+main (int argc, char **argv)
+{
+  ppu = (__ea int *) spu;
+  spu2 = (int *) ppu;
+
+  if ((int) ppu != (int) __ea_local_store + (int) spu)
+    abort ();
+
+  if (spu != spu2)
+    abort ();
+
+  return 0;
+}
Index: gcc/testsuite/gcc.target/spu/ea/errors.c
===================================================================
--- gcc/testsuite/gcc.target/spu/ea/errors.c	(.../gcc-4_3-branch)	(revision 0)
+++ gcc/testsuite/gcc.target/spu/ea/errors.c	(.../cell-4_3-branch)	(revision 156810)
@@ -0,0 +1,38 @@
+/* Invalid __ea declarations.  */
+/* { dg-do compile } */
+/* { dg-options "-std=gnu99 -pedantic-errors" } */
+
+extern __ea void f1 ();	 /* { dg-error "'__ea' specified for function 'f1'" } */
+
+void func ()
+{
+  register __ea int local1; /* { dg-error "'__ea' combined with 'register' qualifier for 'local1'" } */
+  auto __ea int local2;     /* { dg-error "'__ea' combined with 'auto' qualifier for 'local2'" } */
+  __ea int local3;	    /* { dg-error "'__ea' specified for auto variable 'local3'" } */
+  register int *__ea p1;    /* { dg-error "'__ea' combined with 'register' qualifier for 'p1'" } */
+  auto char *__ea p2;       /* { dg-error "'__ea' combined with 'auto' qualifier for 'p2'" } */
+  void *__ea p3;            /* { dg-error "'__ea' specified for auto variable 'p3'" } */
+  register __ea int a1[2];  /* { dg-error "'__ea' combined with 'register' qualifier for 'a1'" } */
+  auto __ea char a2[1];     /* { dg-error "'__ea' combined with 'auto' qualifier for 'a2'" } */
+  __ea char a3[5];          /* { dg-error "'__ea' specified for auto variable 'a3'" } */
+}
+
+void func2 (__ea int x)	    /* { dg-error "'__ea' specified for parameter 'x'" } */
+{ }
+
+struct st {
+  __ea int x;		    /* { dg-error "'__ea' specified for structure field 'x'" } */
+  int *__ea q;		    /* { dg-error "'__ea' specified for structure field 'q'" } */
+} s;
+
+__ea int func3 (int x) {    /* { dg-error "'__ea' specified for function 'func3'" } */
+  return x;
+}
+
+struct A { int a; };
+
+int func4 (int *__ea x)	    /* { dg-error "'__ea' specified for parameter 'x'" } */
+{
+  struct A i = (__ea struct A) { 1 };	/* { dg-error "compound literal qualified by address-space qualifier" } */
+  return i.a;
+}
Index: gcc/testsuite/gcc.target/spu/ea/options1.c
===================================================================
--- gcc/testsuite/gcc.target/spu/ea/options1.c	(.../gcc-4_3-branch)	(revision 0)
+++ gcc/testsuite/gcc.target/spu/ea/options1.c	(.../cell-4_3-branch)	(revision 156810)
@@ -0,0 +1,5 @@
+/* Test -mcache-size.  */
+/* { dg-options "-mcache-size=128" } */
+/* { dg-do compile } */
+
+int x;
Index: gcc/testsuite/gcc.target/spu/split0-1.c
===================================================================
--- gcc/testsuite/gcc.target/spu/split0-1.c	(.../gcc-4_3-branch)	(revision 0)
+++ gcc/testsuite/gcc.target/spu/split0-1.c	(.../cell-4_3-branch)	(revision 156810)
@@ -0,0 +1,17 @@
+/* Make sure there are only 2 loads. */
+/* { dg-do compile { target spu-*-* } } */
+/* { dg-options "-O2" } */
+/* { dg-final { scan-assembler-times "lqd	\\$\[0-9\]+,0\\(\\$\[0-9\]+\\)" 1 } } */
+/* { dg-final { scan-assembler-times "lqd	\\$\[0-9\]+,16\\(\\$\[0-9\]+\\)" 1 } } */
+/* { dg-final { scan-assembler-times "lq\[dx\]" 2 } } */
+  
+struct __attribute__ ((__aligned__(16))) S {
+  int a, b, c, d;
+  int e, f, g, h;
+};
+  
+int
+f(struct S *s)
+{ 
+  return s->a + s->b + s->c + s->d + s->e + s->f + s->g + s->h;
+} 
Index: gcc/testsuite/gcc.target/spu/vector-ansi.c
===================================================================
--- gcc/testsuite/gcc.target/spu/vector-ansi.c	(.../gcc-4_3-branch)	(revision 0)
+++ gcc/testsuite/gcc.target/spu/vector-ansi.c	(.../cell-4_3-branch)	(revision 156810)
@@ -0,0 +1,35 @@
+/* { dg-do compile } */
+/* { dg-options "-ansi" } */
+
+/* This is done by spu_internals.h, but we not include it here to keep
+   down the dependencies.  */
+
+#ifndef __VECTOR_KEYWORD_SUPPORTED__
+#define vector __vector
+#endif
+
+/* __vector is expanded unconditionally by the preprocessor.  */
+__vector int vi;
+__vector unsigned char vuc;
+__vector signed char vsc;
+__vector unsigned short vus;
+__vector signed short vss;
+__vector unsigned int vui;
+__vector signed int vsi;
+__vector unsigned long long ull;
+__vector signed long long sll;
+__vector float vf;
+__vector double vd;
+
+/* vector is expanded by the define above, regardless of context.  */
+vector int vi;
+vector unsigned char vuc;
+vector signed char vsc;
+vector unsigned short vus;
+vector signed short vss;
+vector unsigned int vui;
+vector signed int vsi;
+vector unsigned long long ull;
+vector signed long long sll;
+vector float vf;
+vector double vd;
Index: gcc/testsuite/gcc.dg/vector-subscript-2.c
===================================================================
--- gcc/testsuite/gcc.dg/vector-subscript-2.c	(.../gcc-4_3-branch)	(revision 0)
+++ gcc/testsuite/gcc.dg/vector-subscript-2.c	(.../cell-4_3-branch)	(revision 156810)
@@ -0,0 +1,14 @@
+/* { dg-do compile } */
+/* { dg-options "-W -Wall" } */
+
+/* Check that subscripting of vectors work with register storage class decls.  */
+
+#define vector __attribute__((vector_size(16) ))
+
+
+float vf(void)
+{
+  register vector float a;
+  return a[0]; /* { dg-bogus "register" } */
+}
+
Index: gcc/testsuite/gcc.dg/vmx/1b-07-ansi.c
===================================================================
--- gcc/testsuite/gcc.dg/vmx/1b-07-ansi.c	(.../gcc-4_3-branch)	(revision 0)
+++ gcc/testsuite/gcc.dg/vmx/1b-07-ansi.c	(.../cell-4_3-branch)	(revision 156810)
@@ -0,0 +1,59 @@
+/* { dg-do compile } */
+/* { dg-options "-ansi -maltivec" } */
+
+#include <altivec.h>
+vector char bool _0 ;
+vector bool char _8 ;
+vector char unsigned _56 ;
+vector unsigned char _64 ;
+vector char signed _112 ;
+vector signed char _120 ;
+/* bool is permitted in the predefine method, as it is expanded
+   unconditionally to int.  */
+bool _168 ;
+vector pixel _170 ;
+vector int bool _178 ;
+vector bool int _186 ;
+vector short bool _234 ;
+vector bool short _242 ;
+vector unsigned int _290 ;
+vector int unsigned _298 ;
+vector unsigned short _346 ;
+vector short unsigned _354 ;
+vector signed int _402 ;
+vector int signed _410 ;
+vector signed short _458 ;
+vector short signed _466 ;
+vector int bool _514 ;
+vector int bool _544 ;
+vector int bool _559 ;
+vector bool int _589 ;
+vector int short bool _874 ;
+vector int bool short _889 ;
+vector short int bool _904 ;
+vector short bool int _919 ;
+vector bool int short _934 ;
+vector bool short int _949 ;
+vector unsigned int _1234 ;
+vector int unsigned _1249 ;
+vector unsigned int _1279 ;
+vector int unsigned _1294 ;
+vector unsigned int _1309 ;
+vector int unsigned short _1594 ;
+vector int short unsigned _1609 ;
+vector unsigned int short _1624 ;
+vector unsigned short int _1639 ;
+vector short int unsigned _1654 ;
+vector short unsigned int _1669 ;
+vector signed int _1954 ;
+vector int signed _1969 ;
+vector signed int _1999 ;
+vector int signed _2014 ;
+vector signed int _2029 ;
+vector int signed short _2314 ;
+vector int short signed _2329 ;
+vector signed int short _2344 ;
+vector signed short int _2359 ;
+vector short int signed _2374 ;
+vector short signed int _2389 ;
+vector float _2674 ;
Index: gcc/testsuite/gcc.dg/vmx/1b-07.c
===================================================================
--- gcc/testsuite/gcc.dg/vmx/1b-07.c	(.../gcc-4_3-branch)	(revision 156795)
+++ gcc/testsuite/gcc.dg/vmx/1b-07.c	(.../cell-4_3-branch)	(revision 156810)
@@ -6,7 +6,6 @@
 vector unsigned char _64 ;
 vector char signed _112 ;
 vector signed char _120 ;
-bool _168 ;
 vector pixel _170 ;
 vector int bool _178 ;
 vector bool int _186 ;
Index: gcc/testsuite/gcc.dg/vmx/1b-06-ansi.c
===================================================================
--- gcc/testsuite/gcc.dg/vmx/1b-06-ansi.c	(.../gcc-4_3-branch)	(revision 0)
+++ gcc/testsuite/gcc.dg/vmx/1b-06-ansi.c	(.../cell-4_3-branch)	(revision 156810)
@@ -0,0 +1,24 @@
+/* { dg-do compile } */
+/* { dg-options "-ansi -maltivec" } */
+
+#include <altivec.h>
+vector char bool _4 ;
+vector char unsigned _31 ;
+vector char signed _59 ;
+/* bool is permitted in the predefine method, as it is expanded
+   unconditionally to int.  */
+bool _84 ;
+vector pixel _89 ;
+vector int bool _95 ;
+vector short bool _102 ;
+vector unsigned int _122 ;
+vector unsigned short _129 ;
+vector signed int _150 ;
+vector signed short _157 ;
+vector int bool _179 ;
+vector int short bool _186 ;
+vector unsigned int _206 ;
+vector int unsigned short _213 ;
+vector signed int _234 ;
+vector int signed short _241 ;
+vector float _339 ;
Index: gcc/testsuite/gcc.dg/vmx/1b-06.c
===================================================================
--- gcc/testsuite/gcc.dg/vmx/1b-06.c	(.../gcc-4_3-branch)	(revision 156795)
+++ gcc/testsuite/gcc.dg/vmx/1b-06.c	(.../cell-4_3-branch)	(revision 156810)
@@ -3,7 +3,6 @@
 vector char bool _4 ;
 vector char unsigned _31 ;
 vector char signed _59 ;
-bool _84 ;
 vector pixel _89 ;
 vector int bool _95 ;
 vector short bool _102 ;
Index: gcc/testsuite/gcc.dg/array-8.c
===================================================================
--- gcc/testsuite/gcc.dg/array-8.c	(.../gcc-4_3-branch)	(revision 156795)
+++ gcc/testsuite/gcc.dg/array-8.c	(.../cell-4_3-branch)	(revision 156810)
@@ -35,7 +35,7 @@
   f().c[0];
   0[f().c];
   /* Various invalid cases.  */
-  c[c]; /* { dg-error "subscripted value is neither array nor pointer" } */
+  c[c]; /* { dg-error "subscripted value is not an array, a pointer, or a vector" } */
   p[1.0]; /* { dg-error "array subscript is not an integer" } */
   1.0[a]; /* { dg-error "array subscript is not an integer" } */
   fp[0]; /* { dg-error "subscripted value is pointer to function" } */
Index: gcc/testsuite/gcc.dg/vector-subscript-1.c
===================================================================
--- gcc/testsuite/gcc.dg/vector-subscript-1.c	(.../gcc-4_3-branch)	(revision 0)
+++ gcc/testsuite/gcc.dg/vector-subscript-1.c	(.../cell-4_3-branch)	(revision 156810)
@@ -0,0 +1,16 @@
+/* { dg-do compile } */
+/* { dg-options "-w" } */
+
+#define vector __attribute__((vector_size(16) ))
+/* Check that vector[index] works and index[vector] is rejected.  */
+
+float vf(vector float a)
+{
+  return 0[a]; /* { dg-error "" } */
+}
+
+
+float fv(vector float a)
+{
+  return a[0];
+}
Index: gcc/testsuite/g++.dg/ext/vector-subscript-0.C
===================================================================
--- gcc/testsuite/g++.dg/ext/vector-subscript-0.C	(.../gcc-4_3-branch)	(revision 0)
+++ gcc/testsuite/g++.dg/ext/vector-subscript-0.C	(.../cell-4_3-branch)	(revision 156810)
@@ -0,0 +1,15 @@
+/* { dg-do compile } */
+// { dg-options "" }
+
+#define vector __attribute__((vector_size(16) ))
+
+template<int i>
+static inline int f(vector int a)
+{
+  return (a[i]);
+}
+int myfunc2( int x, int y, int z, int w )
+{
+    vector int temp = (vector int){x, y, z, w};
+    return f<0>(temp);
+}
Index: gcc/testsuite/g++.dg/ext/vector-subscript-1.C
===================================================================
--- gcc/testsuite/g++.dg/ext/vector-subscript-1.C	(.../gcc-4_3-branch)	(revision 0)
+++ gcc/testsuite/g++.dg/ext/vector-subscript-1.C	(.../cell-4_3-branch)	(revision 156810)
@@ -0,0 +1,16 @@
+/* { dg-do compile } */
+/* { dg-options "-w" } */
+
+#define vector __attribute__((vector_size(16) ))
+/* Check that vector[index] works and index[vector] is rejected.  */
+
+float vf(vector float a)
+{
+  return 0[a]; /* { dg-error "" } */
+}
+
+
+float fv(vector float a)
+{
+  return a[0];
+}
Index: gcc/testsuite/g++.dg/ext/vector-subscript-2.C
===================================================================
--- gcc/testsuite/g++.dg/ext/vector-subscript-2.C	(.../gcc-4_3-branch)	(revision 0)
+++ gcc/testsuite/g++.dg/ext/vector-subscript-2.C	(.../cell-4_3-branch)	(revision 156810)
@@ -0,0 +1,14 @@
+/* { dg-do compile } */
+/* { dg-options "-W -Wall" } */
+
+/* Check that subscripting of vectors work with register storage class decls.  */
+
+#define vector __attribute__((vector_size(16) ))
+
+
+float vf(void)
+{
+  register vector float a;
+  return a[0]; /* { dg-bogus "register" } */
+}
+
Index: gcc/testsuite/g++.dg/torture/vector-subscript-1.C
===================================================================
--- gcc/testsuite/g++.dg/torture/vector-subscript-1.C	(.../gcc-4_3-branch)	(revision 0)
+++ gcc/testsuite/g++.dg/torture/vector-subscript-1.C	(.../cell-4_3-branch)	(revision 156810)
@@ -0,0 +1,60 @@
+/* { dg-do run } */
+/* { dg-options "-w" } */
+#define vector __attribute__((vector_size(sizeof(int)*4) ))
+
+/* Check to make sure that we extract and insert the vector at the same
+   location for vector subscripting and that vectors layout are the same
+   as arrays. */
+
+struct TV4
+{
+    vector int v;
+};
+
+typedef struct TV4 MYV4;
+static inline int *f(MYV4 *a, int i)
+{
+  return &(a->v[i]);
+}
+
+static inline MYV4 myfunc2( int x, int y, int z, int w )
+{
+    MYV4 temp;
+    *f(&temp, 0 ) = x;
+    *f(&temp, 1 ) = y;
+    *f(&temp, 2 ) = z;
+    *f(&temp, 3 ) = w;
+    return temp;
+}
+
+MYV4 val3;
+
+__attribute__((noinline)) void modify (void) 
+{
+    val3 = myfunc2( 1, 2, 3, 4 );
+}
+
+int main( int argc, char* argv[] )
+{
+  int a[4];
+  int i;
+  
+  modify();
+  
+  if (*f(&val3, 0 ) != 1)
+    __builtin_abort ();
+  if (*f(&val3, 1 ) != 2)
+    __builtin_abort ();
+  if (*f(&val3, 2 ) != 3)
+    __builtin_abort ();
+  if (*f(&val3, 3 ) != 4)
+    __builtin_abort ();
+    
+  __builtin_memcpy(a, &val3, 16);
+  for(i = 0; i < 4; i++)
+    if (a[i] != i+1)
+      __builtin_abort ();
+  
+  
+  return 0;
+}
Index: gcc/testsuite/g++.dg/torture/vector-subscript-2.C
===================================================================
--- gcc/testsuite/g++.dg/torture/vector-subscript-2.C	(.../gcc-4_3-branch)	(revision 0)
+++ gcc/testsuite/g++.dg/torture/vector-subscript-2.C	(.../cell-4_3-branch)	(revision 156810)
@@ -0,0 +1,67 @@
+/* { dg-do run } */
+/* { dg-options "-w" } */
+#define vector __attribute__((vector_size(sizeof(int)*4) ))
+
+/* Check to make sure that we extract and insert the vector at the same
+   location for vector subscripting (with constant indexes) and
+   that vectors layout are the same as arrays. */
+
+struct TV4
+{
+    vector int v;
+};
+typedef TV4 MYV4;
+
+static inline MYV4 myfunc2( int x, int y, int z, int w )
+{
+    MYV4 temp;
+    temp.v[0] = x;
+    temp.v[1] = y;
+    temp.v[2] = z;
+    temp.v[3] = w;
+    return temp;
+}
+MYV4 val3;
+__attribute__((noinline)) void modify (void) 
+{
+    val3 = myfunc2( 1, 2, 3, 4 );
+}
+int main( int argc, char* argv[] )
+{
+  int a[4];
+  int i;
+  
+  /* Set up the vector.  */
+  modify();
+  
+  /* Check the vector via the global variable.  */
+  if (val3.v[0] != 1)
+    __builtin_abort ();
+  if (val3.v[1] != 2)
+    __builtin_abort ();
+  if (val3.v[2] != 3)
+    __builtin_abort ();
+  if (val3.v[3] != 4)
+    __builtin_abort ();
+    
+  vector int a1 = val3.v;
+  
+   /* Check the vector via a local variable.  */
+  if (a1[0] != 1)
+    __builtin_abort ();
+  if (a1[1] != 2)
+    __builtin_abort ();
+  if (a1[2] != 3)
+    __builtin_abort ();
+  if (a1[3] != 4)
+    __builtin_abort ();
+    
+  __builtin_memcpy(a, &val3, sizeof(a));  
+   /* Check the vector via copying it to an array.  */
+  for(i = 0; i < 4; i++)
+    if (a[i] != i+1)
+      __builtin_abort ();
+  
+  
+  return 0;
+}
Index: gcc/testsuite/g++.dg/torture/vector-subscript-3.C
===================================================================
--- gcc/testsuite/g++.dg/torture/vector-subscript-3.C	(.../gcc-4_3-branch)	(revision 0)
+++ gcc/testsuite/g++.dg/torture/vector-subscript-3.C	(.../cell-4_3-branch)	(revision 156810)
@@ -0,0 +1,60 @@
+/* { dg-do run } */
+/* { dg-options "-w" } */
+#define vector __attribute__((vector_size(sizeof(int)*4) ))
+
+/* Check to make sure that we extract and insert the vector at the same
+   location for vector subscripting and that vectors layout are the same
+   as arrays. */
+
+struct TV4
+{
+    vector int v;
+};
+
+typedef struct TV4 MYV4;
+template<int i>
+static inline int *f(MYV4 *a)
+{
+  return &(a->v[i]);
+}
+static inline MYV4 myfunc2( int x, int y, int z, int w )
+{
+    MYV4 temp;
+    *f<0>(&temp) = x;
+    *f<1>(&temp) = y;
+    *f<2>(&temp) = z;
+    *f<3>(&temp) = w;
+    return temp;
+}
+
+MYV4 val3;
+
+__attribute__((noinline)) void modify (void) 
+{
+    val3 = myfunc2( 1, 2, 3, 4 );
+}
+
+int main( int argc, char* argv[] )
+{
+  int a[4];
+  int i;
+  
+  modify();
+  
+  if (*f<0>(&val3) != 1)
+    __builtin_abort ();
+  if (*f<1>(&val3) != 2)
+    __builtin_abort ();
+  if (*f<2>(&val3) != 3)
+    __builtin_abort ();
+  if (*f<3>(&val3) != 4)
+    __builtin_abort ();
+    
+  __builtin_memcpy(a, &val3, 16);
+  for(i = 0; i < 4; i++)
+    if (a[i] != i+1)
+      __builtin_abort ();
+  
+  
+  return 0;
+}
Index: gcc/testsuite/g++.dg/torture/vector-subscript-4.C
===================================================================
--- gcc/testsuite/g++.dg/torture/vector-subscript-4.C	(.../gcc-4_3-branch)	(revision 0)
+++ gcc/testsuite/g++.dg/torture/vector-subscript-4.C	(.../cell-4_3-branch)	(revision 156810)
@@ -0,0 +1,68 @@
+/* { dg-do run } */
+/* { dg-options "-w" } */
+#define vector __attribute__((vector_size(sizeof(int)*4) ))
+
+/* Check to make sure that we extract and insert the vector at the same
+   location for vector subscripting (with constant indexes) and
+   that vectors layout are the same as arrays. */
+
+struct TV4
+{
+    vector int v;
+};
+typedef struct TV4 MYV4;
+
+template <typename type>
+static inline type myfunc2( int x, int y, int z, int w )
+{
+    type temp;
+    temp.v[0] = x;
+    temp.v[1] = y;
+    temp.v[2] = z;
+    temp.v[3] = w;
+    return temp;
+}
+MYV4 val3;
+__attribute__((noinline)) void modify (void) 
+{
+    val3 = myfunc2<MYV4>( 1, 2, 3, 4 );
+}
+int main( int argc, char* argv[] )
+{
+  int a[4];
+  int i;
+  
+  /* Set up the vector.  */
+  modify();
+  
+  /* Check the vector via the global variable.  */
+  if (val3.v[0] != 1)
+    __builtin_abort ();
+  if (val3.v[1] != 2)
+    __builtin_abort ();
+  if (val3.v[2] != 3)
+    __builtin_abort ();
+  if (val3.v[3] != 4)
+    __builtin_abort ();
+    
+  vector int a1 = val3.v;
+  
+   /* Check the vector via a local variable.  */
+  if (a1[0] != 1)
+    __builtin_abort ();
+  if (a1[1] != 2)
+    __builtin_abort ();
+  if (a1[2] != 3)
+    __builtin_abort ();
+  if (a1[3] != 4)
+    __builtin_abort ();
+    
+  __builtin_memcpy(a, &val3, sizeof(a));  
+   /* Check the vector via copying it to an array.  */
+  for(i = 0; i < 4; i++)
+    if (a[i] != i+1)
+      __builtin_abort ();
+  
+  
+  return 0;
+}
Index: gcc/testsuite/lib/target-supports.exp
===================================================================
--- gcc/testsuite/lib/target-supports.exp	(.../gcc-4_3-branch)	(revision 156795)
+++ gcc/testsuite/lib/target-supports.exp	(.../cell-4_3-branch)	(revision 156810)
@@ -485,6 +485,41 @@
     }]
 }
 
+# Return 1 if extended address spaces (__ea) is supported, 0
+# otherwise.
+#
+# This won't change for different subtargets so cache the result.
+
+proc check_effective_target_ea {} {
+    global et_ea_saved
+    global tool
+
+    if [info exists et_ea_saved] {
+	verbose "check_effective_target_ea: using cached result" 2
+    } else {
+	set et_ea_saved 1
+
+	set src ea[pid].c
+	set asm ea[pid].S
+	verbose "check_effective_target_ea: compiling testfile $src" 2
+	set f [open $src "w"]
+	# Compile a small test program.
+	puts $f "extern __ea int i;\n"
+	close $f
+
+	# Test for extended address space support on the target.
+	set comp_output \
+	    [${tool}_target_compile $src $asm assembly ""]
+	file delete $src
+	if { [string match "*not supported*" $comp_output] } {
+	    set et_ea_saved 0
+	}
+	remove-build-file $asm
+    }
+    verbose "check_effective_target_ea: returning $et_ea_saved" 2
+    return $et_ea_saved
+}
+
 # Return 1 if thread local storage (TLS) is supported, 0 otherwise.
 #
 # This won't change for different subtargets so cache the result.
Index: gcc/cp/typeck.c
===================================================================
--- gcc/cp/typeck.c	(.../gcc-4_3-branch)	(revision 156795)
+++ gcc/cp/typeck.c	(.../cell-4_3-branch)	(revision 156810)
@@ -2458,6 +2458,17 @@
   return error_mark_node;
 }
 
+/* Like cxx_mark_addressable but don't check register qualifier.  */
+static void
+mark_addressable_vector (tree x)
+{
+  while (handled_component_p (x))
+    x = TREE_OPERAND (x, 0);
+  if (TREE_CODE (x) != VAR_DECL && TREE_CODE (x) != PARM_DECL)
+    return ;
+  TREE_ADDRESSABLE (x) = 1;
+}
+
 /* This handles expressions of the form "a[i]", which denotes
    an array reference.
 
@@ -2504,6 +2515,22 @@
     default:
       break;
     }
+  
+  /* For vector[index], convert the vector to a pointer of the underlying
+     type. */
+  if (TREE_CODE (TREE_TYPE (array)) == VECTOR_TYPE)
+    {
+      tree type = TREE_TYPE (array);
+      tree type1;
+      /* Mark the vector as addressable but ignore the
+         register storage class.  */   
+      mark_addressable_vector (array);
+      type = build_qualified_type (TREE_TYPE (type), TYPE_QUALS (type));
+      type = build_pointer_type (type);
+      type1 = build_pointer_type (TREE_TYPE (array));
+      array = build1 (ADDR_EXPR, type1, array);
+      array = convert (type, array);
+    }
 
   if (TREE_CODE (TREE_TYPE (array)) == ARRAY_TYPE)
     {
@@ -6965,7 +6992,7 @@
   type = strip_array_types (CONST_CAST_TREE(type));
   if (type == error_mark_node)
     return TYPE_UNQUALIFIED;
-  return TYPE_QUALS (type);
+  return TYPE_QUALS (CONST_CAST_TREE (type));
 }
 
 /* Returns nonzero if the TYPE is const from a C++ perspective: look inside
Index: gcc/cp/decl2.c
===================================================================
--- gcc/cp/decl2.c	(.../gcc-4_3-branch)	(revision 156795)
+++ gcc/cp/decl2.c	(.../cell-4_3-branch)	(revision 156810)
@@ -280,6 +280,17 @@
     maybe_retrofit_in_chrg (function);
 }
 
+/* Like cxx_mark_addressable but don't check register storage class.  */
+static void
+mark_addressable_vector (tree x)
+{
+  while (handled_component_p (x))
+    x = TREE_OPERAND (x, 0);
+  if (TREE_CODE (x) != VAR_DECL && TREE_CODE (x) != PARM_DECL)
+    return ;
+  TREE_ADDRESSABLE (x) = 1;
+}
+
 /* Create an ARRAY_REF, checking for the user doing things backwards
    along the way.  */
 
@@ -316,6 +327,21 @@
   else
     {
       tree p1, p2, i1, i2;
+      /* For vector[index], convert the vector to a pointer of the underlying
+	 type. */
+      if (TREE_CODE (type) == VECTOR_TYPE)
+	{
+	  tree type = TREE_TYPE (array_expr);
+	  tree type1;
+	  /* Mark the vector as addressable but ignore the
+	     register storage class.  */
+	  mark_addressable_vector (array_expr);
+	  type = build_qualified_type (TREE_TYPE (type), TYPE_QUALS (type));
+	  type = build_pointer_type (type);
+	  type1 = build_pointer_type (TREE_TYPE (array_expr));
+	  array_expr = build1 (ADDR_EXPR, type1, array_expr);
+	  array_expr = convert (type, array_expr);
+      }
 
       /* Otherwise, create an ARRAY_REF for a pointer or array type.
 	 It is a little-known fact that, if `a' is an array and `i' is
Index: gcc/tree-ssa-loop-ivopts.c
===================================================================
--- gcc/tree-ssa-loop-ivopts.c	(.../gcc-4_3-branch)	(revision 156795)
+++ gcc/tree-ssa-loop-ivopts.c	(.../cell-4_3-branch)	(revision 156810)
@@ -1912,9 +1912,16 @@
 static tree
 generic_type_for (tree type)
 {
-  if (POINTER_TYPE_P (type))
+  if (GENERIC_ADDR_SPACE_POINTER_TYPE_P (type))
     return unsigned_type_for (type);
 
+  if (OTHER_ADDR_SPACE_POINTER_TYPE_P (type))
+    {
+      int qual = ENCODE_QUAL_ADDR_SPACE (TYPE_ADDR_SPACE (TREE_TYPE (type)));
+      return build_pointer_type
+	(build_qualified_type (void_type_node, qual));
+    }
+
   if (TYPE_UNSIGNED (type))
     return type;
 
Index: gcc/c-objc-common.c
===================================================================
--- gcc/c-objc-common.c	(.../gcc-4_3-branch)	(revision 156795)
+++ gcc/c-objc-common.c	(.../cell-4_3-branch)	(revision 156810)
@@ -187,6 +187,10 @@
 int
 c_types_compatible_p (tree x, tree y)
 {
+  if (TYPE_ADDR_SPACE (strip_array_types (x))
+      != TYPE_ADDR_SPACE (strip_array_types (y)))
+    return false;
+
   return comptypes (TYPE_MAIN_VARIANT (x), TYPE_MAIN_VARIANT (y));
 }
 
Index: gcc/c-tree.h
===================================================================
--- gcc/c-tree.h	(.../gcc-4_3-branch)	(revision 156795)
+++ gcc/c-tree.h	(.../cell-4_3-branch)	(revision 156810)
@@ -286,6 +286,8 @@
   BOOL_BITFIELD restrict_p : 1;
   /* Whether "_Sat" was specified.  */
   BOOL_BITFIELD saturating_p : 1;
+  /* Whether the declaration is in another address space.  */
+  unsigned char address_space;
 };
 
 /* The various kinds of declarators in C.  */
@@ -516,6 +518,7 @@
 					       struct c_typespec);
 extern struct c_declspecs *declspecs_add_scspec (struct c_declspecs *, tree);
 extern struct c_declspecs *declspecs_add_attrs (struct c_declspecs *, tree);
+extern struct c_declspecs *declspecs_add_addrspace (struct c_declspecs *, tree);
 extern struct c_declspecs *finish_declspecs (struct c_declspecs *);
 
 /* in c-objc-common.c */
Index: gcc/ifcvt.c
===================================================================
--- gcc/ifcvt.c	(.../gcc-4_3-branch)	(revision 156795)
+++ gcc/ifcvt.c	(.../cell-4_3-branch)	(revision 156810)
@@ -73,6 +73,10 @@
 
 #define NULL_BLOCK	((basic_block) NULL)
 
+/* boolean to indicate that aggressive cmov branch elimination
+   should be tried. */
+static int aggressive_cmov;
+
 /* # of IF-THEN or IF-THEN-ELSE blocks we looked at  */
 static int num_possible_if_blocks;
 
@@ -104,6 +108,8 @@
 static int cond_exec_find_if_block (ce_if_block_t *);
 static int find_if_case_1 (basic_block, edge, edge);
 static int find_if_case_2 (basic_block, edge, edge);
+static int block_modifies_live_reg (basic_block, basic_block);
+static int find_andif_orif_block (ce_if_block_t *);
 static int find_memory (rtx *, void *);
 static int dead_or_predicable (basic_block, basic_block, basic_block,
 			       basic_block, int);
@@ -1492,6 +1498,522 @@
   return FALSE;
 }
 
+/* Determine if there are instructions within a basic block that
+   will prevent the block from being eliminated by conditional 
+   move transformation. */ 
+
+static int
+safe_to_transform_p (basic_block blk)
+{
+  rtx insn, set;
+
+  if (!blk)
+    return TRUE;
+
+  for (insn = BB_HEAD (blk); insn; insn = NEXT_INSN (insn))
+    {
+      if (active_insn_p (insn))
+	switch (GET_CODE (insn))
+	  {
+
+	  case JUMP_INSN:
+	    break;
+
+	  case INSN:
+	    if ((set = single_set (insn)))
+	      {
+		if (GET_CODE ((SET_DEST (set))) == MEM)
+		  return FALSE;
+		if (!noce_operand_ok (SET_SRC (set)))
+		  return FALSE;
+		break;
+	      }
+	    /* fallthrough */
+
+	  default:
+	    /* Anything else, e.g., calls returns, cannot be handled. */
+	    return FALSE;
+	  }
+      if (insn == BB_END (blk))
+	break;
+    }
+
+  return TRUE;
+}
+
+/* Determine if it is cost effective to proceed with aggressive cmov
+   transformation. */
+
+static int
+cost_effective_p (int fallthru_cost, int taken_cost, rtx jump_insn)
+{
+  rtx note;
+  int prob;			/* conditional branch probability */
+  int predicted_cost;
+  int cmov_cost = taken_cost + fallthru_cost;
+
+  /* Factor in BRANCH costs.  There will usually be a jump instruction
+     at the bottom of the fallthru block.  The cost of this jump could
+     depend upon architecture specific features, such as hardware 
+     branch predicition or insertion of branch hint instructions. 
+     If FALLTHRU_BRANCH_COST is undefined, we assume the branch is
+     correctly predicted without additional cost. */
+
+#ifdef FALLTHRU_BRANCH_COST
+  fallthru_cost += FALLTHRU_BRANCH_COST;
+#endif
+
+  taken_cost += BRANCH_COST;	/* always mispredicted */
+
+  if ((note = find_reg_note (jump_insn, REG_BR_PROB, 0)))
+    {
+      prob = INTVAL (XEXP (note, 0));
+      predicted_cost =
+	(prob * taken_cost +
+	 (REG_BR_PROB_BASE - prob) * fallthru_cost) / REG_BR_PROB_BASE;
+    }
+  else
+    {
+      predicted_cost = taken_cost + fallthru_cost;
+    }
+
+  return (cmov_cost > predicted_cost) ? FALSE : TRUE;
+}
+
+/* Throughout the rtx X, replace many registers according to REG_MAP.
+   REG_MAP[R] is the replacement for register R, or 0 for don't replace.
+   NREGS is the length of REG_MAP; regs >= NREGS are not mapped.
+
+   If X is a SET that modifies register R and R is in EXPOSED_SET, then
+   replace register R with a new pseudo and update REG_MAP[R].
+
+   Also check for cases that we don't handle, e.g., stores and
+   instructions with side effects. */
+
+static int remap_regs_status;
+static void
+validate_remap_regs_1 (rtx * loc, rtx * reg_map, rtx * reg_map_orig,
+		       unsigned int nregs, rtx insn, regset exposed_set)
+{
+  enum rtx_code code;
+  int i, regno;
+  const char *fmt;
+  rtx x = *loc;
+
+  if (!x || remap_regs_status < 0)
+    return;
+
+  code = GET_CODE (x);
+  switch (code)
+    {
+    case SCRATCH:
+    case PC:
+    case CC0:
+    case CONST_INT:
+    case CONST_DOUBLE:
+    case CONST_VECTOR:
+    case CONST:
+    case SYMBOL_REF:
+    case LABEL_REF:
+      return;
+
+    case REG:
+      /* Verify that the register has an entry before trying to access it.  */
+      regno = REGNO (x);
+      if (REGNO (x) < nregs && reg_map[regno] != 0)
+	validate_change (insn, loc, reg_map[regno], 1);
+      return;
+
+    case SET:
+      validate_remap_regs_1 (&SET_SRC (x), reg_map, reg_map_orig, nregs, insn,
+			     exposed_set);
+
+      loc = &SET_DEST (x);
+      x = SET_DEST (x);
+      if (GET_CODE (x) == SUBREG)
+	{
+	  loc = &SUBREG_REG (x);
+	  x = SUBREG_REG (x);
+	}
+      if (GET_CODE (x) == MEM || GET_CODE (x) == STRICT_LOW_PART)
+	{
+	  remap_regs_status = -1;
+	  return;
+	}
+      if (GET_CODE (x) == REG)
+	{
+	  regno = REGNO (x);
+	  if (REGNO_REG_SET_P (exposed_set, regno))
+	    {
+	      /* Ensure there is a conditional move instruction
+	         for registers of this mode. */
+	      if (!can_conditionally_move_p (GET_MODE (x)))
+		{
+		  remap_regs_status = -2;
+		  return;
+		}
+	      reg_map_orig[regno] = x;
+	      reg_map[regno] = gen_reg_rtx (GET_MODE (x));
+	      ORIGINAL_REGNO (reg_map[regno]) = regno;
+	      validate_change (insn, loc, reg_map[regno], 1);
+	    }
+	}
+      else
+	validate_remap_regs_1 (loc, reg_map, reg_map_orig, nregs, insn,
+			       exposed_set);
+      return;
+
+    case CLOBBER:
+      if (GET_CODE (x) == SUBREG)
+	{
+	  loc = &SUBREG_REG (x);
+	  x = SUBREG_REG (x);
+	}
+      if (GET_CODE (x) == REG && REGNO_REG_SET_P (exposed_set, REGNO (x)))
+	abort ();
+      break;
+
+    default:
+      break;
+    }
+
+  fmt = GET_RTX_FORMAT (code);
+  for (i = GET_RTX_LENGTH (code) - 1; i >= 0; i--)
+    {
+      if (fmt[i] == 'e')
+	validate_remap_regs_1 (&XEXP (x, i), reg_map, reg_map_orig, nregs,
+			       insn, exposed_set);
+      else if (fmt[i] == 'E')
+	{
+	  int j;
+	  for (j = 0; j < XVECLEN (x, i); j++)
+	    validate_remap_regs_1 (&XVECEXP (x, i, j), reg_map, reg_map_orig,
+				   nregs, insn, exposed_set);
+	}
+    }
+}
+
+static int
+validate_remap_regs (rtx * reg_map, rtx * reg_map_orig, unsigned int nregs,
+		     rtx insn, regset exposed_set)
+{
+  /* REG_NOTES needs to be first because it has no SETs. */
+  remap_regs_status = 0;
+  validate_remap_regs_1 (&REG_NOTES (insn), reg_map, reg_map_orig, nregs,
+			 insn, exposed_set);
+  validate_remap_regs_1 (&PATTERN (insn), reg_map, reg_map_orig, nregs, insn,
+			 exposed_set);
+  return remap_regs_status;
+}
+
+
+/* For all instructions in TGT remap registers such that any
+   modification of a register in EXPOSED_SET is instead replaced with a
+   pseudo register and any corresponding references are also updated.
+   The register being replaced is saved in REG_MAP_ORIG and the new
+   pseudo register is stored in REG_MAP.  */
+static int
+remap_instructions (basic_block tgt, rtx * reg_map, rtx * reg_map_orig,
+		    int nregs, regset exposed_set)
+{
+  rtx insn, next_insn, bb_end;
+  int status;
+
+  next_insn = BB_HEAD (tgt);
+  bb_end = BB_END (tgt);
+  do
+    {
+      insn = next_insn;
+      next_insn = NEXT_INSN (insn);
+
+      if (GET_CODE (insn) == JUMP_INSN)
+	break;
+
+      if (GET_CODE (insn) == CALL_INSN)
+	return -3;
+
+      /* Don't move labels and notes. */
+      if (GET_CODE (insn) != INSN)
+	continue;
+
+      if (!noce_operand_ok (PATTERN (insn)))
+	return -4;
+
+      status =
+	validate_remap_regs (reg_map, reg_map_orig, nregs, insn, exposed_set);
+      if (status < 0)
+	return status;
+    }
+  while (insn != bb_end);
+  return 0;
+}
+
+/* Move instructions from TGT to the end of HDR (before the jump).  
+   HDR is the test block and TGT is either the target block or fallthru
+   block of an if statement. */
+static int
+hoist_instructions (basic_block hdr, basic_block tgt)
+{
+  rtx insn, next_insn, bb_end;
+  rtx jump_insn = BB_END (hdr);
+  int n_insns = 0;
+
+  next_insn = BB_HEAD (tgt);
+  bb_end = BB_END (tgt);
+  do
+    {
+      insn = next_insn;
+      next_insn = NEXT_INSN (insn);
+
+      if (GET_CODE (insn) == JUMP_INSN)
+	break;
+
+      /* Don't move labels and notes. */
+      if (GET_CODE (insn) != INSN)
+	continue;
+
+      remove_insn (insn);
+      NEXT_INSN (insn) = 0;
+      PREV_INSN (insn) = 0;
+      emit_insn_before (insn, jump_insn);
+      n_insns++;
+
+    }
+  while (insn != bb_end);
+  return n_insns;
+}
+
+/* Every REG in X should be marked in EXPOSED_SET.  */
+static void
+mark_regs_in_rtx (rtx x, regset exposed_set)
+{
+  int i, code;
+  const char *fmt;
+
+  if (!x)
+    return;
+
+  code = GET_CODE (x);
+  if (code == REG)
+    {
+      SET_REGNO_REG_SET (exposed_set, REGNO (x));
+      return;
+    }
+
+  fmt = GET_RTX_FORMAT (code);
+  for (i = GET_RTX_LENGTH (code) - 1; i >= 0; i--)
+    {
+      if (fmt[i] == 'e')
+	mark_regs_in_rtx (XEXP (x, i), exposed_set);
+      else if (fmt[i] == 'E')
+	{
+	  int j;
+	  for (j = 0; j < XVECLEN (x, i); j++)
+	    mark_regs_in_rtx (XVECEXP (x, i, j), exposed_set);
+	}
+    }
+}
+
+/* If *PX is a REG and the corresponding entry is set in REG_MAP_ORIG 
+   we need to make a copy of the register and change *PX.  */
+static void
+copy_regs_in_rtx (rtx * px, rtx * reg_map_orig, int nregs)
+{
+  rtx x = *px;
+  int i, code;
+  const char *fmt;
+
+  if (!x)
+    return;
+
+  code = GET_CODE (x);
+  if (code == REG)
+    {
+      if (REGNO (x) < (unsigned) nregs && reg_map_orig[REGNO (x)])
+	{
+	  rtx nr = gen_reg_rtx (GET_MODE (x));
+	  emit_move_insn (nr, x);
+	  /* We don't check if this replacement is valid because we are
+	     working on a copy. */
+	  *px = nr;
+	}
+      return;
+    }
+
+  fmt = GET_RTX_FORMAT (code);
+  for (i = GET_RTX_LENGTH (code) - 1; i >= 0; i--)
+    {
+      if (fmt[i] == 'e')
+	copy_regs_in_rtx (&XEXP (x, i), reg_map_orig, nregs);
+      else if (fmt[i] == 'E')
+	{
+	  int j;
+	  for (j = 0; j < XVECLEN (x, i); j++)
+	    copy_regs_in_rtx (&XVECEXP (x, i, j), reg_map_orig, nregs);
+	}
+    }
+}
+
+
+/* Try to merge blocks and eliminate a conditional branch by converting 
+   instructions in the blocks following the branch to conditional move 
+   instructions.  This routine differs from noce_try_cmove() and 
+   noce_try_cmove_arith() in that situations are handled involving 
+   assignment to multiple variables and assignment to temporary variables 
+   used within either the branch fallthru or branch taken block. */
+
+static int
+noce_try_cmove_aggressive (struct noce_if_info *if_info)
+{
+#if HAVE_conditional_move
+  basic_block hdr = if_info->test_bb;
+  basic_block fallthru = if_info->then_bb;
+  basic_block taken = if_info->else_bb;
+  int fallthru_cost = 0, taken_cost = 0;
+  rtx *reg_map_fallthru = 0, *reg_map_taken = 0, *reg_map_orig = 0, seq;
+  regset exposed_set;
+  int i, nregs, status, n_insns = 0, n_cmovs = 0;
+
+  /* Ensure the condition in the branch at the end of the header block 
+     is conformable to transformation. */
+
+  if (!onlyjump_p (if_info->jump))
+    return FALSE;
+
+  if (!if_info->cond || (GET_MODE (XEXP (if_info->cond, 0)) == BLKmode))
+    return FALSE;
+
+  fallthru_cost = fallthru ? count_bb_insns (fallthru) : 0;
+  taken_cost = taken ? count_bb_insns (taken) : 0;
+
+  /* Determine if it is cost effective to proceed. */
+
+  if (!cost_effective_p (fallthru_cost, taken_cost, if_info->jump))
+    return FALSE;
+
+  /* Determine which register we need to replace with pseudos. */
+  exposed_set = ALLOC_REG_SET (&reg_obstack);
+  COPY_REG_SET (exposed_set, DF_LR_OUT (if_info->test_bb));
+  mark_regs_in_rtx (if_info->cond, exposed_set);
+  if (fallthru)
+    IOR_REG_SET (exposed_set, DF_LR_OUT (fallthru));
+  if (taken)
+    IOR_REG_SET (exposed_set, DF_LR_OUT (taken));
+
+  nregs = max_reg_num ();
+
+  /* Change all register references so that there will be no conflicts 
+     when moving the fallthru and taken blocks to the test block. */
+  status = 0;
+  reg_map_orig = xcalloc (nregs, sizeof (rtx));
+  if (fallthru)
+    {
+      reg_map_fallthru = xcalloc (nregs, sizeof (rtx));
+      status =
+	remap_instructions (fallthru, reg_map_fallthru, reg_map_orig, nregs,
+			    exposed_set);
+    }
+  if (status >= 0 && taken)
+    {
+      reg_map_taken = xcalloc (nregs, sizeof (rtx));
+      status =
+	remap_instructions (taken, reg_map_taken, reg_map_orig, nregs,
+			    exposed_set);
+    }
+
+  if (status >= 0)
+    {
+      rtx cond;
+      /* Move taken and fallthru instructions to the test block. */
+      if (taken)
+	n_insns += hoist_instructions (hdr, taken);
+      if (fallthru)
+	n_insns += hoist_instructions (hdr, fallthru);
+
+      /* Create the conditional moves.   */
+      start_sequence ();
+
+      /* When registers used in info.cond are modified in the fallthru
+         or taken blocks we must make a copy to use for the conditional
+         moves.  Don't modify the original condition because we could
+         still fail after this point. */
+      cond = copy_rtx (if_info->cond);
+      copy_regs_in_rtx (&cond, reg_map_orig, nregs);
+
+      for (i = 0; i < nregs; i++)
+	{
+	  enum machine_mode mode;
+	  rtx a, b, target;
+	  int code;
+	  if (reg_map_orig[i])
+	    {
+	      target = reg_map_orig[i];
+	      mode = GET_MODE (target);
+	      a = target;
+	      b = target;
+	      if (taken && reg_map_taken[i])
+		a = reg_map_taken[i];
+	      if (fallthru && reg_map_fallthru[i])
+		b = reg_map_fallthru[i];
+	      if (a == b)
+		abort ();
+	      code = GET_CODE (cond);
+	      if (!emit_conditional_move (target, code,
+					  XEXP (cond, 0),
+					  XEXP (cond, 1),
+					  VOIDmode,
+					  a, b,
+					  mode,
+					  (code == LTU || code == GEU
+					   || code == LEU || code == GTU)))
+		{
+		  status = -6;
+		  break;
+		}
+
+	      n_cmovs++;
+	    }
+	}
+      seq = get_insns ();
+      end_sequence ();
+
+      if (status >= 0)
+	{
+	  emit_insn_before_setloc (seq, if_info->jump,
+				   INSN_LOCATOR (if_info->jump));
+
+	  if (dump_file)
+	    fprintf (dump_file,
+		     "%d insn%s hoisted out of if statement and %d cmov insn%s generated.\n",
+		     n_insns, (n_insns == 1) ? " was" : "s were", n_cmovs,
+		     (n_cmovs == 1) ? " was" : "s were");
+	  if (n_cmovs > 0)
+	    {
+	      cond_exec_changed_p = TRUE;
+	      num_updated_if_blocks++;
+	    }
+
+	}
+    }
+
+  if (status < 0)
+    cancel_changes (0);
+  else if (!apply_change_group ())
+    status = -5;
+
+
+  CLEAR_REG_SET (exposed_set);
+  free (reg_map_orig);
+  if (reg_map_fallthru)
+    free (reg_map_fallthru);
+  if (reg_map_taken)
+    free (reg_map_taken);
+  return status >= 0 && n_cmovs > 0;
+#else
+  return FALSE;
+#endif
+}
+
 /* For most cases, the simplified condition we found is the best
    choice, but this is not the case for the min/max/abs transforms.
    For these we wish to know that it is A or B in the condition.  */
@@ -2151,6 +2673,188 @@
   return false;
 }
 
+/* Given a simple IF-ANDIF-THEN block, attempt to convert it
+   without using conditional execution.  Return TRUE if we were
+   successful at converting the block.  */
+
+static int
+noce_process_andif_orif_block (ce_if_block_t * ce_info, int andif)
+{
+  basic_block test_bb = ce_info->test_bb;	/* Basic block test is in */
+  basic_block then_bb = ce_info->then_bb;	/* Basic block for THEN block */
+  basic_block else_bb = ce_info->else_bb;	/* Basic block for ELSE block */
+  /* We've found a pattern of the form
+
+     (1) if (a && b) ...
+     (2) if (a || b) ...
+
+     Convert it to
+
+     (1) if ((a) & (b)) ...
+     (2) if ((a) | (b)) ...
+
+   */
+
+  edge e;
+  rtx test_earliest, then_earliest;
+  rtx jump, cond;
+  rtx then_jump, then_cond;
+  rtx note, then_note, set;
+  enum machine_mode mode;
+  HOST_WIDE_INT prob, then_prob;
+
+  /* If the conditional jump is more than just a conditional jump,
+     then we can not do if-conversion on this block.  */
+  jump = BB_END (test_bb);
+  then_jump = BB_END (then_bb);
+  if (!onlyjump_p (jump) || !onlyjump_p (then_jump))
+    return FALSE;
+
+  /* Check that we don't modify anything that is
+     live at the beginning of the ELSE block. */
+
+  /* The THEN block should be small enough to merge. */
+  if (count_bb_insns (then_bb) > BRANCH_COST)
+    return FALSE;
+
+  if (block_modifies_live_reg (then_bb, else_bb))
+    return FALSE;
+  if (!safe_to_transform_p (then_bb))
+    return FALSE;
+
+  /* Get the mode of jump's condition. The cond return by get_condition
+     might have mode VOIDmode. */
+  if (!any_condjump_p (jump) || !(set = pc_set (jump)))
+    return FALSE;
+
+  mode = GET_MODE (XEXP (SET_SRC (set), 0));
+  if (mode == VOIDmode)
+    return FALSE;
+
+  /* FIXME: This may be too conservative. valid_at_insn_p can be false? */
+  /* If this is not a standard conditional jump, we can't parse it.  */
+  cond = get_condition (jump, &test_earliest, 0, true);
+  if (!cond || !test_earliest)
+    return FALSE;
+
+  then_cond = get_condition (then_jump, &then_earliest, 0, true);
+  if (!then_cond || !then_earliest)
+    return FALSE;
+
+  /* We must be comparing objects whose modes imply the size.  */
+  if (GET_MODE (XEXP (cond, 0)) == BLKmode
+      || GET_MODE (XEXP (then_cond, 0)) == BLKmode)
+    return FALSE;
+
+  /* Only merge if both branches are more likely to go to the common
+     destination.  When that happens bb reordering can't arrange for
+     both of the branches to fall-thru to the more likely case.  By
+     merging the two branches we exchange the cost of a mispredicted
+     branch for the additional tests.
+     This is the most important test because it limits the optimization
+     to cases it is most likely to be a win.  But it ignores the cases
+     where this optimization on it's own is not a win but in combination
+     with some other if conversions it would be. */
+  note = find_reg_note (jump, REG_BR_PROB, NULL_RTX);
+  then_note = find_reg_note (then_jump, REG_BR_PROB, NULL_RTX);
+  prob = note ? INTVAL (XEXP (note, 0)) : REG_BR_PROB_BASE / 2;
+  then_prob = then_note ? INTVAL (XEXP (then_note, 0)) : REG_BR_PROB_BASE / 2;
+  if (prob < REG_BR_PROB_BASE / 2
+      || (andif ? then_prob < REG_BR_PROB_BASE / 2 : then_prob >
+	  REG_BR_PROB_BASE / 2))
+    return FALSE;
+
+
+  {
+    enum rtx_code code;
+    rtx seq_a, seq_b, seq_t;
+    rtx a, b, t;
+    rtx new_jump;
+
+    start_sequence ();
+    a = gen_reg_rtx (mode);
+    code = GET_CODE (cond);
+    a = emit_store_flag (a, code, XEXP (cond, 0), XEXP (cond, 1), VOIDmode,
+			 (code == LTU || code == LEU || code == GEU
+			  || code == GTU), 0);
+    seq_a = get_insns ();
+    end_sequence ();
+
+    if (!a)
+      return FALSE;
+
+    start_sequence ();
+    b = gen_reg_rtx (mode);
+    code = GET_CODE (then_cond);
+    b =
+      emit_store_flag (b, code, XEXP (then_cond, 0), XEXP (then_cond, 1),
+		       VOIDmode, (code == LTU || code == LEU || code == GEU
+				  || code == GTU), 0);
+    seq_b = get_insns ();
+    end_sequence ();
+
+    if (!b)
+      return FALSE;
+
+    emit_insn_before (seq_a, test_earliest);
+    emit_insn_before (seq_b, then_earliest);
+
+    start_sequence ();
+    t = gen_reg_rtx (mode);
+    if (andif)
+      emit_insn (gen_rtx_SET (VOIDmode, t, gen_rtx_IOR (mode, a, b)));
+    else
+      emit_insn (gen_rtx_SET
+		 (VOIDmode, t, gen_rtx_AND (mode, gen_rtx_NOT (mode, a), b)));
+    emit_cmp_and_jump_insns (t, const0_rtx, NE, 0, mode, 0,
+			     JUMP_LABEL (then_jump));
+    seq_t = get_insns ();
+    new_jump = get_last_insn ();
+    end_sequence ();
+
+    emit_insn_before (seq_t, then_jump);
+
+    if (GET_CODE (new_jump) != JUMP_INSN)
+      abort ();
+    JUMP_LABEL (new_jump) = JUMP_LABEL (then_jump);
+    LABEL_NUSES (JUMP_LABEL (new_jump)) += 1;
+
+    /* Compute the new branch probability. */
+    then_prob = ((REG_BR_PROB_BASE - prob) * then_prob) / REG_BR_PROB_BASE;
+    if (andif)
+      then_prob += prob;
+
+    REG_NOTES (new_jump) = gen_rtx_EXPR_LIST (REG_BR_PROB,
+					      GEN_INT (then_prob),
+					      REG_NOTES (new_jump));
+
+    /* We have previously tested that then_bb has exactly two
+       successors. */
+    gcc_assert (EDGE_COUNT (then_bb->succs) == 2);
+    e = EDGE_SUCC (then_bb, 0);
+    if (e->flags & EDGE_FALLTHRU)
+      {
+	e->probability = REG_BR_PROB_BASE - then_prob;
+	EDGE_SUCC (then_bb, 1)->probability = then_prob;
+      }
+    else
+      {
+	e->probability = then_prob;
+	EDGE_SUCC (then_bb, 1)->probability = REG_BR_PROB_BASE - then_prob;
+      }
+
+    delete_insn (jump);
+    delete_insn (then_jump);
+
+    /* Merge the blocks!  */
+    ce_info->else_bb = 0;
+    ce_info->join_bb = 0;
+    merge_if_block (ce_info);
+  }
+
+  return TRUE;
+}
+
 /* Return whether we can use store speculation for MEM.  TOP_BB is the
    basic block above the conditional block where we are considering
    doing the speculative store.  We look for whether MEM is set
@@ -2841,6 +3545,9 @@
       && cond_move_process_if_block (&if_info))
     return TRUE;
 
+  if (aggressive_cmov && noce_try_cmove_aggressive (&if_info))
+    return TRUE;
+
   return FALSE;
 }
 
@@ -2922,7 +3629,8 @@
 			&& CALL_P (last)
 			&& SIBLING_CALL_P (last))
 		    || ((EDGE_SUCC (combo_bb, 0)->flags & EDGE_EH)
-			&& can_throw_internal (last)));
+			&& can_throw_internal (last))
+		    || aggressive_cmov);
     }
 
   /* The JOIN block may have had quite a number of other predecessors too.
@@ -3013,7 +3721,11 @@
   IFCVT_INIT_EXTRA_FIELDS (&ce_info);
 #endif
 
-  if (! reload_completed
+  if (flag_aggressive_cmov
+      && !reload_completed && find_andif_orif_block (&ce_info))
+    goto success;
+
+  if (!reload_completed
       && noce_find_if_block (test_bb, then_edge, else_edge, pass))
     goto success;
 
@@ -3113,6 +3825,77 @@
   return n_insns;
 }
 
+/* Determine if a given basic block heads a simple IF-ANDIF-THEN or
+   IF-ORIF-THEN block.  If so, we'll try to convert the insns to not
+   require the branch.  Return TRUE if we were successful at converting
+   the the block.  */
+
+static int
+find_andif_orif_block (ce_if_block_t * ce_info)
+{
+  basic_block test_bb = ce_info->test_bb;
+  basic_block then_bb = ce_info->then_bb;
+  basic_block else_bb = ce_info->else_bb;
+  edge then_then_edge;
+  edge then_else_edge;
+
+  /* The THEN block of an IF-ANDIF-THEN/IF-ORIF-THEN combo must have
+   * exactly one predecessor.  */
+  if (!single_pred_p (then_bb))
+    return FALSE;
+
+  /* The THEN block of an IF-THEN combo must have exactly two
+   * successors.  */
+  if (EDGE_COUNT (then_bb->succs) != 2)
+    return FALSE;
+
+  then_then_edge = EDGE_SUCC (then_bb, 0);
+  then_else_edge = EDGE_SUCC (then_bb, 1);
+
+  /* Neither edge should be abnormal.  */
+  if ((then_then_edge->flags & EDGE_COMPLEX)
+      || (then_else_edge->flags & EDGE_COMPLEX))
+    return FALSE;
+
+  /* The THEN edge is canonically the one that falls through.  */
+  if (then_then_edge->flags & EDGE_FALLTHRU)
+    ;
+  else if (then_else_edge->flags & EDGE_FALLTHRU)
+    {
+      edge e = then_else_edge;
+      then_else_edge = then_then_edge;
+      then_then_edge = e;
+    }
+  else
+    /* Otherwise this must be a multiway branch of some sort.  */
+    return FALSE;
+
+  /* One of then's successors must point to ELSE. */
+  if (then_then_edge->dest == else_bb)
+    {
+      if (dump_file)
+	fprintf (dump_file,
+		 "\nIF-ORIF-THEN block found, start %d, then %d, else %d, then-then %d\n",
+		 test_bb->index, then_bb->index, else_bb->index,
+		 then_then_edge->dest->index);
+      num_possible_if_blocks++;
+      return noce_process_andif_orif_block (ce_info, 0);
+    }
+  else if (then_else_edge->dest == else_bb)
+    {
+      if (dump_file)
+	fprintf (dump_file,
+		 "\nIF-ANDIF-THEN block found, start %d, then %d, else %d, then-then %d\n",
+		 test_bb->index, then_bb->index, else_bb->index,
+		 then_then_edge->dest->index);
+      num_possible_if_blocks++;
+      return noce_process_andif_orif_block (ce_info, 1);
+    }
+  else
+    return FALSE;
+
+}
+
 /* Determine if a given basic block heads a simple IF-THEN or IF-THEN-ELSE
    block.  If so, we'll try to convert the insns to not require the branch.
    Return TRUE if we were successful at converting the block.  */
@@ -3743,6 +4526,52 @@
   return MEM_P (*px);
 }
 
+/* Return TRUE if insns in MERGE_BB modified registers that are live at
+   the start of ELSE_BB */
+static int
+block_modifies_live_reg (basic_block merge_bb, basic_block else_bb)
+{
+  bitmap merge_set, tmp;
+  unsigned fail = 0;
+  rtx insn;
+
+  /* Collect: MERGE_SET = set of registers set in MERGE_BB */
+
+  merge_set = BITMAP_ALLOC (&reg_obstack);
+  tmp = BITMAP_ALLOC (&reg_obstack);
+
+  /* ??? bb->local_set is only valid during calculate_global_regs_live,
+     so we must recompute usage for MERGE_BB.  Not so bad, I suppose, 
+     since we've already asserted that MERGE_BB is small.  */
+  FOR_BB_INSNS (merge_bb, insn)
+    {
+      if (INSN_P (insn))
+        {
+	  unsigned int uid = INSN_UID (insn);
+	  struct df_ref **def_rec;
+	  for (def_rec = DF_INSN_UID_DEFS (uid); *def_rec; def_rec++)
+	    {
+	      struct df_ref *def = *def_rec;
+	      bitmap_set_bit (merge_set, DF_REF_REGNO (def));
+	    }
+        }
+    }
+
+  /* We can perform the transformation if
+     MERGE_SET & df_get_live_in (else_bb)
+     is empty.  */
+
+  bitmap_and (tmp, df_get_live_in (else_bb), merge_set);
+  fail = bitmap_count_bits (tmp) != 0;
+
+  FREE_REG_SET (tmp);
+  FREE_REG_SET (merge_set);
+
+  if (fail)
+    return TRUE;
+  return FALSE;
+}
+
 /* Used by the code above to perform the actual rtl transformations.
    Return TRUE if successful.
 
@@ -4045,6 +4874,8 @@
 {
   basic_block bb;
   int pass;
+  int block_num, nblocks;
+  int *worklist;
 
   if (optimize == 1)
     {
@@ -4066,6 +4897,9 @@
 
   df_set_flags (DF_LR_RUN_DCE);
 
+  aggressive_cmov = (flag_aggressive_cmov
+		     && !(HAVE_conditional_execution && reload_completed));
+
   /* Go through each of the basic blocks looking for things to convert.  If we
      have conditional execution, we make multiple passes to allow us to handle
      IF-THEN{-ELSE} blocks within other IF-THEN{-ELSE} blocks.  */
@@ -4083,13 +4917,35 @@
 	fprintf (dump_file, "\n\n========== Pass %d ==========\n", pass);
 #endif
 
-      FOR_EACH_BB (bb)
+      if (aggressive_cmov)
 	{
-          basic_block new_bb;
-          while (!df_get_bb_dirty (bb) 
-                 && (new_bb = find_if_header (bb, pass)) != NULL)
-            bb = new_bb;
+	  /* For aggressive conditional move, walk the basic blocks in
+	     post order rather than just normal order.  */
+	  worklist =
+	    (int *) xmalloc ((n_basic_blocks - NUM_FIXED_BLOCKS) *
+			     sizeof (int));
+	  post_order_compute (worklist, false, false);
+	  for (block_num = 0, nblocks = n_basic_blocks - NUM_FIXED_BLOCKS;
+	       block_num < nblocks; block_num++)
+	    {
+	      basic_block new_bb;
+	      bb = BASIC_BLOCK (worklist[block_num]);
+	      while (!df_get_bb_dirty (bb)
+		     && (new_bb = find_if_header (bb, pass)) != NULL)
+		bb = new_bb;
+	    }
+	  free (worklist);
 	}
+      else
+	{
+	  FOR_EACH_BB (bb)
+	    {
+	      basic_block new_bb;
+	      while (!df_get_bb_dirty (bb)
+		     && (new_bb = find_if_header (bb, pass)) != NULL)
+		bb = new_bb;
+	    }
+	}
 
 #ifdef IFCVT_MULTIPLE_DUMPS
       if (dump_file && cond_exec_changed_p)
@@ -4100,6 +4956,10 @@
 	    print_rtl_with_bb (dump_file, get_insns ());
 	}
 #endif
+      if (aggressive_cmov && cond_exec_changed_p)
+	{
+	  cleanup_cfg (CLEANUP_NO_INSN_DEL);
+	}
     }
   while (cond_exec_changed_p);
 
Index: gcc/dwarf2out.c
===================================================================
--- gcc/dwarf2out.c	(.../gcc-4_3-branch)	(revision 156795)
+++ gcc/dwarf2out.c	(.../cell-4_3-branch)	(revision 156810)
@@ -8914,6 +8914,9 @@
       add_AT_unsigned (mod_type_die, DW_AT_byte_size,
 		       simple_type_size_in_bits (type) / BITS_PER_UNIT);
       item_type = TREE_TYPE (type);
+      if (TYPE_ADDR_SPACE (item_type))
+	add_AT_unsigned (mod_type_die, DW_AT_address_class,
+			 TYPE_ADDR_SPACE (strip_array_types (item_type)));
     }
   else if (code == REFERENCE_TYPE)
     {
@@ -8921,6 +8924,9 @@
       add_AT_unsigned (mod_type_die, DW_AT_byte_size,
 		       simple_type_size_in_bits (type) / BITS_PER_UNIT);
       item_type = TREE_TYPE (type);
+      if (TYPE_ADDR_SPACE (item_type))
+	add_AT_unsigned (mod_type_die, DW_AT_address_class,
+			 TYPE_ADDR_SPACE (strip_array_types (item_type)));
     }
   else if (is_subrange_type (type))
     {
Index: gcc/expr.c
===================================================================
--- gcc/expr.c	(.../gcc-4_3-branch)	(revision 156795)
+++ gcc/expr.c	(.../cell-4_3-branch)	(revision 156810)
@@ -6904,17 +6904,22 @@
 		       enum expand_modifier modifier)
 {
   enum machine_mode rmode;
+  enum machine_mode addrmode;
   rtx result;
 
   /* Target mode of VOIDmode says "whatever's natural".  */
   if (tmode == VOIDmode)
     tmode = TYPE_MODE (TREE_TYPE (exp));
 
+  addrmode = Pmode;
+  if (OTHER_ADDR_SPACE_POINTER_TYPE_P (TREE_TYPE (exp)))
+    addrmode = targetm.addr_space_pointer_mode (TYPE_ADDR_SPACE (TREE_TYPE (exp)));
+
   /* We can get called with some Weird Things if the user does silliness
      like "(short) &a".  In that case, convert_memory_address won't do
      the right thing, so ignore the given target mode.  */
-  if (tmode != Pmode && tmode != ptr_mode)
-    tmode = Pmode;
+  if (tmode != addrmode && tmode != ptr_mode)
+    tmode = addrmode;
 
   result = expand_expr_addr_expr_1 (TREE_OPERAND (exp, 0), target,
 				    tmode, modifier);
@@ -7154,6 +7159,7 @@
   int ignore;
   tree context, subexp0, subexp1;
   bool reduce_bit_field = false;
+  rtx (*genfn) (rtx, rtx);
 #define REDUCE_BIT_FIELD(expr)	(reduce_bit_field && !ignore		  \
 				 ? reduce_to_bit_field_precision ((expr), \
 								  target, \
@@ -8126,6 +8132,27 @@
 	  return target;
 	}
 
+      /* Handle casts of pointers to/from address space qualified
+	 pointers.  */
+      if (OTHER_ADDR_SPACE_POINTER_TYPE_P (type)
+	  && GENERIC_ADDR_SPACE_POINTER_TYPE_P (TREE_TYPE (TREE_OPERAND (exp, 0))))
+	{
+	  rtx reg = gen_reg_rtx (TYPE_MODE (type));
+	  op0 = expand_expr (TREE_OPERAND (exp, 0), NULL_RTX, VOIDmode, modifier);
+	  genfn = targetm.addr_space_conversion_rtl (0, 1);
+	  emit_insn (genfn (reg, op0));
+	  return reg;
+	}
+      else if (GENERIC_ADDR_SPACE_POINTER_TYPE_P (type)
+	       && (OTHER_ADDR_SPACE_POINTER_TYPE_P (TREE_TYPE (TREE_OPERAND (exp, 0)))))
+	{
+	  rtx reg = gen_reg_rtx (Pmode);
+	  op0 = expand_expr (TREE_OPERAND (exp, 0), NULL_RTX, VOIDmode, modifier);
+	  genfn = targetm.addr_space_conversion_rtl (1, 0);
+	  emit_insn (genfn (reg, op0));
+	  return reg;
+	}
+
       if (mode == TYPE_MODE (TREE_TYPE (TREE_OPERAND (exp, 0))))
 	{
 	  op0 = expand_expr (TREE_OPERAND (exp, 0), target, VOIDmode,
Index: gcc/predict.c
===================================================================
--- gcc/predict.c	(.../gcc-4_3-branch)	(revision 156795)
+++ gcc/predict.c	(.../cell-4_3-branch)	(revision 156810)
@@ -1054,7 +1054,13 @@
 	      && (fndecl = get_callee_fndecl (call))
 	      && DECL_BUILT_IN_CLASS (fndecl) == BUILT_IN_NORMAL
 	      && DECL_FUNCTION_CODE (fndecl) == BUILT_IN_EXPECT
-	      && call_expr_nargs (call) == 2)
+	      && call_expr_nargs (call) == 2
+#ifdef HAVE_builtin_expect
+	      /* When the target provides a builtin_expect rtl pattern
+	         keep the calls that aren't constant. */
+	      && TREE_CONSTANT (CALL_EXPR_ARG (call, 1))
+#endif
+	      )
 	    {
 	      GIMPLE_STMT_OPERAND (stmt, 1) = CALL_EXPR_ARG (call, 0);
 	      update_stmt (stmt);
Index: gcc/recog.c
===================================================================
--- gcc/recog.c	(.../gcc-4_3-branch)	(revision 156795)
+++ gcc/recog.c	(.../cell-4_3-branch)	(revision 156810)
@@ -94,6 +94,9 @@
 /* Nonzero after thread_prologue_and_epilogue_insns has run.  */
 int epilogue_completed;
 
+/* Nonzero after split0 pass has run.  */
+int split0_completed;
+
 /* Initialize data used by the function `recog'.
    This must be called once in the compilation of a function
    before any insn recognition may be done in the function.  */
@@ -3508,4 +3511,39 @@
   0                                     /* letter */
 };
 
+static bool
+gate_handle_split_before_cse2 (void)
+{
+#ifdef SPLIT_BEFORE_CSE2
+  return SPLIT_BEFORE_CSE2;
+#else
+  return 0;
+#endif
+}
 
+static unsigned int
+rest_of_handle_split_before_cse2 (void)
+{
+  split_all_insns_noflow ();
+  split0_completed = 1;
+  return 0;
+}
+
+struct tree_opt_pass pass_split_before_cse2 =
+{
+  "split0",                             /* name */
+  gate_handle_split_before_cse2,        /* gate */
+  rest_of_handle_split_before_cse2,     /* execute */
+  NULL,                                 /* sub */
+  NULL,                                 /* next */
+  0,                                    /* static_pass_number */
+  0,                                    /* tv_id */
+  0,                                    /* properties_required */
+  0,                                    /* properties_provided */
+  0,                                    /* properties_destroyed */
+  0,                                    /* todo_flags_start */
+  TODO_dump_func,                       /* todo_flags_finish */
+  0                                     /* letter */
+};
+
+
Index: gcc/c-decl.c
===================================================================
--- gcc/c-decl.c	(.../gcc-4_3-branch)	(revision 156795)
+++ gcc/c-decl.c	(.../cell-4_3-branch)	(revision 156810)
@@ -61,6 +61,7 @@
 #include "except.h"
 #include "langhooks-def.h"
 #include "pointer-set.h"
+#include "targhooks.h"
 
 /* In grokdeclarator, distinguish syntactic contexts of declarators.  */
 enum decl_context
@@ -2925,7 +2926,8 @@
 	  else if (!declspecs->tag_defined_p
 		   && (declspecs->const_p
 		       || declspecs->volatile_p
-		       || declspecs->restrict_p))
+		       || declspecs->restrict_p
+		       || declspecs->address_space))
 	    {
 	      if (warned != 1)
 		pedwarn ("empty declaration with type qualifier "
@@ -2994,7 +2996,8 @@
 
   if (!warned && !in_system_header && (declspecs->const_p
 				       || declspecs->volatile_p
-				       || declspecs->restrict_p))
+				       || declspecs->restrict_p
+				       || declspecs->address_space))
     {
       warning (0, "useless type qualifier in empty declaration");
       warned = 2;
@@ -3017,7 +3020,8 @@
 {
   int quals = ((specs->const_p ? TYPE_QUAL_CONST : 0)
 	       | (specs->volatile_p ? TYPE_QUAL_VOLATILE : 0)
-	       | (specs->restrict_p ? TYPE_QUAL_RESTRICT : 0));
+	       | (specs->restrict_p ? TYPE_QUAL_RESTRICT : 0)
+	       | (ENCODE_QUAL_ADDR_SPACE (specs->address_space)));
   gcc_assert (!specs->type
 	      && !specs->decl_attr
 	      && specs->typespec_word == cts_none
@@ -3276,6 +3280,23 @@
       && !flag_no_common)
     DECL_COMMON (decl) = 1;
 
+  if (TREE_CODE (decl) == VAR_DECL
+      && TYPE_ADDR_SPACE (strip_array_types (TREE_TYPE (decl)))
+      && (declspecs->storage_class == csc_static
+	  || (declspecs->storage_class == csc_none
+	      && !current_function_scope)
+	  || (declspecs->storage_class == csc_extern
+	      && initialized)))
+    {
+      static tree ea_name;
+
+      if (!targetm.have_named_sections)
+	error ("%<__ea%> definitions not supported for %qD", decl);
+      if (!ea_name)
+	ea_name = build_string (4, "._ea");
+      DECL_SECTION_NAME (decl) = ea_name;
+    }
+
   /* Set attributes here so if duplicate decl, will have proper attributes.  */
   decl_attributes (&decl, attributes, 0);
 
@@ -3983,6 +4004,7 @@
   int constp;
   int restrictp;
   int volatilep;
+  int addr_space_p;
   int type_quals = TYPE_UNQUALIFIED;
   const char *name, *orig_name;
   tree typedef_type = 0;
@@ -4090,7 +4112,7 @@
      declaration contains the `const'.  A third possibility is that
      there is a type qualifier on the element type of a typedefed
      array type, in which case we should extract that qualifier so
-     that c_apply_type_quals_to_decls receives the full list of
+     that c_apply_type_quals_to_decl receives the full list of
      qualifiers to work with (C90 is not entirely clear about whether
      duplicate qualifiers should be diagnosed in this case, but it
      seems most appropriate to do so).  */
@@ -4098,6 +4120,7 @@
   constp = declspecs->const_p + TYPE_READONLY (element_type);
   restrictp = declspecs->restrict_p + TYPE_RESTRICT (element_type);
   volatilep = declspecs->volatile_p + TYPE_VOLATILE (element_type);
+  addr_space_p = (declspecs->address_space > 0) + (TYPE_ADDR_SPACE (element_type) > 0);
   if (pedantic && !flag_isoc99)
     {
       if (constp > 1)
@@ -4106,16 +4129,79 @@
 	pedwarn ("duplicate %<restrict%>");
       if (volatilep > 1)
 	pedwarn ("duplicate %<volatile%>");
+      if (addr_space_p > 1)
+	pedwarn ("duplicate %qs",
+		 targetm.addr_space_name (TYPE_ADDR_SPACE (element_type)));
+
     }
   if (!flag_gen_aux_info && (TYPE_QUALS (element_type)))
     type = TYPE_MAIN_VARIANT (type);
   type_quals = ((constp ? TYPE_QUAL_CONST : 0)
 		| (restrictp ? TYPE_QUAL_RESTRICT : 0)
-		| (volatilep ? TYPE_QUAL_VOLATILE : 0));
+		| (volatilep ? TYPE_QUAL_VOLATILE : 0)
+		| (addr_space_p ?
+		   ENCODE_QUAL_ADDR_SPACE (declspecs->address_space) : 0));
 
   /* Warn about storage classes that are invalid for certain
      kinds of declarations (parameters, typenames, etc.).  */
 
+  if (((declarator->kind == cdk_pointer
+	&& (DECODE_QUAL_ADDR_SPACE (declarator->u.pointer_quals)) != 0)
+       || addr_space_p)
+      && targetm.addr_space_name == default_addr_space_name)
+    {
+      /* A mere warning is sure to result in improper semantics
+	 at runtime.  Don't bother to allow this to compile.  */
+      error ("extended address space not supported for this target");
+      return 0;
+    }
+  
+  if (declarator->kind == cdk_pointer
+      ? (DECODE_QUAL_ADDR_SPACE (declarator->u.pointer_quals)) != 0
+      : addr_space_p)
+    {
+      const char *addrspace_name;
+
+      addrspace_name = (declspecs->address_space)
+	? targetm.addr_space_name (declspecs->address_space)
+        : targetm.addr_space_name (DECODE_QUAL_ADDR_SPACE (declarator->u.pointer_quals));
+
+      if (decl_context == NORMAL)
+	{
+	  if (declarator->kind == cdk_function)
+	    error ("%qs specified for function %qs", addrspace_name, name);
+	  else
+	    {
+	      switch (storage_class)
+		{
+		case csc_auto:
+		  error ("%qs combined with %<auto%> qualifier for %qs", addrspace_name, name);
+		  break;
+		case csc_register:
+		  error ("%qs combined with %<register%> qualifier for %qs", addrspace_name, name);
+		  break;
+		case csc_none:
+		  if (current_function_scope)
+ 		    {
+ 		      error ("%<__ea%> specified for auto variable %qs", name);
+ 		      break;
+ 		    }
+		  break;
+		case csc_static:
+		  break;
+		case csc_extern:
+		  break;
+		case csc_typedef:
+		  break;
+		}
+	    }
+	}
+      else if (decl_context == PARM && declarator->kind != cdk_array)
+	error ("%qs specified for parameter %qs", addrspace_name, name);
+      else if (decl_context == FIELD)
+	error ("%qs specified for structure field %qs", addrspace_name, name);
+    }
+
   if (funcdef_flag
       && (threadp
 	  || storage_class == csc_auto
@@ -4573,9 +4659,15 @@
 	    /* Merge any constancy or volatility into the target type
 	       for the pointer.  */
 
-	    if (pedantic && TREE_CODE (type) == FUNCTION_TYPE
-		&& type_quals)
-	      pedwarn ("ISO C forbids qualified function types");
+	    if (TREE_CODE (type) == FUNCTION_TYPE)
+	      {
+		if (pedantic && type_quals)
+		  pedwarn ("ISO C forbids qualified function types");
+	      }
+#ifdef TARGET_ALL_EA
+	    else if (TARGET_ALL_EA)
+	      type_quals |= ENCODE_QUAL_ADDR_SPACE (1);
+#endif
 	    if (type_quals)
 	      type = c_build_qualified_type (type, type_quals);
 	    size_varies = 0;
@@ -4720,6 +4812,13 @@
 	tree type_as_written;
 	tree promoted_type;
 
+#ifdef TARGET_ALL_EA
+	if (TARGET_ALL_EA
+	    && (POINTER_TYPE_P (type)
+		|| TREE_CODE (type) == ARRAY_TYPE))
+	  type_quals |= ENCODE_QUAL_ADDR_SPACE (1);
+#endif
+
 	/* A parameter declared as an array of T is really a pointer to T.
 	   One declared as a function is really a pointer to a function.  */
 
@@ -4905,6 +5004,14 @@
 	/* An uninitialized decl with `extern' is a reference.  */
 	int extern_ref = !initialized && storage_class == csc_extern;
 
+#ifdef TARGET_ALL_EA
+	if (TARGET_ALL_EA
+	    && (storage_class == csc_static
+		|| storage_class == csc_extern
+		|| (storage_class == csc_none && !current_function_scope)))
+	  type_quals |= ENCODE_QUAL_ADDR_SPACE (1);
+#endif
+  
 	type = c_build_qualified_type (type, type_quals);
 
 	/* C99 6.2.2p7: It is invalid (compile-time undefined
@@ -7159,9 +7266,23 @@
   ret->volatile_p = false;
   ret->restrict_p = false;
   ret->saturating_p = false;
+  ret->address_space = 0;
   return ret;
 }
 
+struct c_declspecs *
+declspecs_add_addrspace (struct c_declspecs *specs, tree addrspace)
+{
+  specs->non_sc_seen_p = true;
+  specs->declspecs_seen_p = true;
+
+  if (specs->address_space > 0)
+    pedwarn ("duplicate %qs", targetm.addr_space_name (specs->address_space));
+
+  specs->address_space = targetm.addr_space_number (addrspace);
+  return specs;
+}
+
 /* Add the type qualifier QUAL to the declaration specifiers SPECS,
    returning SPECS.  */
 
Index: gcc/c-pretty-print.c
===================================================================
--- gcc/c-pretty-print.c	(.../gcc-4_3-branch)	(revision 156795)
+++ gcc/c-pretty-print.c	(.../cell-4_3-branch)	(revision 156810)
@@ -29,6 +29,8 @@
 #include "c-tree.h"
 #include "tree-iterator.h"
 #include "diagnostic.h"
+#include "target.h"
+#include "target-def.h"
 
 /* The pretty-printer code is primarily designed to closely follow
    (GNU) C and C++ grammars.  That is to be contrasted with spaghetti
@@ -220,8 +222,12 @@
        const
        restrict                              -- C99
        __restrict__                          -- GNU C
-       volatile    */
+       address-space-qualifier		     -- GNU C
+       volatile
 
+   address-space-qualifier:
+       identifier			     -- GNU C  */
+
 void
 pp_c_type_qualifier_list (c_pretty_printer *pp, tree t)
 {
@@ -240,6 +246,12 @@
     pp_c_cv_qualifier (pp, "volatile");
   if (qualifiers & TYPE_QUAL_RESTRICT)
     pp_c_cv_qualifier (pp, flag_isoc99 ? "restrict" : "__restrict__");
+
+  if (TYPE_ADDR_SPACE (t))
+    {
+      const char *as = targetm.addr_space_name (TYPE_ADDR_SPACE (t));
+      pp_c_identifier (pp, as);
+    }
 }
 
 /* pointer:
Index: gcc/langhooks.c
===================================================================
--- gcc/langhooks.c	(.../gcc-4_3-branch)	(revision 156795)
+++ gcc/langhooks.c	(.../cell-4_3-branch)	(revision 156810)
@@ -284,7 +284,7 @@
 int
 lhd_tree_dump_type_quals (const_tree t)
 {
-  return TYPE_QUALS (t);
+  return TYPE_QUALS (CONST_CAST_TREE (t));
 }
 
 /* lang_hooks.expr_size: Determine the size of the value of an expression T
Index: gcc/print-rtl.c
===================================================================
--- gcc/print-rtl.c	(.../gcc-4_3-branch)	(revision 156795)
+++ gcc/print-rtl.c	(.../cell-4_3-branch)	(revision 156810)
@@ -556,6 +556,9 @@
       if (MEM_ALIGN (in_rtx) != 1)
 	fprintf (outfile, " A%u", MEM_ALIGN (in_rtx));
 
+      if (MEM_ADDR_SPACE (in_rtx))
+	fprintf (outfile, " AS%u", MEM_ADDR_SPACE (in_rtx));
+
       fputc (']', outfile);
       break;
 
Index: gcc/c-typeck.c
===================================================================
--- gcc/c-typeck.c	(.../gcc-4_3-branch)	(revision 156795)
+++ gcc/c-typeck.c	(.../cell-4_3-branch)	(revision 156810)
@@ -930,7 +930,7 @@
 
   /* Qualifiers must match. C99 6.7.3p9 */
 
-  if (TYPE_QUALS (t1) != TYPE_QUALS (t2))
+  if (TYPE_QUALS (CONST_CAST_TREE (t1)) != TYPE_QUALS (CONST_CAST_TREE (t2)))
     return 0;
 
   /* Allow for two different type nodes which have essentially the same
@@ -2042,6 +2042,17 @@
   return error_mark_node;
 }
 
+/* Like c_mark_addressable but don't check register qualifier.  */
+static void 
+mark_addressable_vector (tree x)
+{   
+  while (handled_component_p (x))
+    x = TREE_OPERAND (x, 0);
+  if (TREE_CODE (x) != VAR_DECL && TREE_CODE (x) != PARM_DECL)
+    return ;
+  TREE_ADDRESSABLE (x) = 1;
+}  
+
 /* This handles expressions of the form "a[i]", which denotes
    an array reference.
 
@@ -2049,7 +2060,10 @@
    If A is a variable or a member, we generate a primitive ARRAY_REF.
    This avoids forcing the array out of registers, and can work on
    arrays that are not lvalues (for example, members of structures returned
-   by functions).  */
+   by functions).  
+   
+   For vector types, allow vector[i] but not i[vector], and create
+   *(((type*)&vectortype) + i) for the expression.  */
 
 tree
 build_array_ref (tree array, tree index)
@@ -2060,13 +2074,15 @@
     return error_mark_node;
 
   if (TREE_CODE (TREE_TYPE (array)) != ARRAY_TYPE
-      && TREE_CODE (TREE_TYPE (array)) != POINTER_TYPE)
+      && TREE_CODE (TREE_TYPE (array)) != POINTER_TYPE
+      /* Allow vector[index] but not index[vector].  */
+      && TREE_CODE (TREE_TYPE (array)) != VECTOR_TYPE)
     {
       tree temp;
       if (TREE_CODE (TREE_TYPE (index)) != ARRAY_TYPE
 	  && TREE_CODE (TREE_TYPE (index)) != POINTER_TYPE)
 	{
-	  error ("subscripted value is neither array nor pointer");
+	  error ("subscripted value is not an array, a pointer, or a vector");
 	  return error_mark_node;
 	}
       temp = array;
@@ -2096,6 +2112,22 @@
   index = default_conversion (index);
 
   gcc_assert (TREE_CODE (TREE_TYPE (index)) == INTEGER_TYPE);
+  
+  /* For vector[index], convert the vector to a pointer of the underlying
+     type. */
+  if (TREE_CODE (TREE_TYPE (array)) == VECTOR_TYPE)
+    {
+      tree type = TREE_TYPE (array);
+      tree type1;
+      /* Mark the vector as addressable but ignore the
+	 register storage class.  */      
+      mark_addressable_vector (array);
+      type = build_qualified_type (TREE_TYPE (type), TYPE_QUALS (type));
+      type = build_pointer_type (type);
+      type1 = build_pointer_type (TREE_TYPE (array));
+      array = build1 (ADDR_EXPR, type1, array);
+      array = convert (type, array);
+    }
 
   if (TREE_CODE (TREE_TYPE (array)) == ARRAY_TYPE)
     {
@@ -8183,6 +8215,16 @@
 		  && TREE_CODE (tt1) == FUNCTION_TYPE)
 		pedwarn ("ISO C forbids comparison of %<void *%>"
 			 " with function pointer");
+
+ 	      /* If this operand is a pointer into another address
+ 		 space, make the result of the comparison such a
+ 		 pointer also.  */
+ 	      if (OTHER_ADDR_SPACE_POINTER_TYPE_P (type0))
+ 		{
+ 		  int qual = ENCODE_QUAL_ADDR_SPACE (TYPE_ADDR_SPACE (TREE_TYPE (type0)));
+ 		  result_type = build_pointer_type
+ 		    (build_qualified_type (void_type_node, qual));
+ 		}
 	    }
 	  else if (VOID_TYPE_P (tt1))
 	    {
@@ -8190,6 +8232,16 @@
 		  && TREE_CODE (tt0) == FUNCTION_TYPE)
 		pedwarn ("ISO C forbids comparison of %<void *%>"
 			 " with function pointer");
+
+ 	      /* If this operand is a pointer into another address
+ 		 space, make the result of the comparison such a
+		 pointer also.  */
+ 	      if (OTHER_ADDR_SPACE_POINTER_TYPE_P (type1))
+ 		{
+ 		  int qual = ENCODE_QUAL_ADDR_SPACE (TYPE_ADDR_SPACE (TREE_TYPE (type1)));
+ 		  result_type = build_pointer_type
+ 		    (build_qualified_type (void_type_node, qual));
+ 		}
 	    }
 	  else
 	    /* Avoid warning about the volatile ObjC EH puts on decls.  */
Index: gcc/coretypes.h
===================================================================
--- gcc/coretypes.h	(.../gcc-4_3-branch)	(revision 156795)
+++ gcc/coretypes.h	(.../cell-4_3-branch)	(revision 156810)
@@ -60,9 +60,11 @@
 
 /* Provide forward struct declaration so that we don't have to include
    all of cpplib.h whenever a random prototype includes a pointer.
-   Note that the cpp_reader typedef remains part of cpplib.h.  */
+   Note that the cpp_reader and cpp_token typedefs remain part of
+   cpplib.h.  */
 
 struct cpp_reader;
+struct cpp_token;
 
 /* The thread-local storage model associated with a given VAR_DECL
    or SYMBOL_REF.  This isn't used much, but both trees and RTL refer
Index: gcc/emit-rtl.c
===================================================================
--- gcc/emit-rtl.c	(.../gcc-4_3-branch)	(revision 156795)
+++ gcc/emit-rtl.c	(.../cell-4_3-branch)	(revision 156810)
@@ -189,7 +189,7 @@
 static hashval_t mem_attrs_htab_hash (const void *);
 static int mem_attrs_htab_eq (const void *, const void *);
 static mem_attrs *get_mem_attrs (alias_set_type, tree, rtx, rtx, unsigned int,
-				 enum machine_mode);
+				 unsigned char, enum machine_mode);
 static hashval_t reg_attrs_htab_hash (const void *);
 static int reg_attrs_htab_eq (const void *, const void *);
 static reg_attrs *get_reg_attrs (tree, int);
@@ -317,7 +317,7 @@
 
 static mem_attrs *
 get_mem_attrs (alias_set_type alias, tree expr, rtx offset, rtx size,
-	       unsigned int align, enum machine_mode mode)
+	       unsigned int align, unsigned char addrspace, enum machine_mode mode)
 {
   mem_attrs attrs;
   void **slot;
@@ -337,6 +337,7 @@
   attrs.offset = offset;
   attrs.size = size;
   attrs.align = align;
+  attrs.addrspace = addrspace;
 
   slot = htab_find_slot (mem_attrs_htab, &attrs, INSERT);
   if (*slot == 0)
@@ -1739,7 +1740,8 @@
 
   /* Now set the attributes we computed above.  */
   MEM_ATTRS (ref)
-    = get_mem_attrs (alias, expr, offset, size, align, GET_MODE (ref));
+    = get_mem_attrs (alias, expr, offset, size, align,
+		     TYPE_ADDR_SPACE (strip_array_types (type)), GET_MODE (ref));
 
   /* If this is already known to be a scalar or aggregate, we are done.  */
   if (MEM_IN_STRUCT_P (ref) || MEM_SCALAR_P (ref))
@@ -1767,7 +1769,7 @@
   MEM_ATTRS (mem)
     = get_mem_attrs (MEM_ALIAS_SET (mem), REG_EXPR (reg),
 		     GEN_INT (REG_OFFSET (reg)),
-		     MEM_SIZE (mem), MEM_ALIGN (mem), GET_MODE (mem));
+		     MEM_SIZE (mem), MEM_ALIGN (mem), MEM_ADDR_SPACE (mem), GET_MODE (mem));
 }
 
 /* Set the alias set of MEM to SET.  */
@@ -1781,17 +1783,27 @@
 #endif
 
   MEM_ATTRS (mem) = get_mem_attrs (set, MEM_EXPR (mem), MEM_OFFSET (mem),
-				   MEM_SIZE (mem), MEM_ALIGN (mem),
+				   MEM_SIZE (mem), MEM_ALIGN (mem), MEM_ADDR_SPACE (mem),
 				   GET_MODE (mem));
 }
 
+/* Set the address space of MEM to ADDRSPACE (target-defined).  */
+
+void
+set_mem_addr_space (rtx mem, unsigned char addrspace)
+{
+  MEM_ATTRS (mem) = get_mem_attrs (MEM_ALIAS_SET (mem), MEM_EXPR (mem),
+				   MEM_OFFSET (mem), MEM_SIZE (mem), MEM_ALIGN (mem),
+				   addrspace, GET_MODE (mem));
+}
+
 /* Set the alignment of MEM to ALIGN bits.  */
 
 void
 set_mem_align (rtx mem, unsigned int align)
 {
   MEM_ATTRS (mem) = get_mem_attrs (MEM_ALIAS_SET (mem), MEM_EXPR (mem),
-				   MEM_OFFSET (mem), MEM_SIZE (mem), align,
+				   MEM_OFFSET (mem), MEM_SIZE (mem), align, MEM_ADDR_SPACE (mem),
 				   GET_MODE (mem));
 }
 
@@ -1802,7 +1814,7 @@
 {
   MEM_ATTRS (mem)
     = get_mem_attrs (MEM_ALIAS_SET (mem), expr, MEM_OFFSET (mem),
-		     MEM_SIZE (mem), MEM_ALIGN (mem), GET_MODE (mem));
+		     MEM_SIZE (mem), MEM_ALIGN (mem), MEM_ADDR_SPACE (mem), GET_MODE (mem));
 }
 
 /* Set the offset of MEM to OFFSET.  */
@@ -1811,7 +1823,7 @@
 set_mem_offset (rtx mem, rtx offset)
 {
   MEM_ATTRS (mem) = get_mem_attrs (MEM_ALIAS_SET (mem), MEM_EXPR (mem),
-				   offset, MEM_SIZE (mem), MEM_ALIGN (mem),
+				   offset, MEM_SIZE (mem), MEM_ALIGN (mem), MEM_ADDR_SPACE (mem),
 				   GET_MODE (mem));
 }
 
@@ -1821,7 +1833,7 @@
 set_mem_size (rtx mem, rtx size)
 {
   MEM_ATTRS (mem) = get_mem_attrs (MEM_ALIAS_SET (mem), MEM_EXPR (mem),
-				   MEM_OFFSET (mem), size, MEM_ALIGN (mem),
+				   MEM_OFFSET (mem), size, MEM_ALIGN (mem), MEM_ADDR_SPACE (mem),
 				   GET_MODE (mem));
 }
 
@@ -1889,7 +1901,7 @@
     }
 
   MEM_ATTRS (new)
-    = get_mem_attrs (MEM_ALIAS_SET (memref), 0, 0, size, align, mmode);
+    = get_mem_attrs (MEM_ALIAS_SET (memref), 0, 0, size, align, MEM_ADDR_SPACE (memref), mmode);
 
   return new;
 }
@@ -1956,7 +1968,8 @@
     size = plus_constant (MEM_SIZE (memref), -offset);
 
   MEM_ATTRS (new) = get_mem_attrs (MEM_ALIAS_SET (memref), MEM_EXPR (memref),
-				   memoffset, size, memalign, GET_MODE (new));
+				   memoffset, size, memalign, MEM_ADDR_SPACE (memref),
+				   GET_MODE (new));
 
   /* At some point, we should validate that this offset is within the object,
      if all the appropriate values are known.  */
@@ -2014,7 +2027,7 @@
   MEM_ATTRS (new)
     = get_mem_attrs (MEM_ALIAS_SET (memref), MEM_EXPR (memref), 0, 0,
 		     MIN (MEM_ALIGN (memref), pow2 * BITS_PER_UNIT),
-		     GET_MODE (new));
+ 		     MEM_ADDR_SPACE (memref), GET_MODE (new));
   return new;
 }
 
@@ -2118,7 +2131,7 @@
   /* ??? Maybe use get_alias_set on any remaining expression.  */
 
   MEM_ATTRS (new) = get_mem_attrs (0, expr, memoffset, GEN_INT (size),
-				   MEM_ALIGN (new), mode);
+ 				   MEM_ALIGN (new), MEM_ADDR_SPACE (new), mode);
 
   return new;
 }
Index: gcc/emit-rtl.h
===================================================================
--- gcc/emit-rtl.h	(.../gcc-4_3-branch)	(revision 156795)
+++ gcc/emit-rtl.h	(.../cell-4_3-branch)	(revision 156810)
@@ -26,6 +26,9 @@
 /* Set the alignment of MEM to ALIGN bits.  */
 extern void set_mem_align (rtx, unsigned int);
 
+/* Set the address space of MEM to ADDRSPACE.  */
+extern void set_mem_addr_space (rtx, unsigned char);
+
 /* Set the expr for MEM to EXPR.  */
 extern void set_mem_expr (rtx, tree);
 
Index: gcc/explow.c
===================================================================
--- gcc/explow.c	(.../gcc-4_3-branch)	(revision 156795)
+++ gcc/explow.c	(.../cell-4_3-branch)	(revision 156810)
@@ -413,7 +413,8 @@
 {
   rtx oldx = x;
 
-  x = convert_memory_address (Pmode, x);
+  if (MEM_P (x) && !targetm.valid_pointer_mode (GET_MODE (x)))
+    x = convert_memory_address (Pmode, x);
 
   /* By passing constant addresses through registers
      we get a chance to cse them.  */
@@ -483,6 +484,8 @@
 
       /* Last resort: copy the value to a register, since
 	 the register is a valid address.  */
+      else if (targetm.valid_pointer_mode (GET_MODE (x)))
+	x = force_reg (GET_MODE (x), x);
       else
 	x = force_reg (Pmode, x);
     }
Index: gcc/print-tree.c
===================================================================
--- gcc/print-tree.c	(.../gcc-4_3-branch)	(revision 156795)
+++ gcc/print-tree.c	(.../cell-4_3-branch)	(revision 156810)
@@ -29,6 +29,8 @@
 #include "ggc.h"
 #include "langhooks.h"
 #include "tree-iterator.h"
+#include "target.h"
+#include "target-def.h"
 
 /* Define the hash table of nodes already seen.
    Such nodes are not repeated; brief cross-references are used.  */
@@ -590,6 +592,11 @@
       if (TYPE_RESTRICT (node))
 	fputs (" restrict", file);
 
+      /* FIXME: Use a target hook here to translate the address space
+	 number.  */
+      if (TYPE_ADDR_SPACE (node))
+	fputs (" __ea", file);
+
       if (TYPE_LANG_FLAG_0 (node))
 	fputs (" type_0", file);
       if (TYPE_LANG_FLAG_1 (node))
Index: gcc/common.opt
===================================================================
--- gcc/common.opt	(.../gcc-4_3-branch)	(revision 156795)
+++ gcc/common.opt	(.../cell-4_3-branch)	(revision 156810)
@@ -259,6 +259,10 @@
 fabi-version=
 Common Joined UInteger Var(flag_abi_version) Init(2)
 
+faggressive-cmov
+Common Report Var(flag_aggressive_cmov) Init(0)
+Perform aggressive speculative branch elimination using cmov instructions
+
 falign-functions
 Common Report Var(align_functions,0)
 Align the start of functions
Index: gcc/varasm.c
===================================================================
--- gcc/varasm.c	(.../gcc-4_3-branch)	(revision 156795)
+++ gcc/varasm.c	(.../cell-4_3-branch)	(revision 156810)
@@ -1250,6 +1250,7 @@
   const char *name = 0;
   int reg_number;
   rtx x;
+  enum machine_mode addrmode;
 
   /* Check that we are not being given an automatic variable.  */
   gcc_assert (TREE_CODE (decl) != PARM_DECL
@@ -1394,7 +1395,13 @@
   if (use_object_blocks_p () && use_blocks_for_decl_p (decl))
     x = create_block_symbol (name, get_block_for_decl (decl), -1);
   else
-    x = gen_rtx_SYMBOL_REF (Pmode, name);
+    {
+      addrmode = (TREE_TYPE (decl) == error_mark_node)
+	? Pmode
+	: targetm.addr_space_pointer_mode
+	    (TYPE_ADDR_SPACE (strip_array_types (TREE_TYPE (decl))));
+      x = gen_rtx_SYMBOL_REF (addrmode, name);
+    }
   SYMBOL_REF_WEAK (x) = DECL_WEAK (decl);
   SET_SYMBOL_REF_DECL (x, decl);
 
@@ -6257,6 +6264,13 @@
   return (mode == ptr_mode || mode == Pmode);
 }
 
+enum machine_mode
+default_addr_space_pointer_mode (int addrspace)
+{
+  gcc_assert (addrspace == 0);
+  return ptr_mode;
+}
+
 /* Default function to output code that will globalize a label.  A
    target must define GLOBAL_ASM_OP or provide its own function to
    globalize a label.  */
Index: gcc/tree-ssa.c
===================================================================
--- gcc/tree-ssa.c	(.../gcc-4_3-branch)	(revision 156795)
+++ gcc/tree-ssa.c	(.../cell-4_3-branch)	(revision 156810)
@@ -973,6 +973,12 @@
 	  && TYPE_VOLATILE (TREE_TYPE (outer_type)))
 	return false;
 
+      /* Do not lose casts between pointers in different address
+	 spaces.  */
+      if (TYPE_ADDR_SPACE (TREE_TYPE (inner_type))
+	  != TYPE_ADDR_SPACE (TREE_TYPE (outer_type)))
+	return false;
+
       /* Do not lose casts between pointers with different
 	 TYPE_REF_CAN_ALIAS_ALL setting or alias sets.  */
       if ((TYPE_REF_CAN_ALIAS_ALL (inner_type)
@@ -1070,7 +1076,9 @@
      recursing though.  */
   if (POINTER_TYPE_P (inner_type)
       && POINTER_TYPE_P (outer_type)
-      && TREE_CODE (TREE_TYPE (outer_type)) == VOID_TYPE)
+      && TREE_CODE (TREE_TYPE (outer_type)) == VOID_TYPE
+      && GENERIC_ADDR_SPACE_POINTER_TYPE_P (inner_type)
+      && GENERIC_ADDR_SPACE_POINTER_TYPE_P (outer_type))
     return true;
 
   return useless_type_conversion_p_1 (outer_type, inner_type);
Index: gcc/target-def.h
===================================================================
--- gcc/target-def.h	(.../gcc-4_3-branch)	(revision 156795)
+++ gcc/target-def.h	(.../cell-4_3-branch)	(revision 156810)
@@ -437,6 +437,26 @@
 #define TARGET_VALID_POINTER_MODE default_valid_pointer_mode
 #endif
 
+#ifndef TARGET_ADDR_SPACE_POINTER_MODE
+#define TARGET_ADDR_SPACE_POINTER_MODE default_addr_space_pointer_mode
+#endif
+
+#ifndef TARGET_ADDR_SPACE_NAME
+#define TARGET_ADDR_SPACE_NAME default_addr_space_name
+#endif
+
+#ifndef TARGET_ADDR_SPACE_NUMBER
+#define TARGET_ADDR_SPACE_NUMBER default_addr_space_number
+#endif
+
+#ifndef TARGET_ADDR_SPACE_CONVERSION_RTL
+#define TARGET_ADDR_SPACE_CONVERSION_RTL default_addr_space_conversion_rtl
+#endif
+
+#ifndef TARGET_VALID_ADDR_SPACE
+#define TARGET_VALID_ADDR_SPACE hook_bool_const_tree_false
+#endif
+
 #ifndef TARGET_SCALAR_MODE_SUPPORTED_P
 #define TARGET_SCALAR_MODE_SUPPORTED_P default_scalar_mode_supported_p
 #endif
@@ -750,6 +770,11 @@
   TARGET_MIN_DIVISIONS_FOR_RECIP_MUL,		\
   TARGET_MODE_REP_EXTENDED,			\
   TARGET_VALID_POINTER_MODE,                    \
+  TARGET_ADDR_SPACE_POINTER_MODE,		\
+  TARGET_ADDR_SPACE_NAME,			\
+  TARGET_ADDR_SPACE_NUMBER,			\
+  TARGET_ADDR_SPACE_CONVERSION_RTL,		\
+  TARGET_VALID_ADDR_SPACE,			\
   TARGET_SCALAR_MODE_SUPPORTED_P,		\
   TARGET_VECTOR_MODE_SUPPORTED_P,               \
   TARGET_VECTOR_OPAQUE_P,			\
Index: gcc/rtl.h
===================================================================
--- gcc/rtl.h	(.../gcc-4_3-branch)	(revision 156795)
+++ gcc/rtl.h	(.../cell-4_3-branch)	(revision 156810)
@@ -146,6 +146,7 @@
   rtx offset;			/* Offset from start of DECL, as CONST_INT.  */
   rtx size;			/* Size in bytes, as a CONST_INT.  */
   unsigned int align;		/* Alignment of MEM in bits.  */
+  unsigned char addrspace;	/* Address space (0 for generic).  */
 } mem_attrs;
 
 /* Structure used to describe the attributes of a REG in similar way as
@@ -1196,6 +1197,10 @@
    RTX that is always a CONST_INT.  */
 #define MEM_OFFSET(RTX) (MEM_ATTRS (RTX) == 0 ? 0 : MEM_ATTRS (RTX)->offset)
 
+/* For a MEM rtx, the address space.  If 0, the MEM belongs to the
+   generic address space.  */
+#define MEM_ADDR_SPACE(RTX) (MEM_ATTRS (RTX) == 0 ? 0 : MEM_ATTRS (RTX)->addrspace)
+
 /* For a MEM rtx, the size in bytes of the MEM, if known, as an RTX that
    is always a CONST_INT.  */
 #define MEM_SIZE(RTX)							\
@@ -1996,6 +2001,9 @@
 /* Nonzero after thread_prologue_and_epilogue_insns has run.  */
 extern int epilogue_completed;
 
+/* Nonzero after the split0 pass has completed. */
+extern int split0_completed;
+
 /* Set to 1 while reload_as_needed is operating.
    Required by some machines to handle any generated moves differently.  */
 
Index: gcc/output.h
===================================================================
--- gcc/output.h	(.../gcc-4_3-branch)	(revision 156795)
+++ gcc/output.h	(.../cell-4_3-branch)	(revision 156810)
@@ -613,6 +613,7 @@
 extern void default_file_start (void);
 extern void file_end_indicate_exec_stack (void);
 extern bool default_valid_pointer_mode (enum machine_mode);
+extern enum machine_mode default_addr_space_pointer_mode (int);
 
 extern void default_elf_asm_output_external (FILE *file, tree,
 					     const char *);
Index: gcc/c-common.c
===================================================================
--- gcc/c-common.c	(.../gcc-4_3-branch)	(revision 156795)
+++ gcc/c-common.c	(.../cell-4_3-branch)	(revision 156810)
@@ -4120,18 +4120,6 @@
   return 1;
 }
 
-/* Recursively examines the array elements of TYPE, until a non-array
-   element type is found.  */
-
-tree
-strip_array_types (tree type)
-{
-  while (TREE_CODE (type) == ARRAY_TYPE)
-    type = TREE_TYPE (type);
-
-  return type;
-}
-
 /* Recursively remove any '*' or '&' operator from TYPE.  */
 tree
 strip_pointer_operator (tree t)
Index: gcc/c-common.h
===================================================================
--- gcc/c-common.h	(.../gcc-4_3-branch)	(revision 156795)
+++ gcc/c-common.h	(.../cell-4_3-branch)	(revision 156810)
@@ -180,6 +180,8 @@
     CTI_MAX
 };
 
+#define C_CPP_HASHNODE(id) \
+  (&(((struct c_common_identifier *) (id))->node))
 #define C_RID_CODE(id)	(((struct c_common_identifier *) (id))->node.rid_code)
 
 /* Identifier part common to the C front ends.  Inherits from
@@ -733,7 +735,6 @@
 extern void c_register_builtin_type (tree, const char*);
 extern bool c_promoting_integer_type_p (const_tree);
 extern int self_promoting_args_p (const_tree);
-extern tree strip_array_types (tree);
 extern tree strip_pointer_operator (tree);
 extern tree strip_pointer_or_array_types (tree);
 extern HOST_WIDE_INT c_common_to_target_charset (HOST_WIDE_INT);
Index: gcc/config.gcc
===================================================================
--- gcc/config.gcc	(.../gcc-4_3-branch)	(revision 156795)
+++ gcc/config.gcc	(.../cell-4_3-branch)	(revision 156810)
@@ -2489,7 +2489,7 @@
 spu-*-elf*)
 	tm_file="dbxelf.h elfos.h spu/spu-elf.h spu/spu.h"
 	tmake_file="spu/t-spu-elf"
-	extra_headers="spu_intrinsics.h spu_internals.h vmx2spu.h spu_mfcio.h vec_types.h"
+	extra_headers="spu_intrinsics.h spu_internals.h vmx2spu.h spu_mfcio.h vec_types.h spu_cache.h"
 	extra_modes=spu/spu-modes.def
 	c_target_objs="${c_target_objs} spu-c.o"
 	cxx_target_objs="${cxx_target_objs} spu-c.o"
Index: gcc/passes.c
===================================================================
--- gcc/passes.c	(.../gcc-4_3-branch)	(revision 156795)
+++ gcc/passes.c	(.../cell-4_3-branch)	(revision 156810)
@@ -715,6 +715,7 @@
 	}
       NEXT_PASS (pass_web);
       NEXT_PASS (pass_jump_bypass);
+      NEXT_PASS (pass_split_before_cse2);
       NEXT_PASS (pass_cse2);
       NEXT_PASS (pass_rtl_dse1);
       NEXT_PASS (pass_rtl_fwprop_addr);
Index: gcc/c-parser.c
===================================================================
--- gcc/c-parser.c	(.../gcc-4_3-branch)	(revision 156795)
+++ gcc/c-parser.c	(.../cell-4_3-branch)	(revision 156810)
@@ -238,6 +238,8 @@
   C_ID_TYPENAME,
   /* An identifier declared as an Objective-C class name.  */
   C_ID_CLASSNAME,
+  /* An address space identifier.  */
+  C_ID_ADDRSPACE,
   /* Not an identifier.  */
   C_ID_NONE
 } c_id_kind;
@@ -362,6 +364,11 @@
 		break;
 	      }
 	  }
+	else if (targetm.valid_addr_space (token->value))
+	  {
+	    token->id_kind = C_ID_ADDRSPACE;
+	    break;
+	  }
 	else if (c_dialect_objc ())
 	  {
 	    tree objc_interface_decl = objc_is_class_name (token->value);
@@ -463,6 +470,8 @@
 	{
 	case C_ID_ID:
 	  return false;
+	case C_ID_ADDRSPACE:
+	  return true;
 	case C_ID_TYPENAME:
 	  return true;
 	case C_ID_CLASSNAME:
@@ -533,6 +542,8 @@
 	{
 	case C_ID_ID:
 	  return false;
+	case C_ID_ADDRSPACE:
+	  return true;
 	case C_ID_TYPENAME:
 	  return true;
 	case C_ID_CLASSNAME:
@@ -1497,6 +1508,7 @@
      const
      restrict
      volatile
+     address-space-qualifier
 
    (restrict is new in C99.)
 
@@ -1505,6 +1517,12 @@
    declaration-specifiers:
      attributes declaration-specifiers[opt]
 
+   type-qualifier:
+     address-space
+
+   address-space:
+     identifier recognized by the target
+
    storage-class-specifier:
      __thread
 
@@ -1544,6 +1562,16 @@
 	{
 	  tree value = c_parser_peek_token (parser)->value;
 	  c_id_kind kind = c_parser_peek_token (parser)->id_kind;
+
+	  if (kind == C_ID_ADDRSPACE && !c_dialect_objc ())
+	    {
+	      declspecs_add_addrspace (specs, c_parser_peek_token (parser)->value);
+	      c_parser_consume_token (parser);
+	      attrs_ok = true;
+	      seen_type = true;
+	      continue;
+	    }
+
 	  /* This finishes the specifiers unless a type name is OK, it
 	     is declared as a type name and a type name hasn't yet
 	     been seen.  */
@@ -5603,6 +5631,14 @@
   finish_init ();
   maybe_warn_string_init (type, init);
 
+  if (type != error_mark_node
+      && TYPE_ADDR_SPACE (strip_array_types (type))
+      && current_function_decl)
+    {
+      error ("compound literal qualified by address-space qualifier");
+      type = error_mark_node;
+    }
+
   if (pedantic && !flag_isoc99)
     pedwarn ("%HISO C90 forbids compound literals", &start_loc);
   expr.value = build_compound_literal (type, init.value);
Index: gcc/config/spu/spu_cache.h
===================================================================
--- gcc/config/spu/spu_cache.h	(.../gcc-4_3-branch)	(revision 0)
+++ gcc/config/spu/spu_cache.h	(.../cell-4_3-branch)	(revision 156810)
@@ -0,0 +1,41 @@
+/* Copyright (C) 2008 Free Software Foundation, Inc.
+
+   This file is free software; you can redistribute it and/or modify it under
+   the terms of the GNU General Public License as published by the Free
+   Software Foundation; either version 2 of the License, or (at your option)
+   any later version.
+
+   This file is distributed in the hope that it will be useful, but WITHOUT
+   ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+   FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
+   for more details.
+
+   You should have received a copy of the GNU General Public License
+   along with this file; see the file COPYING.  If not, write to the Free
+   Software Foundation, 51 Franklin Street, Fifth Floor, Boston, MA
+   02110-1301, USA.  */
+
+/* As a special exception, if you include this header file into source files
+   compiled by GCC, this header file does not by itself cause  the resulting
+   executable to be covered by the GNU General Public License.  This exception
+   does not however invalidate any other reasons why the executable file might be
+   covered by the GNU General Public License.  */
+
+#ifndef SPU_CACHEH
+#define SPU_CACHEH
+
+void *__cache_fetch_dirty (__ea void *ea, int n_bytes_dirty);
+void *__cache_fetch (__ea void *ea);
+void __cache_evict (__ea void *ea);
+void __cache_flush (void);
+void __cache_touch (__ea void *ea);
+
+#define cache_fetch_dirty(_ea, _n_bytes_dirty) \
+     __cache_fetch_dirty(_ea, _n_bytes_dirty)
+
+#define cache_fetch(_ea) __cache_fetch(_ea)
+#define cache_touch(_ea) __cache_touch(_ea)
+#define cache_evict(_ea) __cache_evict(_ea)
+#define cache_flush() __cache_flush()
+
+#endif
Index: gcc/config/spu/spu-protos.h
===================================================================
--- gcc/config/spu/spu-protos.h	(.../gcc-4_3-branch)	(revision 156795)
+++ gcc/config/spu/spu-protos.h	(.../cell-4_3-branch)	(revision 156810)
@@ -51,10 +51,12 @@
 extern int iohl_immediate_p (rtx op, enum machine_mode mode);
 extern int arith_immediate_p (rtx op, enum machine_mode mode,
 			      HOST_WIDE_INT low, HOST_WIDE_INT high);
+extern bool exp2_immediate_p (rtx op, enum machine_mode mode, int low,
+			      int high);
 extern int spu_constant_address_p (rtx x);
 extern int spu_legitimate_constant_p (rtx x);
 extern int spu_legitimate_address (enum machine_mode mode, rtx x,
-				   int reg_ok_strict);
+				   int reg_ok_strict, int for_split);
 extern rtx spu_legitimize_address (rtx x, rtx oldx, enum machine_mode mode);
 extern int spu_initial_elimination_offset (int from, int to);
 extern rtx spu_function_value (const_tree type, const_tree func);
@@ -64,17 +66,16 @@
 					tree type, int *pretend_size,
 					int no_rtl);
 extern void spu_conditional_register_usage (void);
-extern int aligned_mem_p (rtx mem);
 extern int spu_expand_mov (rtx * ops, enum machine_mode mode);
-extern void spu_split_load (rtx * ops);
-extern void spu_split_store (rtx * ops);
-extern int spu_valid_move (rtx * ops);
+extern int spu_split_load (rtx * ops);
+extern int spu_split_store (rtx * ops);
 extern int fsmbi_const_p (rtx x);
 extern int cpat_const_p (rtx x, enum machine_mode mode);
 extern rtx gen_cpat_const (rtx * ops);
 extern void constant_to_array (enum machine_mode mode, rtx x,
 			       unsigned char *arr);
 extern rtx array_to_constant (enum machine_mode mode, unsigned char *arr);
+extern rtx spu_gen_exp2 (enum machine_mode mode, rtx x);
 extern void spu_allocate_stack (rtx op0, rtx op1);
 extern void spu_restore_stack_nonlocal (rtx op0, rtx op1);
 extern void spu_restore_stack_block (rtx op0, rtx op1);
Index: gcc/config/spu/predicates.md
===================================================================
--- gcc/config/spu/predicates.md	(.../gcc-4_3-branch)	(revision 156795)
+++ gcc/config/spu/predicates.md	(.../cell-4_3-branch)	(revision 156810)
@@ -39,14 +39,14 @@
        (ior (not (match_code "subreg"))
             (match_test "valid_subreg (op)"))))
 
-(define_predicate "spu_mem_operand"
-  (and (match_operand 0 "memory_operand")
-       (match_test "reload_in_progress || reload_completed || aligned_mem_p (op)")))
-
 (define_predicate "spu_mov_operand"
-  (ior (match_operand 0 "spu_mem_operand")
+  (ior (match_operand 0 "memory_operand")
        (match_operand 0 "spu_nonmem_operand")))
 
+(define_predicate "spu_dest_operand"
+  (ior (match_operand 0 "memory_operand")
+       (match_operand 0 "spu_reg_operand")))
+
 (define_predicate "call_operand"
   (and (match_code "mem")
        (match_test "(!TARGET_LARGE_MEM && satisfies_constraint_S (op))
@@ -104,3 +104,13 @@
        (ior (match_test "GET_MODE (XEXP (op, 0)) == HImode")
 	    (match_test "GET_MODE (XEXP (op, 0)) == SImode"))))
 
+(define_predicate "spu_inv_exp2_operand"
+  (and (match_code "const_double,const_vector")
+       (and (match_operand 0 "immediate_operand")
+	    (match_test "exp2_immediate_p (op, mode, -126, 0)"))))
+
+(define_predicate "spu_exp2_operand"
+  (and (match_code "const_double,const_vector")
+       (and (match_operand 0 "immediate_operand")
+	    (match_test "exp2_immediate_p (op, mode, 0, 127)"))))
+
Index: gcc/config/spu/cachemgr.c
===================================================================
--- gcc/config/spu/cachemgr.c	(.../gcc-4_3-branch)	(revision 0)
+++ gcc/config/spu/cachemgr.c	(.../cell-4_3-branch)	(revision 156810)
@@ -0,0 +1,459 @@
+/* Copyright (C) 2008  Free Software Foundation, Inc.
+
+This file is part of GCC.
+
+GCC is free software; you can redistribute it and/or modify it under
+the terms of the GNU General Public License as published by the Free
+Software Foundation; either version 2, or (at your option) any later
+version.
+
+In addition to the permissions in the GNU General Public License, the
+Free Software Foundation gives you unlimited permission to link the
+compiled version of this file into combinations with other programs,
+and to distribute those combinations without any restriction coming
+from the use of this file.  (The General Public License restrictions
+do apply in other respects; for example, they cover modification of
+the file, and distribution when not linked into a combine
+executable.)
+
+GCC is distributed in the hope that it will be useful, but WITHOUT ANY
+WARRANTY; without even the implied warranty of MERCHANTABILITY or
+FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
+for more details.
+
+You should have received a copy of the GNU General Public License
+along with GCC; see the file COPYING.  If not, write to the Free
+Software Foundation, 51 Franklin Street, Fifth Floor, Boston, MA
+02110-1301, USA.  */
+
+#include <spu_mfcio.h>
+#include <spu_internals.h>
+#include <spu_intrinsics.h>
+#include <spu_cache.h>
+
+extern unsigned long long __ea_local_store;
+extern char __cache_tag_array_size;
+
+#define LINE_SIZE 128
+#define TAG_MASK (LINE_SIZE - 1)
+
+#define WAYS 4
+#define SET_MASK ((int) &__cache_tag_array_size - LINE_SIZE)
+
+#define CACHE_LINES ((int) &__cache_tag_array_size / \
+  sizeof (struct __cache_tag_array) * WAYS)
+
+struct __cache_tag_array
+{
+  unsigned int tag_lo[WAYS];
+  unsigned int tag_hi[WAYS];
+  void *base[WAYS];
+  int reserved[WAYS];
+  vector unsigned short dirty_bits[WAYS];
+};
+
+extern struct __cache_tag_array __cache_tag_array[];
+extern char __cache[];
+
+/* In order to make the code seem a little cleaner, and to avoid having
+   64/32 bit ifdefs all over the place, we macro.  */
+
+/* It may seem poor taste to define variables within a macro, but
+   it's C99 compliant.  */
+
+#ifdef __EA64__
+#define CHECK_TAG(_entry, _way, _tag) ((_entry->tag_lo[_way] == \
+  (_tag & 0xFFFFFFFF))&&(_entry->tag_hi[_way] == (_tag >> 32)))
+
+#define GET_TAG(_entry, _way) unsigned long long tag = _entry->tag_hi[_way]; \
+  tag = tag << 32;                                                           \
+  tag |= (_entry->tag_lo[_way]);
+
+#define SET_TAG(_entry, _way, _tag)             \
+  _entry->tag_lo[_way] = (_tag & 0xFFFFFFFF);   \
+  _entry->tag_hi[_way] = (_tag >> 32);
+
+#define addr unsigned long long
+#define si_from_eavoid(_x) si_from_ullong (eavoid_to_eanum(_x))
+#else /*__EA32__*/
+#define CHECK_TAG(_entry, _way, _tag) (_entry->tag_lo[_way] == _tag)
+
+#define GET_TAG(_entry, _way) unsigned long tag = _entry->tag_lo[_way]
+
+#define SET_TAG(_entry, _way, _tag)     \
+  _entry->tag_lo[_way] = _tag;
+
+#define addr unsigned long
+#define si_from_eavoid(_x) si_from_uint (eavoid_to_eanum(_x))
+#endif
+
+/* In GET_ENTRY, we cast away the high 32 bits,
+   as the tag is only in the low 32.  */
+
+#define GET_ENTRY(_addr) ((struct __cache_tag_array *)                  \
+        si_to_ptr(si_a                                                  \
+                   (si_and(si_from_uint((unsigned int) (addr) _addr),   \
+                           si_from_uint(SET_MASK)),                     \
+                    si_from_uint((unsigned int) __cache_tag_array))));
+
+#define GET_CACHE_LINE(_addr, _way)  ((void *) (__cache +       \
+  (_addr & SET_MASK) * WAYS) + (_way * LINE_SIZE));
+
+#define eavoid_to_eanum(_ea) ((addr) _ea)
+
+#define CHECK_DIRTY(_vec) (si_to_uint (si_orx ((qword) _vec)))
+#define SET_EMPTY(_entry, _way) (_entry->tag_lo[_way] = 1)
+#define CHECK_EMPTY(_entry, _way) (_entry->tag_lo[_way] == 1)
+
+#define LS_FLAG 0x80000000
+#define SET_IS_LS(_entry, _way) (_entry->reserved[_way] |= LS_FLAG)
+#define CHECK_IS_LS(_entry, _way) (_entry->reserved[_way] & LS_FLAG)
+#define GET_LRU(_entry, _way) (_entry->reserved[_way] & ~(LS_FLAG))
+
+static void __cache_flush_stub (void) __attribute__ ((destructor));
+static int dma_tag = 32;
+
+static void
+__cache_evict_entry (struct __cache_tag_array *entry, int way)
+{
+
+  GET_TAG (entry, way);
+
+  if ((CHECK_DIRTY (entry->dirty_bits[way])) && (!CHECK_IS_LS (entry, way)))
+    {
+#ifdef NONATOMIC
+      /* Non-atomic writes.  */
+      unsigned int oldmask, mach_stat;
+      char *line = ((void *) 0);
+
+      /* Enter critical section.  */
+      mach_stat = spu_readch (SPU_RdMachStat);
+      spu_idisable ();
+
+      /* Issue DMA request.  */
+      line = GET_CACHE_LINE (entry->tag_lo[way], way);
+      mfc_put (line, tag, LINE_SIZE, dma_tag, 0, 0);
+
+      /* Wait for DMA completion.  */
+      oldmask = mfc_read_tag_mask ();
+      mfc_write_tag_mask (1 << dma_tag);
+      mfc_read_tag_status_all ();
+      mfc_write_tag_mask (oldmask);
+
+      /* Leave critical section.  */
+      if (__builtin_expect (mach_stat & 1, 0))
+	spu_ienable ();
+#else
+      /* Allocate a buffer large enough that we know it has 128 bytes
+         that are 128 byte aligned (for DMA). */
+
+      char buffer[LINE_SIZE + 127];
+      qword *buf_ptr = (qword *) (((unsigned int) (buffer) + 127) & ~127);
+      qword *line = GET_CACHE_LINE (entry->tag_lo[way], way);
+      qword bits;
+      unsigned int mach_stat;
+
+      /* Enter critical section.  */
+      mach_stat = spu_readch (SPU_RdMachStat);
+      spu_idisable ();
+
+      do
+	{
+	  /* We atomically read the current memory into a buffer
+	     modify the dirty bytes in the buffer, and write it
+	     back. If writeback fails, loop and try again.  */
+
+	  mfc_getllar (buf_ptr, tag, 0, 0);
+	  mfc_read_atomic_status ();
+
+	  /* The method we're using to write 16 dirty bytes into
+	     the buffer at a time uses fsmb which in turn uses
+	     the least significant 16 bits of word 0, so we
+	     load the bits and rotate so that the first bit of
+	     the bitmap is in the first bit that fsmb will use.  */
+
+	  bits = (qword) entry->dirty_bits[way];
+	  bits = si_rotqbyi (bits, -2);
+
+	  /* Si_fsmb creates the mask of dirty bytes.
+	     Use selb to nab the appropriate bits.  */
+	  buf_ptr[0] = si_selb (buf_ptr[0], line[0], si_fsmb (bits));
+
+	  /* Rotate to next 16 byte section of cache.  */
+	  bits = si_rotqbyi (bits, 2);
+
+	  buf_ptr[1] = si_selb (buf_ptr[1], line[1], si_fsmb (bits));
+	  bits = si_rotqbyi (bits, 2);
+	  buf_ptr[2] = si_selb (buf_ptr[2], line[2], si_fsmb (bits));
+	  bits = si_rotqbyi (bits, 2);
+	  buf_ptr[3] = si_selb (buf_ptr[3], line[3], si_fsmb (bits));
+	  bits = si_rotqbyi (bits, 2);
+	  buf_ptr[4] = si_selb (buf_ptr[4], line[4], si_fsmb (bits));
+	  bits = si_rotqbyi (bits, 2);
+	  buf_ptr[5] = si_selb (buf_ptr[5], line[5], si_fsmb (bits));
+	  bits = si_rotqbyi (bits, 2);
+	  buf_ptr[6] = si_selb (buf_ptr[6], line[6], si_fsmb (bits));
+	  bits = si_rotqbyi (bits, 2);
+	  buf_ptr[7] = si_selb (buf_ptr[7], line[7], si_fsmb (bits));
+	  bits = si_rotqbyi (bits, 2);
+
+	  mfc_putllc (buf_ptr, tag, 0, 0);
+	}
+      while (mfc_read_atomic_status ());
+
+      /* Leave critical section.  */
+      if (__builtin_expect (mach_stat & 1, 0))
+	spu_ienable ();
+#endif
+    }
+
+  /* In any case, marking the lo tag with 1 which denotes empty.  */
+  SET_EMPTY (entry, way);
+  entry->dirty_bits[way] = (vector unsigned short) si_from_uint (0);
+}
+
+void
+__cache_evict (__ea void *ea)
+{
+  addr tag = (eavoid_to_eanum (ea) & ~(TAG_MASK));
+  struct __cache_tag_array *entry = GET_ENTRY (ea);
+  int i = 0;
+
+  /* Cycles through all the possible ways an address could be at
+     and evicts the way if found */
+
+  for (i = 0; i < WAYS; i++)
+    {
+      if (CHECK_TAG (entry, i, tag))
+	{
+	  __cache_evict_entry (entry, i);
+	}
+    }
+}
+
+static void *
+__cache_fill (int way, addr tag)
+{
+  unsigned int oldmask, mach_stat;
+  char *line = ((void *) 0);
+
+  /* Reserve our DMA tag.  */
+  if (dma_tag == 32)
+    dma_tag = mfc_tag_reserve ();
+
+  /* Enter critical section.  */
+  mach_stat = spu_readch (SPU_RdMachStat);
+  spu_idisable ();
+
+  /* Issue DMA request.  */
+  line = GET_CACHE_LINE (tag, way);
+  mfc_get (line, tag, LINE_SIZE, dma_tag, 0, 0);
+
+  /* Wait for DMA completion.  */
+  oldmask = mfc_read_tag_mask ();
+  mfc_write_tag_mask (1 << dma_tag);
+  mfc_read_tag_status_all ();
+  mfc_write_tag_mask (oldmask);
+
+  /* Leave critical section.  */
+  if (__builtin_expect (mach_stat & 1, 0))
+    spu_ienable ();
+
+  return (void *) line;
+}
+
+static void
+__cache_miss (__ea void *ea, struct __cache_tag_array *entry, int way)
+{
+
+  addr tag = (eavoid_to_eanum (ea) & ~(TAG_MASK));
+  unsigned int lru = 0;
+  int i = 0;
+  int idx = 0;
+
+  /* If way > 4, then there are no empty slots, so we must evict
+     the least recently used entry. */
+  if (way >= 4)
+    {
+      for (i = 0; i < WAYS; i++)
+	{
+	  if (GET_LRU (entry, i) > lru)
+	    {
+	      lru = GET_LRU (entry, i);
+	      idx = i;
+	    }
+	}
+      __cache_evict_entry (entry, idx);
+      way = idx;
+    }
+
+  /* Set the empty entry's tag and fill it's cache line. */
+
+  SET_TAG (entry, way, tag);
+  entry->reserved[way] = 0;
+
+  /* Check if the address is just an effective address within the
+     SPU's local store. */
+
+  /* Because the LS is not 256k aligned, we can't do a nice and mask
+     here to compare, so we must check the whole range.  */
+
+  if ((eavoid_to_eanum (ea) >= (addr) __ea_local_store) &&
+      (eavoid_to_eanum (ea) < (addr) (__ea_local_store + 0x40000)))
+    {
+      SET_IS_LS (entry, way);
+      entry->base[way] =
+	(void *) ((unsigned int) (eavoid_to_eanum (ea) -
+				  (addr) __ea_local_store) & ~(0x7f));
+    }
+  else
+    {
+      entry->base[way] = __cache_fill (way, tag);
+    }
+}
+
+void *
+__cache_fetch_dirty (__ea void *ea, int n_bytes_dirty)
+{
+#ifdef __EA64__
+  unsigned int tag_hi;
+  qword etag_hi;
+#endif
+  unsigned int tag_lo;
+  struct __cache_tag_array *entry;
+
+  qword etag_lo;
+  qword equal;
+  qword bit_mask;
+  qword way;
+
+  /* This first chunk, we merely fill the pointer and tag.  */
+
+  entry = GET_ENTRY (ea);
+
+#ifndef __EA64__
+  tag_lo =
+    si_to_uint (si_andc
+		(si_shufb
+		 (si_from_eavoid (ea), si_from_uint (0),
+		  si_from_uint (0x00010203)), si_from_uint (TAG_MASK)));
+#else
+  tag_lo =
+    si_to_uint (si_andc
+		(si_shufb
+		 (si_from_eavoid (ea), si_from_uint (0),
+		  si_from_uint (0x04050607)), si_from_uint (TAG_MASK)));
+
+  tag_hi =
+    si_to_uint (si_shufb
+		(si_from_eavoid (ea), si_from_uint (0),
+		 si_from_uint (0x00010203)));
+#endif
+
+  /* Increment LRU in reserved bytes.  */
+  si_stqd (si_ai (si_lqd (si_from_ptr (entry), 48), 1),
+	   si_from_ptr (entry), 48);
+
+missreturn:
+  /* Check if the entry's lo_tag is equal to the address' lo_tag.  */
+  etag_lo = si_lqd (si_from_ptr (entry), 0);
+  equal = si_ceq (etag_lo, si_from_uint (tag_lo));
+#ifdef __EA64__
+  /* And the high tag too  */
+  etag_hi = si_lqd (si_from_ptr (entry), 16);
+  equal = si_and (equal, (si_ceq (etag_hi, si_from_uint (tag_hi))));
+#endif
+
+  if ((si_to_uint (si_orx (equal)) == 0))
+    goto misshandler;
+
+  if (n_bytes_dirty)
+    {
+      /* way = 0x40,0x50,0x60,0x70 for each way, which is also the
+         offset of the appropriate dirty bits.  */
+      way = si_shli (si_clz (si_gbb (equal)), 2);
+
+      /* To create the bit_mask, we set it to all 1s (uint -1), then we
+         shift it over (128 - n_bytes_dirty) times.  */
+
+      bit_mask = si_from_uint (-1);
+
+      bit_mask =
+	si_shlqby (bit_mask, si_from_uint ((LINE_SIZE - n_bytes_dirty) / 8));
+
+      bit_mask =
+	si_shlqbi (bit_mask, si_from_uint ((LINE_SIZE - n_bytes_dirty) % 8));
+
+      /* Rotate it around to the correct offset.  */
+      bit_mask =
+	si_rotqby (bit_mask,
+		   si_from_uint (-1 * (eavoid_to_eanum (ea) & TAG_MASK) / 8));
+
+      bit_mask =
+	si_rotqbi (bit_mask,
+		   si_from_uint (-1 * (eavoid_to_eanum (ea) & TAG_MASK) % 8));
+
+      /* Update the dirty bits.  */
+      si_stqx (si_or (si_lqx (si_from_ptr (entry), way), bit_mask),
+	       si_from_ptr (entry), way);
+    };
+
+  /* We've definitely found the right entry, set LRU (reserved) to 0
+     maintaining the LS flag (MSB). */
+
+  si_stqd (si_andc
+	   (si_lqd (si_from_ptr (entry), 48),
+	    si_and (equal, si_from_uint (~(LS_FLAG)))),
+	   si_from_ptr (entry), 48);
+
+  return (void *)
+    si_to_ptr (si_a
+	       (si_orx
+		(si_and (si_lqd (si_from_ptr (entry), 32), equal)),
+		si_from_uint (((unsigned int) (addr) ea) & TAG_MASK)));
+
+misshandler:
+  equal = si_ceqi (etag_lo, 1);
+  __cache_miss (ea, entry, (si_to_uint (si_clz (si_gbb (equal))) - 16) >> 2);
+  goto missreturn;
+}
+
+void *
+__cache_fetch (__ea void *ea)
+{
+  return __cache_fetch_dirty (ea, 0);
+}
+
+void
+__cache_touch (__ea void *ea __attribute__ ((unused)))
+{
+  /* NO-OP for now.  */
+}
+
+static void
+__cache_flush_stub (void)
+{
+  __cache_flush ();
+}
+
+void
+__cache_flush (void)
+{
+  struct __cache_tag_array *entry = __cache_tag_array;
+  unsigned int i = 0;
+  int j = 0;
+
+  /* Cycle through each cache entry and evict all used ways.  */
+
+  for (i = 0; i < (CACHE_LINES / WAYS); i++)
+    {
+      for (j = 0; j < WAYS; j++)
+	{
+	  if (!CHECK_EMPTY (entry, j))
+	    {
+	      __cache_evict_entry (entry, j);
+	    }
+	}
+      entry++;
+    }
+}
Index: gcc/config/spu/spu-builtins.def
===================================================================
--- gcc/config/spu/spu-builtins.def	(.../gcc-4_3-branch)	(revision 156795)
+++ gcc/config/spu/spu-builtins.def	(.../cell-4_3-branch)	(revision 156810)
@@ -235,8 +235,8 @@
 
 /* definitions to support generic builtin functions: */
 
-DEF_BUILTIN (SPU_CONVTS,     CODE_FOR_spu_cflts,      "spu_convts",     B_INSN,     _A3(SPU_BTI_V4SI,     SPU_BTI_V4SF,   SPU_BTI_U7))
-DEF_BUILTIN (SPU_CONVTU,     CODE_FOR_spu_cfltu,      "spu_convtu",     B_INSN,     _A3(SPU_BTI_UV4SI,    SPU_BTI_V4SF,   SPU_BTI_U7))
+DEF_BUILTIN (SPU_CONVTS,     CODE_FOR_spu_cflts,      "spu_convts",     B_INSN,     _A3(SPU_BTI_V4SI,     SPU_BTI_V4SF,   SPU_BTI_INTSI))
+DEF_BUILTIN (SPU_CONVTU,     CODE_FOR_spu_cfltu,      "spu_convtu",     B_INSN,     _A3(SPU_BTI_UV4SI,    SPU_BTI_V4SF,   SPU_BTI_INTSI))
 DEF_BUILTIN (SPU_ROUNDTF,    CODE_FOR_spu_frds,       "spu_roundtf",    B_INSN,     _A2(SPU_BTI_V4SF,     SPU_BTI_V2DF))
 DEF_BUILTIN (SPU_MULH,       CODE_FOR_spu_mpyh,       "spu_mulh",       B_INSN,     _A3(SPU_BTI_V4SI,     SPU_BTI_V8HI,   SPU_BTI_V8HI))
 DEF_BUILTIN (SPU_MULSR,      CODE_FOR_spu_mpys,       "spu_mulsr",      B_INSN,     _A3(SPU_BTI_V4SI,     SPU_BTI_V8HI,   SPU_BTI_V8HI))
@@ -257,8 +257,8 @@
 /* definitions to support overloaded generic builtin functions:  */
 
 DEF_BUILTIN (SPU_CONVTF,           CODE_FOR_nothing,       "spu_convtf",           B_OVERLOAD, _A1(SPU_BTI_VOID))
-DEF_BUILTIN (SPU_CONVTF_0,         CODE_FOR_spu_cuflt,     "spu_convtf_0",         B_INTERNAL, _A3(SPU_BTI_V4SF,   SPU_BTI_UV4SI,  SPU_BTI_U7))
-DEF_BUILTIN (SPU_CONVTF_1,         CODE_FOR_spu_csflt,     "spu_convtf_1",         B_INTERNAL, _A3(SPU_BTI_V4SF,   SPU_BTI_V4SI,   SPU_BTI_U7))
+DEF_BUILTIN (SPU_CONVTF_0,         CODE_FOR_spu_cuflt,     "spu_convtf_0",         B_INTERNAL, _A3(SPU_BTI_V4SF,   SPU_BTI_UV4SI,  SPU_BTI_UINTSI))
+DEF_BUILTIN (SPU_CONVTF_1,         CODE_FOR_spu_csflt,     "spu_convtf_1",         B_INTERNAL, _A3(SPU_BTI_V4SF,   SPU_BTI_V4SI,   SPU_BTI_UINTSI))
 DEF_BUILTIN (SPU_EXTEND,           CODE_FOR_nothing,       "spu_extend",           B_OVERLOAD, _A1(SPU_BTI_VOID))
 DEF_BUILTIN (SPU_EXTEND_0,         CODE_FOR_spu_xsbh,      "spu_extend_0",         B_INTERNAL, _A2(SPU_BTI_V8HI,   SPU_BTI_V16QI))
 DEF_BUILTIN (SPU_EXTEND_1,         CODE_FOR_spu_xshw,      "spu_extend_1",         B_INTERNAL, _A2(SPU_BTI_V4SI,   SPU_BTI_V8HI))
Index: gcc/config/spu/spu-builtins.md
===================================================================
--- gcc/config/spu/spu-builtins.md	(.../gcc-4_3-branch)	(revision 156795)
+++ gcc/config/spu/spu-builtins.md	(.../cell-4_3-branch)	(revision 156810)
@@ -23,9 +23,8 @@
 
 (define_expand "spu_lqd"
   [(set (match_operand:TI 0 "spu_reg_operand" "")
-        (mem:TI (and:SI (plus:SI (match_operand:SI 1 "spu_reg_operand" "")
-				 (match_operand:SI 2 "spu_nonmem_operand" ""))
-		        (const_int -16))))]
+        (mem:TI (plus:SI (match_operand:SI 1 "spu_reg_operand" "")
+			 (match_operand:SI 2 "spu_nonmem_operand" ""))))]
   ""
   {
     if (GET_CODE (operands[2]) == CONST_INT
@@ -42,16 +41,14 @@
 
 (define_expand "spu_lqx"
   [(set (match_operand:TI 0 "spu_reg_operand" "")
-        (mem:TI (and:SI (plus:SI (match_operand:SI 1 "spu_reg_operand" "")
-                                 (match_operand:SI 2 "spu_reg_operand" ""))
-                        (const_int -16))))]
+        (mem:TI (plus:SI (match_operand:SI 1 "spu_reg_operand" "")
+			 (match_operand:SI 2 "spu_reg_operand" ""))))]
   ""
   "")
 
 (define_expand "spu_lqa"
   [(set (match_operand:TI 0 "spu_reg_operand" "")
-        (mem:TI (and:SI (match_operand:SI 1 "immediate_operand" "")
-                        (const_int -16))))]
+        (mem:TI (match_operand:SI 1 "immediate_operand" "")))]
   ""
   {
     if (GET_CODE (operands[1]) == CONST_INT
@@ -61,15 +58,13 @@
 
 (define_expand "spu_lqr"
   [(set (match_operand:TI 0 "spu_reg_operand" "")
-	(mem:TI (and:SI (match_operand:SI 1 "address_operand" "")
-			(const_int -16))))]
+	(mem:TI (match_operand:SI 1 "address_operand" "")))]
   ""
   "")
 
 (define_expand "spu_stqd"
-  [(set (mem:TI (and:SI (plus:SI (match_operand:SI 1 "spu_reg_operand" "")
-				 (match_operand:SI 2 "spu_nonmem_operand" ""))
-		        (const_int -16)))
+  [(set (mem:TI (plus:SI (match_operand:SI 1 "spu_reg_operand" "")
+			 (match_operand:SI 2 "spu_nonmem_operand" "")))
         (match_operand:TI 0 "spu_reg_operand" "r,r"))]
   ""
   {
@@ -86,16 +81,14 @@
   })
 
 (define_expand "spu_stqx"
-  [(set (mem:TI (and:SI (plus:SI (match_operand:SI 1 "spu_reg_operand" "")
-				 (match_operand:SI 2 "spu_reg_operand" ""))
-		        (const_int -16)))
+  [(set (mem:TI (plus:SI (match_operand:SI 1 "spu_reg_operand" "")
+			 (match_operand:SI 2 "spu_reg_operand" "")))
         (match_operand:TI 0 "spu_reg_operand" "r"))]
   ""
   "")
 
 (define_expand "spu_stqa"
-  [(set (mem:TI (and:SI (match_operand:SI 1 "immediate_operand" "")
-			(const_int -16)))
+  [(set (mem:TI (match_operand:SI 1 "immediate_operand" ""))
         (match_operand:TI 0 "spu_reg_operand" "r"))]
   ""
   {
@@ -105,8 +98,7 @@
   })
 
 (define_expand "spu_stqr"
-    [(set (mem:TI (and:SI (match_operand:SI 1 "address_operand" "")
-			  (const_int -16)))
+    [(set (mem:TI (match_operand:SI 1 "address_operand" ""))
 	  (match_operand:TI 0 "spu_reg_operand" ""))]
   ""
   "")
@@ -527,37 +519,119 @@
   [(set_attr "type" "br")])
 
 ;; float convert
-(define_insn "spu_csflt"
-  [(set (match_operand:V4SF 0 "spu_reg_operand" "=r")
-	(unspec:V4SF [(match_operand:V4SI 1 "spu_reg_operand" "r")
-		      (match_operand:SI 2 "immediate_operand" "K")] UNSPEC_CSFLT ))]
+(define_expand "spu_csflt"
+  [(set (match_operand:V4SF 0 "spu_reg_operand")
+	(unspec:V4SF [(match_operand:V4SI 1 "spu_reg_operand")
+		      (match_operand:SI 2 "spu_nonmem_operand")] 0 ))]
   ""
-  "csflt\t%0,%1,%2"
-  [(set_attr "type" "fp7")])
+{
+  if (GET_CODE (operands[2]) == CONST_INT
+      && (INTVAL (operands[2]) < 0 || INTVAL (operands[2]) > 127))
+    {
+      error ("spu_convtf expects an integer literal in the range [0, 127].");
+      operands[2] = force_reg (SImode, operands[2]);
+    }
+  if (GET_CODE (operands[2]) != CONST_INT)
+    {
+      rtx exp2;
+      rtx cnv = gen_reg_rtx (V4SFmode);
+      rtx scale = gen_reg_rtx (SImode);
+      rtx op2 = force_reg (SImode, operands[2]);
+      rtx m1 = spu_gen_exp2 (V4SFmode, GEN_INT (-1));
+      emit_insn (gen_subsi3 (scale, const1_rtx, op2));
+      exp2 = spu_gen_exp2 (V4SFmode, scale);
+      emit_insn (gen_floatv4siv4sf2_mul (cnv, operands[1], m1));
+      emit_insn (gen_mulv4sf3 (operands[0], cnv, exp2));
+    }
+  else
+    {
+      rtx exp2 = spu_gen_exp2 (V4SFmode, operands[2]);
+      emit_insn (gen_floatv4siv4sf2_div (operands[0], operands[1], exp2));
+    }
+  DONE;
+})
 
-(define_insn "spu_cflts"
-  [(set (match_operand:V4SI 0 "spu_reg_operand" "=r")
-	(unspec:V4SI [(match_operand:V4SF 1 "spu_reg_operand" "r")
-                      (match_operand:SI 2 "immediate_operand" "J")] UNSPEC_CFLTS ))]
+(define_expand "spu_cflts"
+  [(set (match_operand:V4SI 0 "spu_reg_operand")
+	(unspec:V4SI [(match_operand:V4SF 1 "spu_reg_operand")
+                      (match_operand:SI 2 "spu_nonmem_operand")] 0 ))]
   ""
-  "cflts\t%0,%1,%2"
-  [(set_attr "type" "fp7")])
+{
+  rtx exp2;
+  if (GET_CODE (operands[2]) == CONST_INT
+      && (INTVAL (operands[2]) < 0 || INTVAL (operands[2]) > 127))
+    {
+      error ("spu_convts expects an integer literal in the range [0, 127].");
+      operands[2] = force_reg (SImode, operands[2]);
+    }
+  exp2 = spu_gen_exp2 (V4SFmode, operands[2]);
+  if (GET_CODE (operands[2]) != CONST_INT)
+    {
+      rtx mul = gen_reg_rtx (V4SFmode);
+      emit_insn (gen_mulv4sf3 (mul, operands[1], exp2));
+      emit_insn (gen_fix_truncv4sfv4si2 (operands[0], mul));
+    }
+  else 
+    emit_insn (gen_fix_truncv4sfv4si2_mul (operands[0], operands[1], exp2));
+  DONE;
+})
 
-(define_insn "spu_cuflt"
+(define_expand "spu_cuflt"
   [(set (match_operand:V4SF 0 "spu_reg_operand" "=r")
-	(unspec:V4SF [(match_operand:V4SI 1 "spu_reg_operand" "r")
-		      (match_operand:SI 2 "immediate_operand" "K")] UNSPEC_CUFLT ))]
+	(unspec:V4SF [(match_operand:V4SI 1 "spu_reg_operand")
+		      (match_operand:SI 2 "spu_nonmem_operand")] 0 ))]
   ""
-  "cuflt\t%0,%1,%2"
-  [(set_attr "type" "fp7")])
+{
+  if (GET_CODE (operands[2]) == CONST_INT
+      && (INTVAL (operands[2]) < 0 || INTVAL (operands[2]) > 127))
+    {
+      error ("spu_convtf expects an integer literal in the range [0, 127].");
+      operands[2] = force_reg (SImode, operands[2]);
+    }
+  if (GET_CODE (operands[2]) != CONST_INT)
+    {
+      rtx exp2;
+      rtx cnv = gen_reg_rtx (V4SFmode);
+      rtx scale = gen_reg_rtx (SImode);
+      rtx op2 = force_reg (SImode, operands[2]);
+      rtx m1 = spu_gen_exp2 (V4SFmode, GEN_INT (-1));
+      emit_insn (gen_subsi3 (scale, const1_rtx, op2));
+      exp2 = spu_gen_exp2 (V4SFmode, scale);
+      emit_insn (gen_floatunsv4siv4sf2_mul (cnv, operands[1], m1));
+      emit_insn (gen_mulv4sf3 (operands[0], cnv, exp2));
+    }
+  else
+    {
+      rtx exp2 = spu_gen_exp2 (V4SFmode, operands[2]);
+      emit_insn (gen_floatunsv4siv4sf2_div (operands[0], operands[1], exp2));
+    }
+  DONE;
+})
 
-(define_insn "spu_cfltu"
-  [(set (match_operand:V4SI 0 "spu_reg_operand" "=r")
-	(unspec:V4SI [(match_operand:V4SF 1 "spu_reg_operand" "r")
-		      (match_operand:SI 2 "immediate_operand" "J")] UNSPEC_CFLTU ))]
+(define_expand "spu_cfltu"
+  [(set (match_operand:V4SI 0 "spu_reg_operand")
+	(unspec:V4SI [(match_operand:V4SF 1 "spu_reg_operand")
+		      (match_operand:SI 2 "spu_nonmem_operand")] 0 ))]
   ""
-  "cfltu\t%0,%1,%2"
-  [(set_attr "type" "fp7")])
+{
+  rtx exp2;
+  if (GET_CODE (operands[2]) == CONST_INT
+      && (INTVAL (operands[2]) < 0 || INTVAL (operands[2]) > 127))
+    {
+      error ("spu_convtu expects an integer literal in the range [0, 127].");
+      operands[2] = force_reg (SImode, operands[2]);
+    }
+  exp2 = spu_gen_exp2 (V4SFmode, operands[2]);
+  if (GET_CODE (operands[2]) != CONST_INT)
+    {
+      rtx mul = gen_reg_rtx (V4SFmode);
+      emit_insn (gen_mulv4sf3 (mul, operands[1], exp2));
+      emit_insn (gen_fixuns_truncv4sfv4si2 (operands[0], mul));
+    }
+  else 
+    emit_insn (gen_fixuns_truncv4sfv4si2_mul (operands[0], operands[1], exp2));
+  DONE;
+})
 
 (define_expand "spu_frds"
    [(set (match_operand:V4SF 0 "spu_reg_operand" "")
Index: gcc/config/spu/cache.S
===================================================================
--- gcc/config/spu/cache.S	(.../gcc-4_3-branch)	(revision 0)
+++ gcc/config/spu/cache.S	(.../cell-4_3-branch)	(revision 156810)
@@ -0,0 +1,47 @@
+/* Copyright (C) 2008  Free Software Foundation, Inc.
+
+This file is part of GCC.
+
+GCC is free software; you can redistribute it and/or modify it under
+the terms of the GNU General Public License as published by the Free
+Software Foundation; either version 2, or (at your option) any later
+version.
+
+In addition to the permissions in the GNU General Public License, the
+Free Software Foundation gives you unlimited permission to link the
+compiled version of this file into combinations with other programs,
+and to distribute those combinations without any restriction coming
+from the use of this file.  (The General Public License restrictions
+do apply in other respects; for example, they cover modification of
+the file, and distribution when not linked into a combine
+executable.)
+
+GCC is distributed in the hope that it will be useful, but WITHOUT ANY
+WARRANTY; without even the implied warranty of MERCHANTABILITY or
+FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
+for more details.
+
+You should have received a copy of the GNU General Public License
+along with GCC; see the file COPYING.  If not, write to the Free
+Software Foundation, 51 Franklin Street, Fifth Floor, Boston, MA
+02110-1301, USA.  */
+
+.data
+.p2align 7
+.global __cache
+__cache:
+.rept __CACHE_SIZE__ * 8
+.fill 128
+.endr
+
+.p2align 7
+.global __cache_tag_array
+__cache_tag_array:
+.rept __CACHE_SIZE__ * 2
+.long 1, 1, 1, 1
+.fill 128-16
+.endr
+__end_cache_tag_array:
+
+.globl __cache_tag_array_size
+.set __cache_tag_array_size, __end_cache_tag_array-__cache_tag_array
Index: gcc/config/spu/spu.opt
===================================================================
--- gcc/config/spu/spu.opt	(.../gcc-4_3-branch)	(revision 156795)
+++ gcc/config/spu/spu.opt	(.../cell-4_3-branch)	(revision 156810)
@@ -82,3 +82,15 @@
 mtune=
 Target RejectNegative Joined Var(spu_tune_string)
 Schedule code for given CPU
+
+mea32
+Target Report RejectNegative Var(spu_ea_model,32) Init(32)
+Access variables in 32-bit PPU objects
+
+mea64
+Target Report RejectNegative Var(spu_ea_model,64) VarExists
+Access variables in 64-bit PPU objects
+
+mall-ea
+Target Report Mask(ALL_EA)
+Implicitly declare all pointers and global or static variables to be __ea
Index: gcc/config/spu/spu-c.c
===================================================================
--- gcc/config/spu/spu-c.c	(.../gcc-4_3-branch)	(revision 156795)
+++ gcc/config/spu/spu-c.c	(.../cell-4_3-branch)	(revision 156810)
@@ -34,6 +34,64 @@
 #include "optabs.h"
 
 
+/* Keep the vector keywords handy for fast comparisons.  */
+static GTY(()) tree __vector_keyword;
+static GTY(()) tree vector_keyword;
+
+static cpp_hashnode *
+spu_categorize_keyword (const cpp_token *tok)
+{
+  if (tok->type == CPP_NAME)
+    {
+      cpp_hashnode *ident = tok->val.node;
+
+      if (ident == C_CPP_HASHNODE (vector_keyword)
+	  || ident == C_CPP_HASHNODE (__vector_keyword))
+	return C_CPP_HASHNODE (__vector_keyword);
+      else
+	return ident;
+    }
+  return 0;
+}
+
+/* Called to decide whether a conditional macro should be expanded.
+   Since we have exactly one such macro (i.e, 'vector'), we do not
+   need to examine the 'tok' parameter.  */
+
+static cpp_hashnode *
+spu_macro_to_expand (cpp_reader *pfile, const cpp_token *tok)
+{
+  cpp_hashnode *expand_this = tok->val.node;
+  cpp_hashnode *ident;
+
+  ident = spu_categorize_keyword (tok);
+  if (ident == C_CPP_HASHNODE (__vector_keyword))
+    {
+      tok = cpp_peek_token (pfile, 0);
+      ident = spu_categorize_keyword (tok);
+
+      if (ident)
+	{
+	  enum rid rid_code = (enum rid)(ident->rid_code);
+	  if (ident->type == NT_MACRO)
+	    {
+	      (void) cpp_get_token (pfile);
+	      tok = cpp_peek_token (pfile, 0);
+	      ident = spu_categorize_keyword (tok);
+	      if (ident)
+		rid_code = (enum rid)(ident->rid_code);
+	    }
+	  
+	  if (rid_code == RID_UNSIGNED || rid_code == RID_LONG
+	      || rid_code == RID_SHORT || rid_code == RID_SIGNED
+	      || rid_code == RID_INT || rid_code == RID_CHAR
+	      || rid_code == RID_FLOAT || rid_code == RID_DOUBLE)
+	    expand_this = C_CPP_HASHNODE (__vector_keyword);
+	}
+    }
+  return expand_this;
+}
+
 /* target hook for resolve_overloaded_builtin(). Returns a function call
    RTX if we can resolve the overloaded builtin */
 tree
@@ -142,6 +200,33 @@
   if (spu_arch == PROCESSOR_CELLEDP)
     builtin_define_std ("__SPU_EDP__");
   builtin_define_std ("__vector=__attribute__((__spu_vector__))");
+  switch (spu_ea_model)
+    {
+    case 32:
+      builtin_define_std ("__EA32__");
+      break;
+    case 64:
+      builtin_define_std ("__EA64__");
+      break;
+    default:
+       gcc_unreachable ();
+    }
+
+  if (!flag_iso)
+    {
+      /* Define this when supporting context-sensitive keywords.  */
+      cpp_define (pfile, "__VECTOR_KEYWORD_SUPPORTED__");
+      cpp_define (pfile, "vector=vector");
+
+      /* Initialize vector keywords.  */
+      __vector_keyword = get_identifier ("__vector");
+      C_CPP_HASHNODE (__vector_keyword)->flags |= NODE_CONDITIONAL;
+      vector_keyword = get_identifier ("vector");
+      C_CPP_HASHNODE (vector_keyword)->flags |= NODE_CONDITIONAL;
+
+      /* Enable context-sensitive macros.  */
+      cpp_get_callbacks (pfile)->macro_to_expand = spu_macro_to_expand;
+    }
 }
 
 void
Index: gcc/config/spu/t-spu-elf
===================================================================
--- gcc/config/spu/t-spu-elf	(.../gcc-4_3-branch)	(revision 156795)
+++ gcc/config/spu/t-spu-elf	(.../cell-4_3-branch)	(revision 156810)
@@ -62,14 +62,65 @@
 CRTSTUFF_T_CFLAGS =
 
 #MULTILIB_OPTIONS=mlarge-mem/mtest-abi
+MULTILIB_OPTIONS=mea64
 #MULTILIB_DIRNAMES=large-mem test-abi
 #MULTILIB_MATCHES=
 
 # Neither gcc or newlib seem to have a standard way to generate multiple
 # crt*.o files.  So we don't use the standard crt0.o name anymore.
 
-EXTRA_MULTILIB_PARTS = crtbegin.o crtend.o
+EXTRA_MULTILIB_PARTS = crtbegin.o crtend.o libgcc_cachemgr.a libgcc_cachemgr_nonatomic.a \
+	libgcc_cache8k.a libgcc_cache16k.a libgcc_cache32k.a libgcc_cache64k.a libgcc_cache128k.a
 
+cachemgr.o: $(srcdir)/config/spu/cachemgr.c
+	$(GCC_FOR_TARGET) $(LIBGCC2_CFLAGS) -c $< -o $@
+
+%/cachemgr.o: $(srcdir)/config/spu/cachemgr.c
+	$(GCC_FOR_TARGET) $(LIBGCC2_CFLAGS) -c $< -o $@
+
+# Specialised rule to add a -D flag.
+cachemgr_nonatomic.o: $(srcdir)/config/spu/cachemgr.c
+	$(GCC_FOR_TARGET) $(LIBGCC2_CFLAGS) -DNONATOMIC -c $< -o $@
+
+%/cachemgr_nonatomic.o: $(srcdir)/config/spu/cachemgr.c
+	$(GCC_FOR_TARGET) $(LIBGCC2_CFLAGS) -DNONATOMIC -c $< -o $@
+
+libgcc_%.a: %.o
+	$(AR_FOR_TARGET) -rcs $@ $<
+
+%/libgcc_%.a: %.o
+	$(AR_FOR_TARGET) -rcs $@ $<
+
+cache8k.o: $(srcdir)/config/spu/cache.S
+	$(GCC_FOR_TARGET) -D__CACHE_SIZE__=8 -o $@ -c $<
+
+cache16k.o: $(srcdir)/config/spu/cache.S
+	$(GCC_FOR_TARGET) -D__CACHE_SIZE__=16 -o $@ -c $<
+
+cache32k.o: $(srcdir)/config/spu/cache.S
+	$(GCC_FOR_TARGET) -D__CACHE_SIZE__=32 -o $@ -c $<
+
+cache64k.o: $(srcdir)/config/spu/cache.S
+	$(GCC_FOR_TARGET) -D__CACHE_SIZE__=64 -o $@ -c $<
+
+cache128k.o: $(srcdir)/config/spu/cache.S
+	$(GCC_FOR_TARGET) -D__CACHE_SIZE__=128 -o $@ -c $<
+
+%/cache8k.o: $(srcdir)/config/spu/cache.S
+	$(GCC_FOR_TARGET) -D__CACHE_SIZE__=8 -o $@ -c $<
+
+%/cache16k.o: $(srcdir)/config/spu/cache.S
+	$(GCC_FOR_TARGET) -D__CACHE_SIZE__=16 -o $@ -c $<
+
+%/cache32k.o: $(srcdir)/config/spu/cache.S
+	$(GCC_FOR_TARGET) -D__CACHE_SIZE__=32 -o $@ -c $<
+
+%/cache64k.o: $(srcdir)/config/spu/cache.S
+	$(GCC_FOR_TARGET) -D__CACHE_SIZE__=64 -o $@ -c $<
+
+%/cache128k.o: $(srcdir)/config/spu/cache.S
+	$(GCC_FOR_TARGET) -D__CACHE_SIZE__=128 -o $@ -c $<
+
 LIBGCC = stmp-multilib
 INSTALL_LIBGCC = install-multilib
 
Index: gcc/config/spu/spu.c
===================================================================
--- gcc/config/spu/spu.c	(.../gcc-4_3-branch)	(revision 156795)
+++ gcc/config/spu/spu.c	(.../cell-4_3-branch)	(revision 156810)
@@ -130,6 +130,19 @@
   int low, high;
 };
 
+struct spu_address_space
+{
+  const char *name;
+  rtx (*to_generic_insn) (rtx, rtx);
+  rtx (*from_generic_insn) (rtx, rtx);
+};
+
+static struct spu_address_space spu_address_spaces[] = {
+  {"generic", NULL, NULL },
+  {"__ea", gen_from_ea, gen_to_ea },
+  {NULL, NULL, NULL},
+};
+
 static struct spu_builtin_range spu_builtin_range[] = {
   {-0x40ll, 0x7fll},		/* SPU_BTI_7     */
   {-0x40ll, 0x3fll},		/* SPU_BTI_S7    */
@@ -189,9 +202,9 @@
 static void spu_va_start (tree, rtx);
 static tree spu_gimplify_va_arg_expr (tree valist, tree type, tree * pre_p,
 				      tree * post_p);
-static int regno_aligned_for_load (int regno);
 static int store_with_one_insn_p (rtx mem);
 static int mem_is_padded_component_ref (rtx x);
+static int reg_aligned_for_addr	(rtx x, int aligned);
 static bool spu_assemble_integer (rtx x, unsigned int size, int aligned_p);
 static void spu_asm_globalize_label (FILE * file, const char *name);
 static unsigned char spu_rtx_costs (rtx x, int code, int outer_code,
@@ -211,7 +224,7 @@
 static unsigned int spu_section_type_flags (tree, const char *, int);
 
 extern const char *reg_names[];
-rtx spu_compare_op0, spu_compare_op1;
+rtx spu_compare_op0, spu_compare_op1, spu_expect_op0, spu_expect_op1;
 
 /* Which instruction set architecture to use.  */
 int spu_arch;
@@ -270,6 +283,30 @@
 
 /*  TARGET overrides.  */
 
+static enum machine_mode spu_ea_pointer_mode (int);
+#undef TARGET_ADDR_SPACE_POINTER_MODE
+#define TARGET_ADDR_SPACE_POINTER_MODE spu_ea_pointer_mode
+
+static const char *spu_addr_space_name (int);
+#undef TARGET_ADDR_SPACE_NAME
+#define TARGET_ADDR_SPACE_NAME spu_addr_space_name
+
+static unsigned char spu_addr_space_number (const tree);
+#undef TARGET_ADDR_SPACE_NUMBER
+#define TARGET_ADDR_SPACE_NUMBER spu_addr_space_number
+
+static rtx (* spu_addr_space_conversion_rtl (int, int)) (rtx, rtx);
+#undef TARGET_ADDR_SPACE_CONVERSION_RTL
+#define TARGET_ADDR_SPACE_CONVERSION_RTL spu_addr_space_conversion_rtl
+
+static bool spu_valid_pointer_mode (enum machine_mode mode);
+#undef TARGET_VALID_POINTER_MODE
+#define TARGET_VALID_POINTER_MODE spu_valid_pointer_mode
+
+static bool spu_valid_addr_space (const_tree);
+#undef TARGET_VALID_ADDR_SPACE
+#define TARGET_VALID_ADDR_SPACE spu_valid_addr_space
+
 #undef TARGET_INIT_BUILTINS
 #define TARGET_INIT_BUILTINS spu_init_builtins
 
@@ -279,10 +316,8 @@
 #undef TARGET_UNWIND_WORD_MODE
 #define TARGET_UNWIND_WORD_MODE spu_unwind_word_mode
 
-/* The .8byte directive doesn't seem to work well for a 32 bit
-   architecture. */
-#undef TARGET_ASM_UNALIGNED_DI_OP
-#define TARGET_ASM_UNALIGNED_DI_OP NULL
+#undef TARGET_ASM_ALIGNED_DI_OP
+#define TARGET_ASM_ALIGNED_DI_OP "\t.quad\t"
 
 #undef TARGET_RTX_COSTS
 #define TARGET_RTX_COSTS spu_rtx_costs
@@ -1064,6 +1099,7 @@
     {
       rtx bcomp;
       rtx loc_ref;
+      rtx jump_pat;
 
       /* We don't have branch on QI compare insns, so we convert the
          QI compare result to a HI result. */
@@ -1081,9 +1117,59 @@
 	bcomp = gen_rtx_NE (comp_mode, compare_result, const0_rtx);
 
       loc_ref = gen_rtx_LABEL_REF (VOIDmode, target);
-      emit_jump_insn (gen_rtx_SET (VOIDmode, pc_rtx,
-				   gen_rtx_IF_THEN_ELSE (VOIDmode, bcomp,
-							 loc_ref, pc_rtx)));
+      jump_pat = gen_rtx_SET (VOIDmode, pc_rtx,
+			      gen_rtx_IF_THEN_ELSE (VOIDmode, bcomp,
+						    loc_ref, pc_rtx));
+
+      if (flag_schedule_insns_after_reload && TARGET_BRANCH_HINTS
+	  && spu_expect_op0 && comp_mode == Pmode
+	  && spu_expect_op0 == spu_compare_op0)
+	{
+	  rtx then_reg = gen_reg_rtx (Pmode);
+	  rtx else_reg = gen_reg_rtx (Pmode);
+	  rtx expect_cmp = gen_reg_rtx (Pmode);
+	  rtx hint_target = gen_reg_rtx (Pmode);
+	  rtx branch_label = gen_label_rtx ();
+	  rtx branch_ref = gen_rtx_LABEL_REF (VOIDmode, branch_label);
+	  rtx then_label = gen_label_rtx ();
+	  rtx then_ref = gen_rtx_LABEL_REF (VOIDmode, then_label);
+	  rtx else_label = gen_label_rtx ();
+	  rtx else_ref = gen_rtx_LABEL_REF (VOIDmode, else_label);
+	  rtvec v;
+
+	  emit_move_insn (then_reg, then_ref);
+	  emit_move_insn (else_reg, else_ref);
+	  emit_insn (gen_clgt_si (expect_cmp, spu_expect_op1, const0_rtx));
+	  emit_insn (gen_selb (hint_target, then_reg, else_reg, expect_cmp));
+	  emit_insn (gen_hbr (branch_ref, hint_target));
+
+	  LABEL_NUSES (branch_label)++;
+	  LABEL_PRESERVE_P (branch_label) = 1;
+	  LABEL_NUSES (then_label)++;
+	  LABEL_PRESERVE_P (then_label) = 1;
+	  LABEL_NUSES (else_label)++;
+	  LABEL_PRESERVE_P (else_label) = 1;
+
+	  /* We delete the labels to make sure they don't get used for
+	     anything else.  The machine reorg phase will move them to
+	     the correct place.  We don't try to reuse existing labels
+	     because we move these around later. */
+	  delete_insn (emit_label (branch_label));
+	  delete_insn (emit_label (then_label));
+	  delete_insn (emit_label (else_label));
+
+	  v = rtvec_alloc (5);
+	  RTVEC_ELT (v, 0) = jump_pat;
+	  RTVEC_ELT (v, 1) = gen_rtx_USE (VOIDmode, branch_ref);
+	  RTVEC_ELT (v, 2) = gen_rtx_USE (VOIDmode, then_ref);
+	  RTVEC_ELT (v, 3) = gen_rtx_USE (VOIDmode, else_ref);
+	  RTVEC_ELT (v, 4) = gen_rtx_CLOBBER (VOIDmode,
+					      gen_rtx_REG (SImode,
+							   HBR_REGNUM));
+	  jump_pat = gen_rtx_PARALLEL (VOIDmode, v);
+	}
+
+      emit_jump_insn (jump_pat);
     }
   else if (is_set == 2)
     {
@@ -1132,6 +1218,7 @@
       else
 	emit_move_insn (target, compare_result);
     }
+  spu_expect_op0 = spu_expect_op1 = 0;
 }
 
 HOST_WIDE_INT
@@ -1581,6 +1668,13 @@
       output_addr_const (file, GEN_INT (val));
       return;
 
+    case 'v':
+    case 'w':
+      constant_to_array (mode, x, arr);
+      val = (((arr[0] << 1) + (arr[1] >> 7)) & 0xff) - 127;
+      output_addr_const (file, GEN_INT (code == 'w' ? -val : val));
+      return;
+
     case 0:
       if (xcode == REG)
 	fprintf (file, "%s", reg_names[REGNO (x)]);
@@ -1593,7 +1687,7 @@
       return;
 
       /* unused letters
-	              o qr  uvw yz
+	              o qr  u   yz
 	AB            OPQR  UVWXYZ */
     default:
       output_operand_lossage ("invalid %%xn code");
@@ -2261,16 +2355,25 @@
   if (NOTE_KIND (before) == NOTE_INSN_BASIC_BLOCK)
     before = NEXT_INSN (before);
 
-  branch_label = gen_label_rtx ();
-  LABEL_NUSES (branch_label)++;
-  LABEL_PRESERVE_P (branch_label) = 1;
-  insn = emit_label_before (branch_label, branch);
-  branch_label = gen_rtx_LABEL_REF (VOIDmode, branch_label);
-  SET_BIT (blocks, BLOCK_FOR_INSN (branch)->index);
+  if (INSN_CODE (branch) == CODE_FOR_expect_then
+      || INSN_CODE (branch) == CODE_FOR_expect_else)
+    {
+      HINTED_P (branch) = 1;
+      hint = PREV_INSN (before);
+    }
+  else
+    {
+      branch_label = gen_label_rtx ();
+      LABEL_NUSES (branch_label)++;
+      LABEL_PRESERVE_P (branch_label) = 1;
+      insn = emit_label_before (branch_label, branch);
+      branch_label = gen_rtx_LABEL_REF (VOIDmode, branch_label);
+      SET_BIT (blocks, BLOCK_FOR_INSN (branch)->index);
 
-  hint = emit_insn_before (gen_hbr (branch_label, target), before);
-  recog_memoized (hint);
-  HINTED_P (branch) = 1;
+      hint = emit_insn_before (gen_hbr (branch_label, target), before);
+      recog_memoized (hint);
+      HINTED_P (branch) = 1;
+    }
 
   if (GET_CODE (target) == LABEL_REF)
     HINTED_P (XEXP (target, 0)) = 1;
@@ -2340,6 +2443,12 @@
 	{
 	  rtx lab = 0;
 	  rtx note = find_reg_note (branch, REG_BR_PROB, 0);
+
+	  if (INSN_CODE (branch) == CODE_FOR_expect_then)
+	    return XEXP (src, 1);
+	  if (INSN_CODE (branch) == CODE_FOR_expect_else)
+	    return XEXP (src, 2);
+
 	  if (note)
 	    {
 	      /* If the more probable case is not a fall through, then
@@ -2387,6 +2496,8 @@
 static bool
 insn_clobbers_hbr (rtx insn)
 {
+  if (NONJUMP_INSN_P (insn) && INSN_CODE (insn) == CODE_FOR_hbr)
+    return 1;
   if (INSN_P (insn)
       && GET_CODE (PATTERN (insn)) == PARALLEL)
     {
@@ -2659,6 +2770,9 @@
 		      branch = insn;
 		      branch_addr = insn_addr;
 		      required_dist = spu_hint_dist;
+		      if (INSN_CODE (branch) == CODE_FOR_expect_then
+			  || INSN_CODE (branch) == CODE_FOR_expect_else)
+			required_dist = 0;
 		    }
 		}
 	    }
@@ -2764,6 +2878,58 @@
 
   pad_bb ();
 
+  for (insn = get_insns (); insn; insn = NEXT_INSN (insn))
+    if (NONJUMP_INSN_P (insn) && INSN_CODE (insn) == CODE_FOR_hbr)
+      {
+	/* Adjust the LABEL_REF in a hint when we have inserted a nop
+	   between its branch label and the branch .  We don't move the
+	   label because GCC expects it at the beginning of the block. */
+	rtx unspec = SET_SRC (XVECEXP (PATTERN (insn), 0, 0));
+	rtx label_ref = XVECEXP (unspec, 0, 0);
+	rtx label = XEXP (label_ref, 0);
+	rtx branch;
+	int offset = 0;
+	for (branch = NEXT_INSN (label);
+	     !JUMP_P (branch) && !CALL_P (branch);
+	     branch = NEXT_INSN (branch))
+	  if (NONJUMP_INSN_P (branch))
+	    offset += get_attr_length (branch);
+	if (offset > 0)
+	  XVECEXP (unspec, 0, 0) = plus_constant (label_ref, offset);
+      }
+    else if (JUMP_P (insn) && (INSN_CODE (insn) == CODE_FOR_expect_then
+			       || INSN_CODE (insn) == CODE_FOR_expect_else))
+      {
+	/* __builtin_expect with a non-constant second argument
+	   generates patterns which contain labels that need to be
+	   relocated.  These are generated in spu_emit_branch_or_set. */
+	rtx set0 = XVECEXP (PATTERN (insn), 0, 0);
+	rtx use1 = XVECEXP (PATTERN (insn), 0, 1);
+	rtx use2 = XVECEXP (PATTERN (insn), 0, 2);
+	rtx use3 = XVECEXP (PATTERN (insn), 0, 3);
+	rtx label0 = XEXP (XEXP (set0, 1), 1);
+	rtx label1 = XEXP (XEXP (use1, 0), 0);
+	rtx label2 = XEXP (XEXP (use2, 0), 0);
+	rtx label3 = XEXP (XEXP (use3, 0), 0);
+	if (GET_CODE (label0) == PC)
+	  label0 = XEXP (XEXP (set0, 1), 2);
+	remove_insn (label1);
+	add_insn_before (label1, insn, 0);
+	if (GET_CODE (XEXP (XEXP (set0, 1), 0)) == NE)
+	  {
+	    remove_insn (label2);
+	    add_insn_after (label2, insn, 0);
+	    remove_insn (label3);
+	    add_insn_after (label3, XEXP (label0, 0), 0);
+	  }
+	else
+	  {
+	    remove_insn (label2);
+	    add_insn_after (label2, XEXP (label0, 0), 0);
+	    remove_insn (label3);
+	    add_insn_after (label3, insn, 0);
+	  }
+      }
 
   if (spu_flag_var_tracking)
     {
@@ -3502,6 +3668,72 @@
   return val >= low && val <= high;
 }
 
+/* TRUE when op is an immediate and an exact power of 2, and given that
+   OP is 2^scale, scale >= LOW && scale <= HIGH.  When OP is a vector,
+   all entries must be the same. */
+bool
+exp2_immediate_p (rtx op, enum machine_mode mode, int low, int high)
+{
+  enum machine_mode int_mode;
+  HOST_WIDE_INT val;
+  unsigned char arr[16];
+  int bytes, i, j;
+
+  gcc_assert (GET_CODE (op) == CONST_INT || GET_CODE (op) == CONST_DOUBLE
+	      || GET_CODE (op) == CONST_VECTOR);
+
+  if (GET_CODE (op) == CONST_VECTOR
+      && !const_vector_immediate_p (op))
+    return 0;
+
+  if (GET_MODE (op) != VOIDmode)
+    mode = GET_MODE (op);
+
+  constant_to_array (mode, op, arr);
+
+  if (VECTOR_MODE_P (mode))
+    mode = GET_MODE_INNER (mode);
+
+  bytes = GET_MODE_SIZE (mode);
+  int_mode = mode_for_size (GET_MODE_BITSIZE (mode), MODE_INT, 0);
+
+  /* Check that bytes are repeated. */
+  for (i = bytes; i < 16; i += bytes)
+    for (j = 0; j < bytes; j++)
+      if (arr[j] != arr[i + j])
+	return 0;
+
+  val = arr[0];
+  for (j = 1; j < bytes; j++)
+    val = (val << 8) | arr[j];
+
+  val = trunc_int_for_mode (val, int_mode);
+
+  /* Currently, we only handle SFmode */
+  gcc_assert (mode == SFmode);
+  if (mode == SFmode)
+    {
+      int exp = (val >> 23) - 127;
+      return val > 0 && (val & 0x007fffff) == 0
+	     &&  exp >= low && exp <= high;
+    }
+  return FALSE;
+}
+
+/* Return true if X is a SYMBOL_REF to an __ea qualified variable.  */
+
+static int
+ea_symbol_ref (rtx *px, void *data ATTRIBUTE_UNUSED)
+{
+  rtx x = *px;
+  tree decl;
+
+  return (GET_CODE (x) == SYMBOL_REF
+ 	  && (decl = SYMBOL_REF_DECL (x)) != 0
+ 	  && TREE_CODE (decl) == VAR_DECL
+ 	  && TYPE_ADDR_SPACE (strip_array_types (TREE_TYPE (decl))));
+}
+
 /* We accept:
    - any 32-bit constant (SImode, SFmode)
    - any constant that can be generated with fsmbi (any mode)
@@ -3513,44 +3745,84 @@
 {
   if (GET_CODE (x) == HIGH)
     x = XEXP (x, 0);
-  /* V4SI with all identical symbols is valid. */
-  if (!flag_pic
-      && GET_MODE (x) == V4SImode
-      && (GET_CODE (CONST_VECTOR_ELT (x, 0)) == SYMBOL_REF
-	  || GET_CODE (CONST_VECTOR_ELT (x, 0)) == LABEL_REF
-	  || GET_CODE (CONST_VECTOR_ELT (x, 0)) == CONST))
-    return CONST_VECTOR_ELT (x, 0) == CONST_VECTOR_ELT (x, 1)
-	   && CONST_VECTOR_ELT (x, 1) == CONST_VECTOR_ELT (x, 2)
-	   && CONST_VECTOR_ELT (x, 2) == CONST_VECTOR_ELT (x, 3);
 
-  if (GET_CODE (x) == CONST_VECTOR
-      && !const_vector_immediate_p (x))
+  /* Reject any __ea qualified reference.  These can't appear in
+     instructions but must be forced to the constant pool.  */
+  if (for_each_rtx (&x, ea_symbol_ref, 0))
     return 0;
+
+  if (GET_CODE (x) == CONST_VECTOR)
+    {
+      /* V4SI with all identical symbols is valid. */
+      if (GET_CODE (CONST_VECTOR_ELT (x, 0)) == SYMBOL_REF
+ 	  || GET_CODE (CONST_VECTOR_ELT (x, 0)) == LABEL_REF
+ 	  || GET_CODE (CONST_VECTOR_ELT (x, 0)) == CONST)
+ 	return (!flag_pic
+ 		&& GET_MODE (x) == V4SImode
+ 		&& CONST_VECTOR_ELT (x, 0) == CONST_VECTOR_ELT (x, 1)
+ 		&& CONST_VECTOR_ELT (x, 1) == CONST_VECTOR_ELT (x, 2)
+ 		&& CONST_VECTOR_ELT (x, 2) == CONST_VECTOR_ELT (x, 3));
+
+      if (!const_vector_immediate_p (x))
+	return 0;
+    }
   return 1;
 }
 
 /* Valid address are:
    - symbol_ref, label_ref, const
    - reg
-   - reg + const, where either reg or const is 16 byte aligned
+   - reg + const, where const is 16 byte aligned
    - reg + reg, alignment doesn't matter
   The alignment matters in the reg+const case because lqd and stqd
-  ignore the 4 least significant bits of the const.  (TODO: It might be
-  preferable to allow any alignment and fix it up when splitting.) */
+  ignore the 4 least significant bits of the const.  
+
+  Addresses are handled in 4 phases. 
+  1) from the beginning of rtl expansion until the split0 pass.  Any
+     address is acceptable.  
+  2) The split0 pass. It is responsible for making every load and store
+     valid.  It calls legitimate_address with FOR_SPLIT set to 1.  This
+     is where non-16-byte aligned loads/stores are split into multiple
+     instructions to extract or insert just the part we care about.
+  3) From the split0 pass to the beginning of reload.  During this
+     phase the constant part of an address must be 16 byte aligned, and
+     we don't allow any loads/store of less than 4 bytes.  We also
+     allow a mask of -16 to be part of the address as an optimization.
+  4) From reload until the end.  Reload can change the modes of loads
+     and stores to something smaller than 4-bytes which we need to allow
+     now, and it also adjusts the address to match.  So in this phase we
+     allow that special case.  Still allow addresses with a mask of -16.
+
+  FOR_SPLIT is only set to 1 for phase 2, otherwise it is 0.  */
 int
-spu_legitimate_address (enum machine_mode mode ATTRIBUTE_UNUSED,
-			rtx x, int reg_ok_strict)
+spu_legitimate_address (enum machine_mode mode, rtx x, int reg_ok_strict,
+			int for_split)
 {
-  if (mode == TImode && GET_CODE (x) == AND
-      && GET_CODE (XEXP (x, 1)) == CONST_INT
-      && INTVAL (XEXP (x, 1)) == (HOST_WIDE_INT) -16)
+  int aligned = (split0_completed || for_split)
+    && !reload_in_progress && !reload_completed;
+  int const_aligned = split0_completed || for_split;
+  if (GET_MODE_SIZE (mode) >= 16)
+    aligned = 0;
+  else if (aligned && GET_MODE_SIZE (mode) < 4)
+    return 0;
+  if (split0_completed
+      && (GET_CODE (x) == AND
+	  && GET_CODE (XEXP (x, 1)) == CONST_INT
+	  && INTVAL (XEXP (x, 1)) == (HOST_WIDE_INT) - 16
+	  && !CONSTANT_P (XEXP (x, 0))))
     x = XEXP (x, 0);
   switch (GET_CODE (x))
     {
-    case SYMBOL_REF:
     case LABEL_REF:
-      return !TARGET_LARGE_MEM;
+      return !TARGET_LARGE_MEM && !aligned;
 
+    case SYMBOL_REF:
+      /* Keep __ea references until reload so that spu_expand_mov
+         can see them in MEMs.  */
+      if (ea_symbol_ref (&x, 0))
+        return !reload_in_progress && !reload_completed;
+      return !TARGET_LARGE_MEM && (!aligned || ALIGNED_SYMBOL_REF_P (x));
+
     case CONST:
       if (!TARGET_LARGE_MEM && GET_CODE (XEXP (x, 0)) == PLUS)
 	{
@@ -3559,20 +3831,33 @@
 
 	  /* Accept any symbol_ref + constant, assuming it does not
 	     wrap around the local store addressability limit.  */
+	  if (ea_symbol_ref (&sym, 0))
+	    return 0;
+
 	  if (GET_CODE (sym) == SYMBOL_REF && GET_CODE (cst) == CONST_INT)
-	    return 1;
+	    {
+	      /* Check for alignment if required.  */
+	      if (!aligned)
+		return 1;
+	      if ((INTVAL (cst) & 15) == 0 && ALIGNED_SYMBOL_REF_P (sym))
+		return 1;
+	    }
 	}
       return 0;
 
     case CONST_INT:
+      /* We don't test alignement here.  For an absolute address we
+         assume the user knows what they are doing. */
       return INTVAL (x) >= 0 && INTVAL (x) <= 0x3ffff;
 
     case SUBREG:
       x = XEXP (x, 0);
-      gcc_assert (GET_CODE (x) == REG);
+      if (GET_CODE (x) != REG)
+	return 0;
 
     case REG:
-      return INT_REG_OK_FOR_BASE_P (x, reg_ok_strict);
+      return INT_REG_OK_FOR_BASE_P (x, reg_ok_strict)
+	&& reg_aligned_for_addr (x, 0);
 
     case PLUS:
     case LO_SUM:
@@ -3583,21 +3868,29 @@
 	  op0 = XEXP (op0, 0);
 	if (GET_CODE (op1) == SUBREG)
 	  op1 = XEXP (op1, 0);
-	/* We can't just accept any aligned register because CSE can
-	   change it to a register that is not marked aligned and then
-	   recog will fail.   So we only accept frame registers because
-	   they will only be changed to other frame registers. */
 	if (GET_CODE (op0) == REG
 	    && INT_REG_OK_FOR_BASE_P (op0, reg_ok_strict)
 	    && GET_CODE (op1) == CONST_INT
 	    && INTVAL (op1) >= -0x2000
 	    && INTVAL (op1) <= 0x1fff
-	    && (regno_aligned_for_load (REGNO (op0)) || (INTVAL (op1) & 15) == 0))
+	    && reg_aligned_for_addr (op0, 0)
+	    && (!const_aligned
+		|| (INTVAL (op1) & 15) == 0
+		|| ((reload_in_progress || reload_completed)
+		    && GET_MODE_SIZE (mode) < 4
+		    && (INTVAL (op1) & 15) == 4 - GET_MODE_SIZE (mode))
+		/* Some passes create a fake register for testing valid
+		 * addresses, be more lenient when we see those.  ivopts
+		 * and reload do it. */
+		|| REGNO (op0) == LAST_VIRTUAL_REGISTER + 1
+		|| REGNO (op0) == LAST_VIRTUAL_REGISTER + 2))
 	  return 1;
 	if (GET_CODE (op0) == REG
 	    && INT_REG_OK_FOR_BASE_P (op0, reg_ok_strict)
+	    && reg_aligned_for_addr (op0, 0)
 	    && GET_CODE (op1) == REG
-	    && INT_REG_OK_FOR_INDEX_P (op1, reg_ok_strict))
+	    && INT_REG_OK_FOR_INDEX_P (op1, reg_ok_strict)
+	    && reg_aligned_for_addr (op1, 0))
 	  return 1;
       }
       break;
@@ -3635,7 +3928,7 @@
       else if (GET_CODE (op1) != REG)
 	op1 = force_reg (Pmode, op1);
       x = gen_rtx_PLUS (Pmode, op0, op1);
-      if (spu_legitimate_address (mode, x, 0))
+      if (spu_legitimate_address (mode, x, 0, 0))
 	return x;
     }
   return NULL_RTX;
@@ -4060,60 +4353,16 @@
     }
 }
 
-/* This is called to decide when we can simplify a load instruction.  We
-   must only return true for registers which we know will always be
-   aligned.  Taking into account that CSE might replace this reg with
-   another one that has not been marked aligned.  
-   So this is really only true for frame, stack and virtual registers,
-   which we know are always aligned and should not be adversely effected
-   by CSE.  */
+/* This is called any time we inspect the alignment of a register for
+   addresses.  */
 static int
-regno_aligned_for_load (int regno)
+reg_aligned_for_addr (rtx x, int aligned)
 {
-  return regno == FRAME_POINTER_REGNUM
-    || (frame_pointer_needed && regno == HARD_FRAME_POINTER_REGNUM)
-    || regno == ARG_POINTER_REGNUM
-    || regno == STACK_POINTER_REGNUM
-    || (regno >= FIRST_VIRTUAL_REGISTER 
-	&& regno <= LAST_VIRTUAL_REGISTER);
-}
-
-/* Return TRUE when mem is known to be 16-byte aligned. */
-int
-aligned_mem_p (rtx mem)
-{
-  if (MEM_ALIGN (mem) >= 128)
+  int regno =
+    REGNO (x) < FIRST_PSEUDO_REGISTER ? ORIGINAL_REGNO (x) : REGNO (x);
+  if (!aligned)
     return 1;
-  if (GET_MODE_SIZE (GET_MODE (mem)) >= 16)
-    return 1;
-  if (GET_CODE (XEXP (mem, 0)) == PLUS)
-    {
-      rtx p0 = XEXP (XEXP (mem, 0), 0);
-      rtx p1 = XEXP (XEXP (mem, 0), 1);
-      if (regno_aligned_for_load (REGNO (p0)))
-	{
-	  if (GET_CODE (p1) == REG && regno_aligned_for_load (REGNO (p1)))
-	    return 1;
-	  if (GET_CODE (p1) == CONST_INT && (INTVAL (p1) & 15) == 0)
-	    return 1;
-	}
-    }
-  else if (GET_CODE (XEXP (mem, 0)) == REG)
-    {
-      if (regno_aligned_for_load (REGNO (XEXP (mem, 0))))
-	return 1;
-    }
-  else if (ALIGNED_SYMBOL_REF_P (XEXP (mem, 0)))
-    return 1;
-  else if (GET_CODE (XEXP (mem, 0)) == CONST)
-    {
-      rtx p0 = XEXP (XEXP (XEXP (mem, 0), 0), 0);
-      rtx p1 = XEXP (XEXP (XEXP (mem, 0), 0), 1);
-      if (GET_CODE (p0) == SYMBOL_REF
-	  && GET_CODE (p1) == CONST_INT && (INTVAL (p1) & 15) == 0)
-	return 1;
-    }
-  return 0;
+  return REGNO_POINTER_ALIGN (regno) >= 128;
 }
 
 /* Encode symbol attributes (local vs. global, tls model) of a SYMBOL_REF
@@ -4142,9 +4391,12 @@
 static int
 store_with_one_insn_p (rtx mem)
 {
+  enum machine_mode mode = GET_MODE (mem);
   rtx addr = XEXP (mem, 0);
-  if (GET_MODE (mem) == BLKmode)
+  if (mode == BLKmode)
     return 0;
+  if (GET_MODE_SIZE (mode) >= 16)
+    return 1;
   /* Only static objects. */
   if (GET_CODE (addr) == SYMBOL_REF)
     {
@@ -4168,6 +4420,245 @@
   return 0;
 }
 
+/* Return 1 when the address is not valid for a simple load and store as
+   required by the '_mov*' patterns.   We could make this less strict
+   for loads, but we prefer mem's to look the same so they are more
+   likely to be merged.  */
+static int
+address_needs_split (rtx mem)
+{
+  if (GET_MODE_SIZE (GET_MODE (mem)) < 16
+      && (GET_MODE_SIZE (GET_MODE (mem)) < 4
+	  || !(store_with_one_insn_p (mem)
+	       || mem_is_padded_component_ref (mem))))
+    return 1;
+
+  return 0;
+}
+
+#define EAmode (spu_ea_model != 32 ? DImode : SImode)
+
+rtx cache_fetch;
+rtx cache_fetch_dirty;
+int ea_alias_set = -1;
+
+/* MEM is known to be an __ea qualified memory access.  Emit a call to
+   fetch the ppu memory to local store, and return its address in local
+   store.  */
+
+static void
+ea_load_store (rtx mem, bool is_store, rtx ea_addr, rtx data_addr)
+{
+  if (is_store)
+    {
+      rtx ndirty = GEN_INT (GET_MODE_SIZE (GET_MODE (mem)));
+      if (!cache_fetch_dirty)
+	cache_fetch_dirty = init_one_libfunc ("__cache_fetch_dirty");
+      emit_library_call_value (cache_fetch_dirty, data_addr, LCT_NORMAL, Pmode,
+			       2, ea_addr, EAmode, ndirty, SImode);
+    }
+  else
+    {
+      if (!cache_fetch)
+	cache_fetch = init_one_libfunc ("__cache_fetch");
+      emit_library_call_value (cache_fetch, data_addr, LCT_NORMAL, Pmode,
+			       1, ea_addr, EAmode);
+    }
+}
+
+/* Like ea_load_store, but do the cache tag comparison and, for stores,
+   dirty bit marking, inline.
+
+   The cache control data structure is an array of
+
+   struct __cache_tag_array
+     {
+        unsigned int tag_lo[4];
+        unsigned int tag_hi[4];
+        void *data_pointer[4];
+        int reserved[4];
+        vector unsigned short dirty_bits[4];
+     }  */
+
+static void
+ea_load_store_inline (rtx mem, bool is_store, rtx ea_addr, rtx data_addr)
+{
+  rtx ea_addr_si;
+  HOST_WIDE_INT v;
+  rtx tag_size_sym = gen_rtx_SYMBOL_REF (Pmode, "__cache_tag_array_size");
+  rtx tag_arr_sym = gen_rtx_SYMBOL_REF (Pmode, "__cache_tag_array");
+  rtx index_mask = gen_reg_rtx (SImode);
+  rtx tag_arr = gen_reg_rtx (Pmode);
+  rtx splat_mask = gen_reg_rtx (TImode);
+  rtx splat = gen_reg_rtx (V4SImode);
+  rtx splat_hi = NULL_RTX;
+  rtx tag_index = gen_reg_rtx (Pmode);
+  rtx block_off = gen_reg_rtx (SImode);
+  rtx tag_addr = gen_reg_rtx (Pmode);
+  rtx tag = gen_reg_rtx (V4SImode);
+  rtx cache_tag = gen_reg_rtx (V4SImode);
+  rtx cache_tag_hi = NULL_RTX;
+  rtx cache_ptrs = gen_reg_rtx (TImode);
+  rtx cache_ptrs_si = gen_reg_rtx (SImode);
+  rtx tag_equal = gen_reg_rtx (V4SImode);
+  rtx tag_equal_hi = NULL_RTX;
+  rtx tag_eq_pack = gen_reg_rtx (V4SImode);
+  rtx tag_eq_pack_si = gen_reg_rtx (SImode);
+  rtx eq_index = gen_reg_rtx (SImode);
+  rtx bcomp, hit_label, hit_ref, cont_label, insn;
+
+  if (spu_ea_model != 32)
+    {
+      splat_hi = gen_reg_rtx (V4SImode);
+      cache_tag_hi = gen_reg_rtx (V4SImode);
+      tag_equal_hi = gen_reg_rtx (V4SImode);
+    }
+
+  emit_move_insn (index_mask, plus_constant (tag_size_sym, -128));
+  emit_move_insn (tag_arr, tag_arr_sym);
+  v = 0x0001020300010203LL;
+  emit_move_insn (splat_mask, immed_double_const (v, v, TImode));
+  ea_addr_si = ea_addr;
+  if (spu_ea_model != 32)
+    ea_addr_si = convert_to_mode (SImode, ea_addr, 1);
+
+  /* tag_index = ea_addr & (tag_array_size - 128)  */
+  emit_insn (gen_andsi3 (tag_index, ea_addr_si, index_mask));
+
+  /* splat ea_addr to all 4 slots.  */
+  emit_insn (gen_shufb (splat, ea_addr_si, ea_addr_si, splat_mask));
+  /* Similarly for high 32 bits of ea_addr.  */
+  if (spu_ea_model != 32)
+    emit_insn (gen_shufb (splat_hi, ea_addr, ea_addr, splat_mask));
+
+  /* block_off = ea_addr & 127  */
+  emit_insn (gen_andsi3 (block_off, ea_addr_si, spu_const (SImode, 127)));
+
+  /* tag_addr = tag_arr + tag_index  */
+  emit_insn (gen_addsi3 (tag_addr, tag_arr, tag_index));
+
+  /* Read cache tags.  */
+  emit_move_insn (cache_tag, gen_rtx_MEM (V4SImode, tag_addr));
+  if (spu_ea_model != 32)
+    emit_move_insn (cache_tag_hi, gen_rtx_MEM (V4SImode,
+					       plus_constant (tag_addr, 16)));
+
+  /* tag = ea_addr & -128  */
+  emit_insn (gen_andv4si3 (tag, splat, spu_const (V4SImode, -128)));
+
+  /* Read all four cache data pointers.  */
+  emit_move_insn (cache_ptrs, gen_rtx_MEM (TImode,
+					   plus_constant (tag_addr, 32)));
+
+  /* Compare tags.  */
+  emit_insn (gen_ceq_v4si (tag_equal, tag, cache_tag));
+  if (spu_ea_model != 32)
+    {
+      emit_insn (gen_ceq_v4si (tag_equal_hi, splat_hi, cache_tag_hi));
+      emit_insn (gen_andv4si3 (tag_equal, tag_equal, tag_equal_hi));
+    }
+
+  /* At most one of the tags compare equal, so tag_equal has one
+     32-bit slot set to all 1's, with the other slots all zero.
+     gbb picks off low bit from each byte in the 128-bit registers,
+     so tag_eq_pack is one of 0xf000, 0x0f00, 0x00f0, 0x000f, assuming
+     we have a hit.  */
+  emit_insn (gen_spu_gbb (tag_eq_pack, spu_gen_subreg (V16QImode, tag_equal)));
+  emit_insn (gen_spu_convert (tag_eq_pack_si, tag_eq_pack));
+
+  /* So counting leading zeros will set eq_index to 16, 20, 24 or 28.  */
+  emit_insn (gen_clzsi2 (eq_index, tag_eq_pack_si));
+
+  /* Allowing us to rotate the corresponding cache data pointer to slot0.
+     (rotating eq_index mod 16 bytes).  */
+  emit_insn (gen_rotqby_ti (cache_ptrs, cache_ptrs, eq_index));
+  emit_insn (gen_spu_convert (cache_ptrs_si, cache_ptrs));
+
+  /* Add block offset to form final data address.  */
+  emit_insn (gen_addsi3 (data_addr, cache_ptrs_si, block_off));
+
+  /* Check that we did hit.  */
+  hit_label = gen_label_rtx ();
+  hit_ref = gen_rtx_LABEL_REF (VOIDmode, hit_label);
+  bcomp = gen_rtx_NE (SImode, tag_eq_pack_si, const0_rtx);
+  insn = emit_jump_insn (gen_rtx_SET (VOIDmode, pc_rtx,
+				      gen_rtx_IF_THEN_ELSE (VOIDmode, bcomp,
+							    hit_ref, pc_rtx)));
+  /* Say that this branch is very likely to happen.  */
+  v = REG_BR_PROB_BASE - REG_BR_PROB_BASE / 100 - 1;
+  REG_NOTES (insn)
+    = gen_rtx_EXPR_LIST (REG_BR_PROB, GEN_INT (v), REG_NOTES (insn));
+
+  ea_load_store (mem, is_store, ea_addr, data_addr);
+  cont_label = gen_label_rtx ();
+  emit_jump_insn (gen_jump (cont_label));
+  emit_barrier ();
+
+  emit_label (hit_label);
+
+  if (is_store)
+    {
+      HOST_WIDE_INT v_hi;
+      rtx dirty_bits = gen_reg_rtx (TImode);
+      rtx dirty_off = gen_reg_rtx (SImode);
+      rtx dirty_128 = gen_reg_rtx (TImode);
+      rtx neg_block_off = gen_reg_rtx (SImode);
+
+      /* Set up mask with one dirty bit per byte of the mem we are
+	 writing, starting from top bit.  */
+      v_hi = v = -1;
+      v <<= (128 - GET_MODE_SIZE (GET_MODE (mem))) & 63;
+      if ((128 - GET_MODE_SIZE (GET_MODE (mem))) >= 64)
+	{
+	  v_hi = v;
+	  v = 0;
+	}
+      emit_move_insn (dirty_bits, immed_double_const (v, v_hi, TImode));
+
+      /* Form index into cache dirty_bits.  eq_index is one of
+	 0x10, 0x14, 0x18 or 0x1c.  Multiplying by 4 gives us
+	 0x40, 0x50, 0x60 or 0x70 which just happens to be the
+	 offset to each of the four dirty_bits elements.  */
+      emit_insn (gen_ashlsi3 (dirty_off, eq_index, spu_const (SImode, 2)));
+
+      emit_insn (gen_spu_lqx (dirty_128, tag_addr, dirty_off));
+
+      /* Rotate bit mask to proper bit.  */
+      emit_insn (gen_negsi2 (neg_block_off, block_off));
+      emit_insn (gen_rotqbybi_ti (dirty_bits, dirty_bits, neg_block_off));
+      emit_insn (gen_rotqbi_ti (dirty_bits, dirty_bits, neg_block_off));
+
+      /* Or in the new dirty bits.  */
+      emit_insn (gen_iorti3 (dirty_128, dirty_bits, dirty_128));
+
+      /* Store.  */
+      emit_insn (gen_spu_stqx (dirty_128, tag_addr, dirty_off));
+    }
+
+  emit_label (cont_label);
+}
+
+static rtx
+expand_ea_mem (rtx mem, bool is_store)
+{
+  rtx ea_addr;
+  rtx data_addr = gen_reg_rtx (Pmode);
+
+  ea_addr = force_reg (EAmode, XEXP (mem, 0));
+  if (optimize_size || optimize == 0)
+    ea_load_store (mem, is_store, ea_addr, data_addr);
+  else
+    ea_load_store_inline (mem, is_store, ea_addr, data_addr);
+
+  mem = change_address (mem, VOIDmode, data_addr);
+
+  if (ea_alias_set == -1)
+    ea_alias_set = new_alias_set ();
+  set_mem_alias_set (mem, 0);
+  set_mem_alias_set (mem, ea_alias_set);
+  return mem;
+}
+
 int
 spu_expand_mov (rtx * ops, enum machine_mode mode)
 {
@@ -4215,24 +4706,15 @@
     }
   else
     {
-      if (GET_CODE (ops[0]) == MEM)
+      if (MEM_P (ops[0]))
 	{
-	  if (!spu_valid_move (ops))
-	    {
-	      emit_insn (gen_store (ops[0], ops[1], gen_reg_rtx (TImode),
-				    gen_reg_rtx (TImode)));
-	      return 1;
-	    }
+ 	  if (MEM_ADDR_SPACE (ops[0]))
+ 	    ops[0] = expand_ea_mem (ops[0], true);
 	}
-      else if (GET_CODE (ops[1]) == MEM)
+      else if (MEM_P (ops[1]))
 	{
-	  if (!spu_valid_move (ops))
-	    {
-	      emit_insn (gen_load
-			 (ops[0], ops[1], gen_reg_rtx (TImode),
-			  gen_reg_rtx (SImode)));
-	      return 1;
-	    }
+ 	  if (MEM_ADDR_SPACE (ops[1]))
+ 	    ops[1] = expand_ea_mem (ops[1], false);
 	}
       /* Catch the SImode immediates greater than 0x7fffffff, and sign
          extend them. */
@@ -4249,7 +4731,7 @@
   return 0;
 }
 
-void
+int
 spu_split_load (rtx * ops)
 {
   enum machine_mode mode = GET_MODE (ops[0]);
@@ -4257,10 +4739,24 @@
   int rot_amt;
 
   addr = XEXP (ops[1], 0);
+  gcc_assert (GET_CODE (addr) != AND);
 
+  if (!address_needs_split (ops[1]))
+    {
+      addr = XEXP (ops[1], 0);
+      if (spu_legitimate_address (mode, addr, 0, 1))
+	return 0;
+      ops[1] = change_address (ops[1], VOIDmode, force_reg (Pmode, addr));
+      emit_move_insn (ops[0], ops[1]);
+      return 1;
+    }
+
   rot = 0;
   rot_amt = 0;
-  if (GET_CODE (addr) == PLUS)
+
+  if (MEM_ALIGN (ops[1]) >= 128)
+    /* Address is already aligned; simply perform a TImode load.  */;
+  else if (GET_CODE (addr) == PLUS)
     {
       /* 8 cases:
          aligned reg   + aligned reg     => lqx
@@ -4274,13 +4770,33 @@
        */
       p0 = XEXP (addr, 0);
       p1 = XEXP (addr, 1);
-      if (REG_P (p0) && !regno_aligned_for_load (REGNO (p0)))
+      if (!reg_aligned_for_addr (p0, 1))
 	{
-	  if (REG_P (p1) && !regno_aligned_for_load (REGNO (p1)))
+	  if (GET_CODE (p1) == REG && !reg_aligned_for_addr (p1, 1))
 	    {
-	      emit_insn (gen_addsi3 (ops[3], p0, p1));
-	      rot = ops[3];
+	      rot = gen_reg_rtx (SImode);
+	      emit_insn (gen_addsi3 (rot, p0, p1));
 	    }
+	  else if (GET_CODE (p1) == CONST_INT && (INTVAL (p1) & 15))
+	    {
+	      if (INTVAL (p1) > 0
+		  && INTVAL (p1) * BITS_PER_UNIT < REG_ALIGN (p0))
+		{
+		  rot = gen_reg_rtx (SImode);
+		  emit_insn (gen_addsi3 (rot, p0, p1));
+		  addr = p0;
+		}
+	      else
+		{
+		  rtx x = gen_reg_rtx (SImode);
+		  emit_move_insn (x, p1);
+		  if (!spu_arith_operand (p1, SImode))
+		    p1 = x;
+		  rot = gen_reg_rtx (SImode);
+		  emit_insn (gen_addsi3 (rot, p0, p1));
+		  addr = gen_rtx_PLUS (Pmode, p0, x);
+		}
+	    }
 	  else
 	    rot = p0;
 	}
@@ -4289,16 +4805,21 @@
 	  if (GET_CODE (p1) == CONST_INT && (INTVAL (p1) & 15))
 	    {
 	      rot_amt = INTVAL (p1) & 15;
-	      p1 = GEN_INT (INTVAL (p1) & -16);
-	      addr = gen_rtx_PLUS (SImode, p0, p1);
+	      if (INTVAL (p1) & -16)
+		{
+		  p1 = GEN_INT (INTVAL (p1) & -16);
+		  addr = gen_rtx_PLUS (SImode, p0, p1);
+		}
+	      else
+		addr = p0;
 	    }
-	  else if (REG_P (p1) && !regno_aligned_for_load (REGNO (p1)))
+	  else if (GET_CODE (p1) == REG && !reg_aligned_for_addr (p1, 1))
 	    rot = p1;
 	}
     }
   else if (GET_CODE (addr) == REG)
     {
-      if (!regno_aligned_for_load (REGNO (addr)))
+      if (!reg_aligned_for_addr (addr, 1))
 	rot = addr;
     }
   else if (GET_CODE (addr) == CONST)
@@ -4317,7 +4838,10 @@
 	    addr = XEXP (XEXP (addr, 0), 0);
 	}
       else
-	rot = addr;
+	{
+	  rot = gen_reg_rtx (Pmode);
+	  emit_move_insn (rot, addr);
+	}
     }
   else if (GET_CODE (addr) == CONST_INT)
     {
@@ -4325,7 +4849,10 @@
       addr = GEN_INT (rot_amt & -16);
     }
   else if (!ALIGNED_SYMBOL_REF_P (addr))
-    rot = addr;
+    {
+      rot = gen_reg_rtx (Pmode);
+      emit_move_insn (rot, addr);
+    }
 
   if (GET_MODE_SIZE (mode) < 4)
     rot_amt += GET_MODE_SIZE (mode) - 4;
@@ -4334,16 +4861,24 @@
 
   if (rot && rot_amt)
     {
-      emit_insn (gen_addsi3 (ops[3], rot, GEN_INT (rot_amt)));
-      rot = ops[3];
+      rtx x = gen_reg_rtx (SImode);
+      emit_insn (gen_addsi3 (x, rot, GEN_INT (rot_amt)));
+      rot = x;
       rot_amt = 0;
     }
 
-  load = ops[2];
+  /* If the source is properly aligned, we don't need to split this insn into
+     a TImode load plus a _spu_convert.  However, we want to perform the split
+     anyway when optimizing to make the MEMs look the same as those used for
+     stores so they are more easily merged.  When *not* optimizing, that will
+     not happen anyway, so we prefer to avoid generating the _spu_convert.  */
+  if (!rot && !rot_amt && !optimize)
+    return 0;
 
-  addr = gen_rtx_AND (SImode, copy_rtx (addr), GEN_INT (-16));
-  mem = change_address (ops[1], TImode, addr);
+  load = gen_reg_rtx (TImode);
 
+  mem = change_address (ops[1], TImode, copy_rtx (addr));
+
   emit_insn (gen_movti (load, mem));
 
   if (rot)
@@ -4351,23 +4886,31 @@
   else if (rot_amt)
     emit_insn (gen_rotlti3 (load, load, GEN_INT (rot_amt * 8)));
 
-  if (reload_completed)
-    emit_move_insn (ops[0], gen_rtx_REG (GET_MODE (ops[0]), REGNO (load)));
-  else
-    emit_insn (gen_spu_convert (ops[0], load));
+  emit_insn (gen_spu_convert (ops[0], load));
+  return 1;
 }
 
-void
+int
 spu_split_store (rtx * ops)
 {
   enum machine_mode mode = GET_MODE (ops[0]);
-  rtx pat = ops[2];
-  rtx reg = ops[3];
+  rtx reg;
   rtx addr, p0, p1, p1_lo, smem;
   int aform;
   int scalar;
 
+  if (!address_needs_split (ops[0]))
+    {
+      addr = XEXP (ops[0], 0);
+      if (spu_legitimate_address (mode, addr, 0, 1))
+	return 0;
+      ops[0] = change_address (ops[0], VOIDmode, force_reg (Pmode, addr));
+      emit_move_insn (ops[0], ops[1]);
+      return 1;
+    }
+
   addr = XEXP (ops[0], 0);
+  gcc_assert (GET_CODE (addr) != AND);
 
   if (GET_CODE (addr) == PLUS)
     {
@@ -4379,7 +4922,7 @@
          unaligned reg + aligned reg     => lqx, c?x, shuf, stqx
          unaligned reg + unaligned reg   => lqx, c?x, shuf, stqx
          unaligned reg + aligned const   => lqd, c?d, shuf, stqx
-         unaligned reg + unaligned const -> not allowed by legitimate address
+         unaligned reg + unaligned const -> lqx, c?d, shuf, stqx
        */
       aform = 0;
       p0 = XEXP (addr, 0);
@@ -4387,8 +4930,20 @@
       if (GET_CODE (p0) == REG && GET_CODE (p1) == CONST_INT)
 	{
 	  p1_lo = GEN_INT (INTVAL (p1) & 15);
-	  p1 = GEN_INT (INTVAL (p1) & -16);
-	  addr = gen_rtx_PLUS (SImode, p0, p1);
+	  if (reg_aligned_for_addr (p0, 1))
+	    {
+	      p1 = GEN_INT (INTVAL (p1) & -16);
+	      if (p1 == const0_rtx)
+		addr = p0;
+	      else
+		addr = gen_rtx_PLUS (SImode, p0, p1);
+	    }
+	  else
+	    {
+	      rtx x = gen_reg_rtx (SImode);
+	      emit_move_insn (x, p1);
+	      addr = gen_rtx_PLUS (SImode, p0, x);
+	    }
 	}
     }
   else if (GET_CODE (addr) == REG)
@@ -4405,31 +4960,34 @@
       p1_lo = addr;
       if (ALIGNED_SYMBOL_REF_P (addr))
 	p1_lo = const0_rtx;
-      else if (GET_CODE (addr) == CONST)
+      else if (GET_CODE (addr) == CONST
+	       && GET_CODE (XEXP (addr, 0)) == PLUS
+	       && ALIGNED_SYMBOL_REF_P (XEXP (XEXP (addr, 0), 0))
+	       && GET_CODE (XEXP (XEXP (addr, 0), 1)) == CONST_INT)
 	{
-	  if (GET_CODE (XEXP (addr, 0)) == PLUS
-	      && ALIGNED_SYMBOL_REF_P (XEXP (XEXP (addr, 0), 0))
-	      && GET_CODE (XEXP (XEXP (addr, 0), 1)) == CONST_INT)
-	    {
-	      HOST_WIDE_INT v = INTVAL (XEXP (XEXP (addr, 0), 1));
-	      if ((v & -16) != 0)
-		addr = gen_rtx_CONST (Pmode,
-				      gen_rtx_PLUS (Pmode,
-						    XEXP (XEXP (addr, 0), 0),
-						    GEN_INT (v & -16)));
-	      else
-		addr = XEXP (XEXP (addr, 0), 0);
-	      p1_lo = GEN_INT (v & 15);
-	    }
+	  HOST_WIDE_INT v = INTVAL (XEXP (XEXP (addr, 0), 1));
+	  if ((v & -16) != 0)
+	    addr = gen_rtx_CONST (Pmode,
+				  gen_rtx_PLUS (Pmode,
+						XEXP (XEXP (addr, 0), 0),
+						GEN_INT (v & -16)));
+	  else
+	    addr = XEXP (XEXP (addr, 0), 0);
+	  p1_lo = GEN_INT (v & 15);
 	}
       else if (GET_CODE (addr) == CONST_INT)
 	{
 	  p1_lo = GEN_INT (INTVAL (addr) & 15);
 	  addr = GEN_INT (INTVAL (addr) & -16);
 	}
+      else
+	{
+	  p1_lo = gen_reg_rtx (SImode);
+	  emit_move_insn (p1_lo, addr);
+	}
     }
 
-  addr = gen_rtx_AND (SImode, copy_rtx (addr), GEN_INT (-16));
+  reg = gen_reg_rtx (TImode);
 
   scalar = store_with_one_insn_p (ops[0]);
   if (!scalar)
@@ -4439,11 +4997,12 @@
          possible, and copying the flags will prevent that in certain
          cases, e.g. consider the volatile flag. */
 
+      rtx pat = gen_reg_rtx (TImode);
       rtx lmem = change_address (ops[0], TImode, copy_rtx (addr));
       set_mem_alias_set (lmem, 0);
       emit_insn (gen_movti (reg, lmem));
 
-      if (!p0 || regno_aligned_for_load (REGNO (p0)))
+      if (!p0 || reg_aligned_for_addr (p0, 1))
 	p0 = stack_pointer_rtx;
       if (!p1_lo)
 	p1_lo = const0_rtx;
@@ -4451,17 +5010,6 @@
       emit_insn (gen_cpat (pat, p0, p1_lo, GEN_INT (GET_MODE_SIZE (mode))));
       emit_insn (gen_shufb (reg, ops[1], reg, pat));
     }
-  else if (reload_completed)
-    {
-      if (GET_CODE (ops[1]) == REG)
-	emit_move_insn (reg, gen_rtx_REG (GET_MODE (reg), REGNO (ops[1])));
-      else if (GET_CODE (ops[1]) == SUBREG)
-	emit_move_insn (reg,
-			gen_rtx_REG (GET_MODE (reg),
-				     REGNO (SUBREG_REG (ops[1]))));
-      else
-	abort ();
-    }
   else
     {
       if (GET_CODE (ops[1]) == REG)
@@ -4473,15 +5021,16 @@
     }
 
   if (GET_MODE_SIZE (mode) < 4 && scalar)
-    emit_insn (gen_shlqby_ti
-	       (reg, reg, GEN_INT (4 - GET_MODE_SIZE (mode))));
+    emit_insn (gen_ashlti3
+	       (reg, reg, GEN_INT (32 - GET_MODE_BITSIZE (mode))));
 
-  smem = change_address (ops[0], TImode, addr);
+  smem = change_address (ops[0], TImode, copy_rtx (addr));
   /* We can't use the previous alias set because the memory has changed
      size and can potentially overlap objects of other types.  */
   set_mem_alias_set (smem, 0);
 
   emit_insn (gen_movti (smem, reg));
+  return 1;
 }
 
 /* Return TRUE if X is MEM which is a struct member reference
@@ -4580,37 +5129,6 @@
     }
 }
 
-int
-spu_valid_move (rtx * ops)
-{
-  enum machine_mode mode = GET_MODE (ops[0]);
-  if (!register_operand (ops[0], mode) && !register_operand (ops[1], mode))
-    return 0;
-
-  /* init_expr_once tries to recog against load and store insns to set
-     the direct_load[] and direct_store[] arrays.  We always want to
-     consider those loads and stores valid.  init_expr_once is called in
-     the context of a dummy function which does not have a decl. */
-  if (cfun->decl == 0)
-    return 1;
-
-  /* Don't allows loads/stores which would require more than 1 insn.
-     During and after reload we assume loads and stores only take 1
-     insn. */
-  if (GET_MODE_SIZE (mode) < 16 && !reload_in_progress && !reload_completed)
-    {
-      if (GET_CODE (ops[0]) == MEM
-	  && (GET_MODE_SIZE (mode) < 4
-	      || !(store_with_one_insn_p (ops[0])
-		   || mem_is_padded_component_ref (ops[0]))))
-	return 0;
-      if (GET_CODE (ops[1]) == MEM
-	  && (GET_MODE_SIZE (mode) < 4 || !aligned_mem_p (ops[1])))
-	return 0;
-    }
-  return 1;
-}
-
 /* Return TRUE if x is a CONST_INT, CONST_DOUBLE or CONST_VECTOR that
    can be generated using the fsmbi instruction. */
 int
@@ -6228,6 +6746,26 @@
   return true;
 }
 
+static enum machine_mode
+spu_ea_pointer_mode (int addrspace)
+{
+  switch (addrspace)
+    {
+    case 0:
+      return ptr_mode;
+    case 1:
+      return (spu_ea_model == 64 ? DImode : ptr_mode);
+    default:
+      gcc_unreachable ();
+    }
+}
+
+static bool
+spu_valid_pointer_mode (enum machine_mode mode)
+{
+  return (mode == ptr_mode || mode == Pmode || mode == spu_ea_pointer_mode (1));
+}
+
 /* Count the total number of instructions in each pipe and return the
    maximum, which is used as the Minimum Iteration Interval (MII)
    in the modulo scheduler.  get_pipe() will return -2, -1, 0, or 1.
@@ -6263,11 +6801,25 @@
 void
 spu_init_expanders (void)
 {   
-  /* HARD_FRAME_REGISTER is only 128 bit aligned when
-   * frame_pointer_needed is true.  We don't know that until we're
-   * expanding the prologue. */
   if (cfun)
-    REGNO_POINTER_ALIGN (HARD_FRAME_POINTER_REGNUM) = 8;
+    {
+      rtx r0, r1;
+      /* HARD_FRAME_REGISTER is only 128 bit aligned when
+         frame_pointer_needed is true.  We don't know that until we're
+         expanding the prologue. */
+      REGNO_POINTER_ALIGN (HARD_FRAME_POINTER_REGNUM) = 8;
+
+      /* A number of passes use LAST_VIRTUAL_REGISTER+1 and
+         LAST_VIRTUAL_REGISTER+2 to test the back-end.  We want to
+         handle those cases specially, so we reserve those two registers
+         here by generating them. */
+      r0 = gen_reg_rtx (SImode);
+      r1 = gen_reg_rtx (SImode);
+      mark_reg_pointer (r0, 128);
+      mark_reg_pointer (r1, 128);
+      gcc_assert (REGNO (r0) == LAST_VIRTUAL_REGISTER + 1
+		  && REGNO (r1) == LAST_VIRTUAL_REGISTER + 2);
+    }
 }
 
 static enum machine_mode
@@ -6307,8 +6859,89 @@
   /* .toe needs to have type @nobits.  */
   if (strcmp (name, ".toe") == 0)
     return SECTION_BSS;
+  if (strcmp (name, "._ea") == 0)
+    return SECTION_WRITE | SECTION_DEBUG;
   return default_section_type_flags (decl, name, reloc);
 }
 
+const char *
+spu_addr_space_name (int addrspace)
+{
+  gcc_assert (addrspace > 0 && addrspace <= 1);
+  return (spu_address_spaces [addrspace].name);
+}
+
+static
+rtx (* spu_addr_space_conversion_rtl (int from, int to)) (rtx, rtx)
+{
+  gcc_assert ((from == 0 && to == 1) || (from == 1 && to == 0));
+
+  if (to == 0)
+    return spu_address_spaces[1].to_generic_insn;
+  else if (to == 1)
+    return spu_address_spaces[1].from_generic_insn;
+
+  return 0;
+}
+
+static
+bool spu_valid_addr_space (const_tree value)
+{
+  int i;
+  if (!value)
+    return false;
+
+  for (i = 0; spu_address_spaces[i].name; i++)
+    if (strcmp (IDENTIFIER_POINTER (value), spu_address_spaces[i].name) == 0)
+      return true;
+  return false;
+}
+
+static
+unsigned char spu_addr_space_number (tree ident)
+{
+  int i;
+  if (!ident)
+    return 0;
+
+  for (i = 0; spu_address_spaces[i].name; i++)
+    if (strcmp (IDENTIFIER_POINTER (ident), spu_address_spaces[i].name) == 0)
+      return i;
+
+  gcc_unreachable ();
+}
+
+/* Generate a constant or register which contains 2^SCALE.  We assume
+   the result is valid for MODE.  Currently, MODE must be V4SFmode and
+   SCALE must be SImode. */
+rtx
+spu_gen_exp2 (enum machine_mode mode, rtx scale)
+{
+  gcc_assert (mode == V4SFmode);
+  gcc_assert (GET_MODE (scale) == SImode || GET_CODE (scale) == CONST_INT);
+  if (GET_CODE (scale) != CONST_INT)
+    {
+      /* unsigned int exp = (127 + scale) << 23;
+	__vector float m = (__vector float) spu_splats (exp); */
+      rtx reg = force_reg (SImode, scale);
+      rtx exp = gen_reg_rtx (SImode);
+      rtx mul = gen_reg_rtx (mode);
+      emit_insn (gen_addsi3 (exp, reg, GEN_INT (127)));
+      emit_insn (gen_ashlsi3 (exp, exp, GEN_INT (23)));
+      emit_insn (gen_spu_splats (mul, gen_rtx_SUBREG (GET_MODE_INNER (mode), exp, 0)));
+      return mul;
+    }
+  else 
+    {
+      HOST_WIDE_INT exp = 127 + INTVAL (scale);
+      unsigned char arr[16];
+      arr[0] = arr[4] = arr[8] = arr[12] = exp >> 1;
+      arr[1] = arr[5] = arr[9] = arr[13] = exp << 7;
+      arr[2] = arr[6] = arr[10] = arr[14] = 0;
+      arr[3] = arr[7] = arr[11] = arr[15] = 0;
+      return array_to_constant (mode, arr);
+    }
+}
+
 #include "gt-spu.h"
 
Index: gcc/config/spu/spu.h
===================================================================
--- gcc/config/spu/spu.h	(.../gcc-4_3-branch)	(revision 156795)
+++ gcc/config/spu/spu.h	(.../cell-4_3-branch)	(revision 156810)
@@ -255,6 +255,11 @@
 #define INT_REG_OK_FOR_BASE_P(X,STRICT) \
 	((!(STRICT) || REGNO_OK_FOR_BASE_P (REGNO (X))))
 
+#define REG_ALIGN(X) \
+	(REG_POINTER(X) \
+	 	? REGNO_POINTER_ALIGN (ORIGINAL_REGNO (X)) \
+		: 0)
+
 #define PREFERRED_RELOAD_CLASS(X,CLASS)  (CLASS)
 
 #define CLASS_MAX_NREGS(CLASS, MODE)	\
@@ -440,7 +445,7 @@
 #endif
 
 #define GO_IF_LEGITIMATE_ADDRESS(MODE, X, ADDR)			\
-    { if (spu_legitimate_address (MODE, X, REG_OK_STRICT_FLAG))	\
+    { if (spu_legitimate_address (MODE, X, REG_OK_STRICT_FLAG, 0))	\
 	goto ADDR;						\
     }
 
@@ -471,9 +476,9 @@
 
 /* Sections */
 
-#define TEXT_SECTION_ASM_OP ".text"
+#define TEXT_SECTION_ASM_OP "\t.text"
 
-#define DATA_SECTION_ASM_OP ".data"
+#define DATA_SECTION_ASM_OP "\t.data"
 
 #define JUMP_TABLES_IN_TEXT_SECTION 1
 
@@ -514,6 +519,17 @@
 #define ASM_OUTPUT_LABELREF(FILE, NAME) \
   asm_fprintf (FILE, "%U%s", default_strip_name_encoding (NAME))
 
+#define ASM_OUTPUT_SYMBOL_REF(FILE, X) \
+  do								\
+    {								\
+      tree decl;						\
+      assemble_name (FILE, XSTR (X, 0));			\
+      if ((decl = SYMBOL_REF_DECL (X)) != 0			\
+	  && TREE_CODE (decl) == VAR_DECL			\
+	  && TYPE_ADDR_SPACE (strip_array_types (TREE_TYPE (decl))))	\
+	fputs ("@ppu", FILE);					\
+    } while (0)
+
 
 /* Instruction Output */
 #define REGISTER_NAMES \
@@ -633,8 +649,14 @@
    conditional branches. */
 extern GTY(()) rtx spu_compare_op0;
 extern GTY(()) rtx spu_compare_op1;
+extern GTY(()) rtx spu_expect_op0;
+extern GTY(()) rtx spu_expect_op1;
 
+#define SPLIT_BEFORE_CSE2 1
 
+#define ADDRESSES_NEVER_TRAP 1
+
+
 /* Builtins.  */
 
 enum spu_builtin_type
Index: gcc/config/spu/spu-elf.h
===================================================================
--- gcc/config/spu/spu-elf.h	(.../gcc-4_3-branch)	(revision 156795)
+++ gcc/config/spu/spu-elf.h	(.../cell-4_3-branch)	(revision 156810)
@@ -66,10 +66,26 @@
 
 #define EH_FRAME_IN_DATA_SECTION 1
 
+#define DRIVER_SELF_SPECS "\
+  %{mcache-size=128   : -lgcc_cache128k ; \
+    mcache-size=64    : -lgcc_cache64k ; \
+    mcache-size=32    : -lgcc_cache32k ; \
+    mcache-size=16    : -lgcc_cache16k ; \
+    mcache-size=8     : -lgcc_cache8k ; \
+    		      : -lgcc_cache64k } \
+  %<mcache-size=* \
+  %{mno-atomic-updates:-lgcc_cachemgr_nonatomic; :-lgcc_cachemgr} \
+  %<matomic-updates %<mno-atomic-updates"
+
 #define LINK_SPEC "%{mlarge-mem: --defsym __stack=0xfffffff0 }"
 
-#define LIB_SPEC \
-	"-( %{!shared:%{g*:-lg}} -lc -lgloss -)"
+/* Match each of the mutually exclusive cache<n>k libraries because
+   lgcc_cache* did not seem to work -- perhaps a bug in the specs
+   handling?  */
+#define LIB_SPEC "-( %{!shared:%{g*:-lg}} -lc -lgloss -) \
+    %{lgcc_cachemgr*:-lgcc_cachemgr%*} \
+    %{lgcc_cache128k} %{lgcc_cache64k} %{lgcc_cache32k} \
+    %{lgcc_cache16k} %{lgcc_cache8k}"
 
 /* Turn off warnings in the assembler too. */
 #undef ASM_SPEC
Index: gcc/config/spu/constraints.md
===================================================================
--- gcc/config/spu/constraints.md	(.../gcc-4_3-branch)	(revision 156795)
+++ gcc/config/spu/constraints.md	(.../cell-4_3-branch)	(revision 156810)
@@ -16,8 +16,14 @@
 ;; <http://www.gnu.org/licenses/>.
 
 
-;; GCC standard constraints:  g, i, m, n, o, p, r, s, E-H, I-P, V, X
-;; unused for SPU:  E-H, L, Q, d, e, h, q, t-z
+;;       ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz
+;; GCC:      ffffiiiiiiii     x x        x x   xxxx xx
+;; SPU:  xxxx    xxx xxxx xxxx x xxx xx x   xxx         xx
+;; FREE:     ffff   i    a          a  a  a        a  aa  aaa
+;; x - used
+;; a - available
+;; i - available for integer immediates
+;; f - available for floating point immediates
 
 ;; For most immediate constraints we have 3 variations to deal with the
 ;; fact const_int has no mode.  One variation treats const_int as 32 bit,
@@ -159,4 +165,15 @@
 		    && INTVAL (XEXP (op, 0)) >= 0
 		    && INTVAL (XEXP (op, 0)) <= 0x3ffff")))
 
+
+;; Floating-point constant constraints.
 
+(define_constraint "v"
+  "Floating point power of 2 with exponent in [0..127]"
+  (and (match_code "const_double,const_vector")
+       (match_test "exp2_immediate_p (op, VOIDmode, 0, 127)")))
+
+(define_constraint "w"
+  "Floating point power of 2 with exponent in [-126..0]"
+  (and (match_code "const_double,const_vector")
+       (match_test "exp2_immediate_p (op, VOIDmode, -126, 0)")))
Index: gcc/config/spu/spu.md
===================================================================
--- gcc/config/spu/spu.md	(.../gcc-4_3-branch)	(revision 156795)
+++ gcc/config/spu/spu.md	(.../cell-4_3-branch)	(revision 156810)
@@ -136,10 +136,6 @@
  (UNSPEC_HEQ            31)
  (UNSPEC_HGT            32)
  (UNSPEC_HLGT           33)
- (UNSPEC_CSFLT          34)
- (UNSPEC_CFLTS          35)
- (UNSPEC_CUFLT          36)
- (UNSPEC_CFLTU          37)
  (UNSPEC_STOP           38)
  (UNSPEC_STOPD          39)
  (UNSPEC_SET_INTR       40)
@@ -226,6 +222,10 @@
                        (DF "di") (V2DF "v2di")])
 (define_mode_attr F2I [(SF "SI") (V4SF "V4SI")
                        (DF "DI") (V2DF "V2DI")])
+(define_mode_attr i2f [(SI "sf") (V4SI "v4sf")
+                       (DI "df") (V2DI "v2df")])
+(define_mode_attr I2F [(SI "SF") (V4SI "V4SF")
+                       (DI "DF") (V2DI "V2DF")])
 
 (define_mode_attr DF2I [(DF "SI") (V2DF "V2DI")])
 
@@ -273,8 +273,7 @@
 (define_split 
   [(set (match_operand 0 "spu_reg_operand")
 	(match_operand 1 "immediate_operand"))]
-
-  ""
+  "split0_completed"
   [(set (match_dup 0)
 	(high (match_dup 1)))
    (set (match_dup 0)
@@ -311,9 +310,10 @@
 ;; move internal
 
 (define_insn "_mov<mode>"
-  [(set (match_operand:MOV 0 "spu_nonimm_operand" "=r,r,r,r,r,m")
+  [(set (match_operand:MOV 0 "spu_dest_operand" "=r,r,r,r,r,m")
 	(match_operand:MOV 1 "spu_mov_operand" "r,A,f,j,m,r"))]
-  "spu_valid_move (operands)"
+  "register_operand(operands[0], <MODE>mode)
+   || register_operand(operands[1], <MODE>mode)"
   "@
    ori\t%0,%1,0
    il%s1\t%0,%S1
@@ -331,9 +331,10 @@
   "iohl\t%0,%2@l")
 
 (define_insn "_movdi"
-  [(set (match_operand:DI 0 "spu_nonimm_operand" "=r,r,r,r,r,m")
+  [(set (match_operand:DI 0 "spu_dest_operand" "=r,r,r,r,r,m")
 	(match_operand:DI 1 "spu_mov_operand" "r,a,f,k,m,r"))]
-  "spu_valid_move (operands)"
+  "register_operand(operands[0], DImode)
+   || register_operand(operands[1], DImode)"
   "@
    ori\t%0,%1,0
    il%d1\t%0,%D1
@@ -344,9 +345,10 @@
   [(set_attr "type" "fx2,fx2,shuf,shuf,load,store")])
 
 (define_insn "_movti"
-  [(set (match_operand:TI 0 "spu_nonimm_operand" "=r,r,r,r,r,m")
+  [(set (match_operand:TI 0 "spu_dest_operand" "=r,r,r,r,r,m")
 	(match_operand:TI 1 "spu_mov_operand" "r,U,f,l,m,r"))]
-  "spu_valid_move (operands)"
+  "register_operand(operands[0], TImode)
+   || register_operand(operands[1], TImode)"
   "@
    ori\t%0,%1,0
    il%t1\t%0,%T1
@@ -356,29 +358,25 @@
    stq%p0\t%1,%0"
   [(set_attr "type" "fx2,fx2,shuf,shuf,load,store")])
 
-(define_insn_and_split "load"
-  [(set (match_operand 0 "spu_reg_operand" "=r")
-	(match_operand 1 "memory_operand" "m"))
-   (clobber (match_operand:TI 2 "spu_reg_operand" "=&r"))
-   (clobber (match_operand:SI 3 "spu_reg_operand" "=&r"))]
-  "GET_MODE(operands[0]) == GET_MODE(operands[1])"
-  "#"
-  ""
+(define_split
+  [(set (match_operand 0 "spu_reg_operand")
+	(match_operand 1 "memory_operand"))]
+  "GET_MODE(operands[0]) == GET_MODE(operands[1]) && !split0_completed"
   [(set (match_dup 0)
 	(match_dup 1))]
-  { spu_split_load(operands); DONE; })
+  { if (spu_split_load(operands))
+      DONE;
+  })
 
-(define_insn_and_split "store"
-  [(set (match_operand 0 "memory_operand" "=m")
-	(match_operand 1 "spu_reg_operand" "r"))
-   (clobber (match_operand:TI 2 "spu_reg_operand" "=&r"))
-   (clobber (match_operand:TI 3 "spu_reg_operand" "=&r"))]
-  "GET_MODE(operands[0]) == GET_MODE(operands[1])"
-  "#"
-  ""
+(define_split
+  [(set (match_operand 0 "memory_operand")
+	(match_operand 1 "spu_reg_operand"))]
+  "GET_MODE(operands[0]) == GET_MODE(operands[1]) && !split0_completed"
   [(set (match_dup 0)
 	(match_dup 1))]
-  { spu_split_store(operands); DONE; })
+  { if (spu_split_store(operands))
+      DONE;
+  })
 
 ;; Operand 3 is the number of bytes. 1:b 2:h 4:w 8:d
 
@@ -589,62 +587,83 @@
 
 ;; float conversions
 
-(define_insn "floatsisf2"
-  [(set (match_operand:SF 0 "spu_reg_operand" "=r")
-	(float:SF (match_operand:SI 1 "spu_reg_operand" "r")))]
+(define_insn "float<mode><i2f>2"
+  [(set (match_operand:<I2F> 0 "spu_reg_operand" "=r")
+	(float:<I2F> (match_operand:VSI 1 "spu_reg_operand" "r")))]
   ""
   "csflt\t%0,%1,0"
   [(set_attr "type" "fp7")])
 
-(define_insn "floatv4siv4sf2"
-  [(set (match_operand:V4SF 0 "spu_reg_operand" "=r")
-	(float:V4SF (match_operand:V4SI 1 "spu_reg_operand" "r")))]
+(define_insn "fix_trunc<mode><f2i>2"
+  [(set (match_operand:<F2I> 0 "spu_reg_operand" "=r")
+	(fix:<F2I> (match_operand:VSF 1 "spu_reg_operand" "r")))]
   ""
-  "csflt\t%0,%1,0"
+  "cflts\t%0,%1,0"
   [(set_attr "type" "fp7")])
 
-(define_insn "fix_truncsfsi2"
-  [(set (match_operand:SI 0 "spu_reg_operand" "=r")
-	(fix:SI (match_operand:SF 1 "spu_reg_operand" "r")))]
+(define_insn "floatuns<mode><i2f>2"
+  [(set (match_operand:<I2F> 0 "spu_reg_operand" "=r")
+	(unsigned_float:<I2F> (match_operand:VSI 1 "spu_reg_operand" "r")))]
   ""
-  "cflts\t%0,%1,0"
+  "cuflt\t%0,%1,0"
   [(set_attr "type" "fp7")])
 
-(define_insn "fix_truncv4sfv4si2"
-  [(set (match_operand:V4SI 0 "spu_reg_operand" "=r")
-	(fix:V4SI (match_operand:V4SF 1 "spu_reg_operand" "r")))]
+(define_insn "fixuns_trunc<mode><f2i>2"
+  [(set (match_operand:<F2I> 0 "spu_reg_operand" "=r")
+	(unsigned_fix:<F2I> (match_operand:VSF 1 "spu_reg_operand" "r")))]
   ""
-  "cflts\t%0,%1,0"
+  "cfltu\t%0,%1,0"
   [(set_attr "type" "fp7")])
 
-(define_insn "floatunssisf2"
-  [(set (match_operand:SF 0 "spu_reg_operand" "=r")
-	(unsigned_float:SF (match_operand:SI 1 "spu_reg_operand" "r")))]
+(define_insn "float<mode><i2f>2_mul"
+  [(set (match_operand:<I2F> 0 "spu_reg_operand" "=r")
+	(mult:<I2F> (float:<I2F> (match_operand:VSI 1 "spu_reg_operand" "r"))
+		    (match_operand:<I2F> 2 "spu_inv_exp2_operand" "w")))]
   ""
-  "cuflt\t%0,%1,0"
+  "csflt\t%0,%1,%w2"
   [(set_attr "type" "fp7")])
 
-(define_insn "floatunsv4siv4sf2"
-  [(set (match_operand:V4SF 0 "spu_reg_operand" "=r")
-	(unsigned_float:V4SF (match_operand:V4SI 1 "spu_reg_operand" "r")))]
+(define_insn "float<mode><i2f>2_div"
+  [(set (match_operand:<I2F> 0 "spu_reg_operand" "=r")
+	(div:<I2F> (float:<I2F> (match_operand:VSI 1 "spu_reg_operand" "r"))
+		   (match_operand:<I2F> 2 "spu_exp2_operand" "v")))]
   ""
-  "cuflt\t%0,%1,0"
+  "csflt\t%0,%1,%v2"
   [(set_attr "type" "fp7")])
 
-(define_insn "fixuns_truncsfsi2"
-  [(set (match_operand:SI 0 "spu_reg_operand" "=r")
-	(unsigned_fix:SI (match_operand:SF 1 "spu_reg_operand" "r")))]
+
+(define_insn "fix_trunc<mode><f2i>2_mul"
+  [(set (match_operand:<F2I> 0 "spu_reg_operand" "=r")
+	(fix:<F2I> (mult:VSF (match_operand:VSF 1 "spu_reg_operand" "r")
+			     (match_operand:VSF 2 "spu_exp2_operand" "v"))))]
   ""
-  "cfltu\t%0,%1,0"
+  "cflts\t%0,%1,%v2"
   [(set_attr "type" "fp7")])
 
-(define_insn "fixuns_truncv4sfv4si2"
-  [(set (match_operand:V4SI 0 "spu_reg_operand" "=r")
-	(unsigned_fix:V4SI (match_operand:V4SF 1 "spu_reg_operand" "r")))]
+(define_insn "floatuns<mode><i2f>2_mul"
+  [(set (match_operand:<I2F> 0 "spu_reg_operand" "=r")
+	(mult:<I2F> (unsigned_float:<I2F> (match_operand:VSI 1 "spu_reg_operand" "r"))
+		    (match_operand:<I2F> 2 "spu_inv_exp2_operand" "w")))]
   ""
-  "cfltu\t%0,%1,0"
+  "cuflt\t%0,%1,%w2"
   [(set_attr "type" "fp7")])
 
+(define_insn "floatuns<mode><i2f>2_div"
+  [(set (match_operand:<I2F> 0 "spu_reg_operand" "=r")
+	(div:<I2F> (unsigned_float:<I2F> (match_operand:VSI 1 "spu_reg_operand" "r"))
+		   (match_operand:<I2F> 2 "spu_exp2_operand" "v")))]
+  ""
+  "cuflt\t%0,%1,%v2"
+  [(set_attr "type" "fp7")])
+
+(define_insn "fixuns_trunc<mode><f2i>2_mul"
+  [(set (match_operand:<F2I> 0 "spu_reg_operand" "=r")
+	(unsigned_fix:<F2I> (mult:VSF (match_operand:VSF 1 "spu_reg_operand" "r")
+				      (match_operand:VSF 2 "spu_exp2_operand" "v"))))]
+  ""
+  "cfltu\t%0,%1,%v2"
+  [(set_attr "type" "fp7")])
+
 (define_insn "extendsfdf2"
   [(set (match_operand:DF 0 "spu_reg_operand" "=r")
 	(unspec:DF [(match_operand:SF 1 "spu_reg_operand" "r")]
@@ -1248,7 +1267,7 @@
    (use (match_operand:<F2I> 2 "spu_reg_operand" "r"))]
   ""
   "#"
-  ""
+  "split0_completed"
   [(set (match_dup:<F2I> 3)
 	(and:<F2I> (match_dup:<F2I> 4)
 		   (match_dup:<F2I> 2)))]
@@ -3621,6 +3640,41 @@
   "bi%b1%b0z\t%1,$lr"
   [(set_attr "type" "br")])
 
+;; The following 2 variations are used when the using __builtin_expect
+;; with a non-constant second argument.  They include a reference to a
+;; deleted label which will be placed immediately after the branch.
+(define_insn "expect_then"
+  [(set (pc)
+	(if_then_else (match_operator 1 "branch_comparison_operator"
+				      [(match_operand 2
+						      "spu_reg_operand" "r")
+				       (const_int 0)])
+		      (label_ref (match_operand 0 "" ""))
+		      (pc)))
+   (use (match_operand 3 "" ""))
+   (use (match_operand 4 "" ""))
+   (use (match_operand 5 "" ""))
+   (clobber (reg:SI 130))]
+  ""
+  "br%b2%b1z\t%2,%0"
+  [(set_attr "type" "br")])
+
+(define_insn "expect_else"
+  [(set (pc)
+	(if_then_else (match_operator 1 "branch_comparison_operator"
+				      [(match_operand 2
+						      "spu_reg_operand" "r")
+				       (const_int 0)])
+		      (pc)
+		      (label_ref (match_operand 0 "" ""))))
+   (use (match_operand 3 "" ""))
+   (use (match_operand 4 "" ""))
+   (use (match_operand 5 "" ""))
+   (clobber (reg:SI 130))]
+  ""
+  "br%b2%b1z\t%2,%0"
+  [(set_attr "type" "br")])
+
 
 ;; Compare insns are next.  Note that the spu has two types of compares,
 ;; signed & unsigned, and one type of branch.
@@ -4592,3 +4646,57 @@
 
   DONE;
 }")
+
+(define_expand "to_ea"
+  [(use (match_operand 0 "" ""))
+   (use (match_operand 1 "" ""))]
+  ""
+{
+  rtx ls_mem, op0, op1;
+  enum machine_mode mode = (spu_ea_model == 32) ? Pmode : DImode;
+
+  ls_mem = gen_rtx_MEM (DImode, gen_rtx_SYMBOL_REF (Pmode, "__ea_local_store"));
+
+  op0 = force_reg (mode, operands[0]);
+  op1 = force_reg (Pmode, operands[1]);
+
+  if (mode == Pmode)
+    emit_insn (gen_addsi3 (op0, op1, force_reg (mode, gen_lowpart (mode, ls_mem))));
+  else
+    {
+      rtx tmp = gen_reg_rtx (DImode);
+      emit_move_insn (tmp, gen_rtx_ZERO_EXTEND (DImode, op1));
+      emit_insn (gen_adddi3 (op0, tmp, force_reg (mode, ls_mem)));
+    }
+  DONE;
+})
+
+(define_expand "from_ea"
+  [(use (match_operand 0 "" ""))
+   (use (match_operand 1 "" ""))]
+  ""
+{
+  rtx ls_mem, ls, op0, op1, tmp;
+  enum machine_mode mode = (spu_ea_model == 32) ? Pmode : DImode;
+
+  ls_mem = gen_rtx_MEM (DImode, gen_rtx_SYMBOL_REF (Pmode, "__ea_local_store"));
+  ls = force_reg (Pmode, gen_lowpart (Pmode, ls_mem));
+
+  op0 = force_reg (Pmode, operands[0]);
+  op1 = force_reg (mode, operands[1]);
+  tmp = (mode == Pmode) ? op1 : force_reg (Pmode, gen_lowpart (Pmode, op1));
+
+  emit_insn (gen_subsi3 (op0, tmp, ls));
+  DONE;
+})
+
+;; Save the operands for use by spu_emit_branch_or_set
+(define_expand "builtin_expect"
+  [(use (match_operand:SI 0 "spu_reg_operand" "r"))
+   (use (match_operand:SI 1 "spu_reg_operand" "r"))]
+  ""
+  {
+    spu_expect_op0 = operands[0];
+    spu_expect_op1 = operands[1];
+    DONE;
+  })
Index: gcc/config/rs6000/rs6000-c.c
===================================================================
--- gcc/config/rs6000/rs6000-c.c	(.../gcc-4_3-branch)	(revision 156795)
+++ gcc/config/rs6000/rs6000-c.c	(.../cell-4_3-branch)	(revision 156810)
@@ -84,6 +84,149 @@
 #define builtin_define(TXT) cpp_define (pfile, TXT)
 #define builtin_assert(TXT) cpp_assert (pfile, TXT)
 
+/* Keep the AltiVec keywords handy for fast comparisons.  */
+static tree __vector_keyword;
+static tree vector_keyword;
+static tree __pixel_keyword;
+static tree pixel_keyword;
+static tree __bool_keyword;
+static tree bool_keyword;
+
+/* Preserved across calls.  */
+static tree expand_bool_pixel;
+
+static cpp_hashnode *
+altivec_categorize_keyword (const cpp_token *tok)
+{
+  if (tok->type == CPP_NAME)
+    {
+      cpp_hashnode *ident = tok->val.node;
+
+      if (ident == C_CPP_HASHNODE (vector_keyword)
+	  || ident == C_CPP_HASHNODE (__vector_keyword))
+	return C_CPP_HASHNODE (__vector_keyword);
+
+      if (ident == C_CPP_HASHNODE (pixel_keyword)
+	  || ident ==  C_CPP_HASHNODE (__pixel_keyword))
+	return C_CPP_HASHNODE (__pixel_keyword);
+
+      if (ident == C_CPP_HASHNODE (bool_keyword)
+	  || ident == C_CPP_HASHNODE (__bool_keyword))
+	return C_CPP_HASHNODE (__bool_keyword);
+
+      return ident;
+    }
+
+  return 0;
+}
+
+static void
+init_vector_keywords (void)
+{
+  /* Keywords without two leading underscores are context-sensitive,
+     and hence implemented as conditional macros, controlled by the
+     rs6000_macro_to_expand() function below.  */
+
+  __vector_keyword = get_identifier ("__vector");
+  C_CPP_HASHNODE (__vector_keyword)->flags |= NODE_CONDITIONAL;
+
+  __pixel_keyword = get_identifier ("__pixel");
+  C_CPP_HASHNODE (__pixel_keyword)->flags |= NODE_CONDITIONAL;
+
+  __bool_keyword = get_identifier ("__bool");
+  C_CPP_HASHNODE (__bool_keyword)->flags |= NODE_CONDITIONAL;
+
+  vector_keyword = get_identifier ("vector");
+  C_CPP_HASHNODE (vector_keyword)->flags |= NODE_CONDITIONAL;
+
+  pixel_keyword = get_identifier ("pixel");
+  C_CPP_HASHNODE (pixel_keyword)->flags |= NODE_CONDITIONAL;
+
+  bool_keyword = get_identifier ("bool");
+  C_CPP_HASHNODE (bool_keyword)->flags |= NODE_CONDITIONAL;
+}
+
+/* Called to decide whether a conditional macro should be expanded.
+   Since we have exactly one such macro (i.e, 'vector'), we do not
+   need to examine the 'tok' parameter.  */
+
+static cpp_hashnode *
+rs6000_macro_to_expand (cpp_reader *pfile, const cpp_token *tok)
+{
+  cpp_hashnode *expand_this = tok->val.node;
+  cpp_hashnode *ident;
+
+  ident = altivec_categorize_keyword (tok);
+
+  if (ident == C_CPP_HASHNODE (__vector_keyword))
+    {
+      tok = cpp_peek_token (pfile, 0);
+      ident = altivec_categorize_keyword (tok);
+
+      if (ident ==  C_CPP_HASHNODE (__pixel_keyword))
+	{
+	  expand_this = C_CPP_HASHNODE (__vector_keyword);
+	  expand_bool_pixel = __pixel_keyword;
+	}
+      else if (ident == C_CPP_HASHNODE (__bool_keyword))
+	{
+	  expand_this = C_CPP_HASHNODE (__vector_keyword);
+	  expand_bool_pixel = __bool_keyword;
+	}
+      else if (ident)
+	{
+	  enum rid rid_code = (enum rid)(ident->rid_code);
+	  if (ident->type == NT_MACRO)
+	    {
+	      (void)cpp_get_token (pfile);
+	      tok = cpp_peek_token (pfile, 0);
+	      ident = altivec_categorize_keyword (tok);
+	      if (ident)
+		rid_code = (enum rid)(ident->rid_code);
+	    }
+
+	  if (rid_code == RID_UNSIGNED || rid_code == RID_LONG
+	      || rid_code == RID_SHORT || rid_code == RID_SIGNED
+	      || rid_code == RID_INT || rid_code == RID_CHAR
+	      || rid_code == RID_FLOAT)
+	    {
+	      expand_this = C_CPP_HASHNODE (__vector_keyword);
+	      /* If the next keyword is bool or pixel, it
+		 will need to be expanded as well.  */
+	      tok = cpp_peek_token (pfile, 1);
+	      ident = altivec_categorize_keyword (tok);
+
+	      if (ident ==  C_CPP_HASHNODE (__pixel_keyword))
+		expand_bool_pixel = __pixel_keyword;
+	      else if (ident == C_CPP_HASHNODE (__bool_keyword))
+		expand_bool_pixel = __bool_keyword;
+	      else
+		{
+		  /* Try two tokens down, too.  */
+		  tok = cpp_peek_token (pfile, 2);
+		  ident = altivec_categorize_keyword (tok);
+		  if (ident ==  C_CPP_HASHNODE (__pixel_keyword))
+		    expand_bool_pixel = __pixel_keyword;
+		  else if (ident == C_CPP_HASHNODE (__bool_keyword))
+		    expand_bool_pixel = __bool_keyword;
+		}
+	    }
+	}
+    }
+  else if (expand_bool_pixel && ident == C_CPP_HASHNODE (__pixel_keyword))
+    {
+      expand_this = C_CPP_HASHNODE (__pixel_keyword);
+      expand_bool_pixel = 0;
+    }
+  else if (expand_bool_pixel && ident == C_CPP_HASHNODE (__bool_keyword))
+    {
+      expand_this = C_CPP_HASHNODE (__bool_keyword);
+      expand_bool_pixel = 0;
+    }
+
+  return expand_this;
+}
+
 void
 rs6000_cpu_cpp_builtins (cpp_reader *pfile)
 {
@@ -120,6 +263,20 @@
       builtin_define ("__vector=__attribute__((altivec(vector__)))");
       builtin_define ("__pixel=__attribute__((altivec(pixel__))) unsigned short");
       builtin_define ("__bool=__attribute__((altivec(bool__))) unsigned");
+
+      if (!flag_iso)
+	{
+	  /* Define this when supporting context-sensitive keywords.  */
+	  builtin_define ("__APPLE_ALTIVEC__");
+	  
+	  builtin_define ("vector=vector");
+	  builtin_define ("pixel=pixel");
+	  builtin_define ("bool=bool");
+	  init_vector_keywords ();
+
+	  /* Enable context-sensitive macros.  */
+	  cpp_get_callbacks (pfile)->macro_to_expand = rs6000_macro_to_expand;
+	}
     }
   if (rs6000_cpu == PROCESSOR_CELL)
     builtin_define ("__PPU__");
Index: gcc/convert.c
===================================================================
--- gcc/convert.c	(.../gcc-4_3-branch)	(revision 156795)
+++ gcc/convert.c	(.../cell-4_3-branch)	(revision 156810)
@@ -34,6 +34,7 @@
 #include "langhooks.h"
 #include "real.h"
 #include "fixed-value.h"
+#include "target.h"
 
 /* Convert EXPR to some pointer or reference type TYPE.
    EXPR must be pointer, reference, integer, enumeral, or literal zero;
@@ -58,13 +59,18 @@
     case INTEGER_TYPE:
     case ENUMERAL_TYPE:
     case BOOLEAN_TYPE:
-      if (TYPE_PRECISION (TREE_TYPE (expr)) != POINTER_SIZE)
-	expr = fold_build1 (NOP_EXPR,
-                            lang_hooks.types.type_for_size (POINTER_SIZE, 0),
-			    expr);
-      return fold_build1 (CONVERT_EXPR, type, expr);
+    {
+      int pointer_size =
+	TYPE_ADDR_SPACE (TREE_TYPE (type))
+	? GET_MODE_BITSIZE (targetm.addr_space_pointer_mode (TYPE_ADDR_SPACE (TREE_TYPE (type))))
+	: POINTER_SIZE;
 
+      if (TYPE_PRECISION (TREE_TYPE (expr)) != pointer_size)
+	expr = fold_build1 (NOP_EXPR, lang_hooks.types.type_for_size (pointer_size, 0), expr);
+    }
+    return fold_build1 (CONVERT_EXPR, type, expr);
 
+
     default:
       error ("cannot convert to a pointer type");
       return convert_to_pointer (type, integer_zero_node);
@@ -478,16 +484,26 @@
     {
     case POINTER_TYPE:
     case REFERENCE_TYPE:
-      if (integer_zerop (expr))
-	return build_int_cst (type, 0);
+      {
+ 	int pointer_size;
 
-      /* Convert to an unsigned integer of the correct width first,
-	 and from there widen/truncate to the required type.  */
-      expr = fold_build1 (CONVERT_EXPR,
-			  lang_hooks.types.type_for_size (POINTER_SIZE, 0),
-			  expr);
-      return fold_convert (type, expr);
+ 	if (integer_zerop (expr))
+ 	  return build_int_cst (type, 0);
 
+ 	/* Convert to an unsigned integer of the correct width first,
+ 	   and from there widen/truncate to the required type.  */
+ 	pointer_size =
+ 	  TYPE_ADDR_SPACE (TREE_TYPE (intype))
+	  ? GET_MODE_BITSIZE (targetm.addr_space_pointer_mode
+			      (TYPE_ADDR_SPACE (strip_array_types (TREE_TYPE (intype)))))
+ 	  : POINTER_SIZE;
+
+ 	expr = fold_build1 (CONVERT_EXPR,
+ 			    lang_hooks.types.type_for_size (pointer_size, 0),
+ 			    expr);
+ 	return fold_build1 (NOP_EXPR, type, expr);
+      }
+
     case INTEGER_TYPE:
     case ENUMERAL_TYPE:
     case BOOLEAN_TYPE:
Index: ChangeLog.cell
===================================================================
--- ChangeLog.cell	(.../gcc-4_3-branch)	(revision 0)
+++ ChangeLog.cell	(.../cell-4_3-branch)	(revision 156810)
@@ -0,0 +1,411 @@
+2008-12-05  Ulrich Weigand  <uweigand@de.ibm.com>
+
+	* gcc/config/spu/spu.h (ADDRESSES_NEVER_TRAP): Define.
+	* gcc/rtlanal.c (rtx_addr_can_trap_p_1): Respect ADDRESSES_NEVER_TRAP.
+	* gcc/doc/tm.texi (ADDRESSES_NEVER_TRAP): Document.
+
+	* gcc/config/spu/spu.c (spu_split_load): Trust MEM_ALIGN.  When not
+	optimizing, do not split load unless necessary.
+
+	* gcc/config/spu/spu.md ("_abs<mode>2"): Do not split in split0 pass.
+
+2008-11-18  Trevor Smigiel <Trevor_Smigiel@playstation.sony.com>
+
+	Change constraint u to w to avoid conflict with a patch I have
+	not contributed yet.
+
+	* gcc/config/spu/spu-builtins.md (spu_csflt, spu_cuflt,
+	spu_cflts, spu_cfltu): Change unspec code to 0 to avoid
+	warnings.
+	* gcc/config/spu/spu.c (print_operand):  Change code u to w.
+	* gcc/config/spu/constraints.md (u): Change constraint u to w.
+	* gcc/config/spu/spu.md (float<mode><i2f>2_mul,
+	fix_trunc<mode><f2i>2_mul, floatuns<mode><i2f>2_mul,
+	fixuns_trunc<mode><f2i>2_mul): Change u to w.
+
+2008-11-18  Trevor Smigiel <Trevor_Smigiel@playstation.sony.com>
+
+	Allow non-literal arguments for spu_convtf, spu_convtu and
+	spu_convts intrinsics.
+
+	* gcc/config/spu/spu-protos.h (exp2_immediate_p, spu_gen_exp2):
+	Declare. 
+	* gcc/config/spu/predicates.md (spu_inv_exp2_operand,
+	spu_exp2_operand): Define.
+	* gcc/config/spu/spu-builtins.def (SPU_CONVTS, SPU_CONVTU,
+	SPU_CONVTF_0, SPU_CONVTF_1):  Change argument type.
+	* gcc/config/spu/spu-builtins.md (spu_csflt, spu_cuflt,
+	spu_cflts, spu_cfltu): Handle non-immediate in operand 2.
+	* gcc/config/spu/spu.c (print_operand):  New codes u and v.
+	(exp2_immediate_p, spu_gen_exp2): Define. 
+	* gcc/config/spu/spu.md (i2f, I2F):  New mode_attr.
+	(floatsisf2, floatv4sisf2, fix_truncsisf2, fix_truncv4sisf2,
+	floatunssisf2, floatunsv4sisf2, fixuns_truncsisf2,
+	fixuns_truncv4sisf2): Redefine using mode macros.
+	(float<mode><i2f>2_mul, float<mode><i2f>2_div,
+	fix_trunc<mode><f2i>2_mul, floatuns<mode><i2f>2_mul,
+	floatuns<mode><i2f>2_div, fixuns_trunc<mode><f2i>2_mul): Define
+	* gcc/config/spu/constraints.md (u, v): New constraints.
+	
+2008-11-17  Trevor Smigiel <Trevor_Smigiel@playstation.sony.com>
+
+	Add aggressive conditional move optimizations.
+
+	* gcc/ifcvt.c (aggressive_cmov, block_modifies_live_reg,
+	find_andif_orif_block): Declare. 
+	(safe_to_transform_p, cost_effective_p, validate_remap_regs_1,
+	validate_remap_regs, remap_instructions, hoist_instructions,
+	mark_regs_in_rtx, copy_regs_in_rtx, noce_try_cmove_aggressive,
+	noce_process_andif_orif_block, find_andif_orif_block,
+	block_modifies_live_reg): Define.
+	(noce_find_if_block): Call noce_try_cmove_aggressive.
+	(find_if_header): Call find_andif_orif_block.
+	(if_convert): Proccess blocks in post order.
+	* gcc/common.opt (-faggressive-cmov): New option.
+
+2008-09-22  Trevor Smigiel <Trevor_Smigiel@playstation.sony.com>
+
+	Use a better method to adjust hint labels on branches.
+
+	* gcc/config/spu/spu.c (spu_emit_branch_or_set,
+	spu_emit_branch_hint, spu_machine_dependent_reorg): Emit hint
+	labels before branch and adjust offset after adding nops.
+
+2008-09-10  Trevor Smigiel <Trevor_Smigiel@playstation.sony.com>
+
+	Support dynamic hints via __builtin_expect with a non-constant
+	second argument.
+
+	* gcc/doc/md.texi (builtin_expect): Document new pattern.
+	* gcc/builtins.c (expand_builtin_expect): Handle target defined
+	builtin_expect.
+	* gcc/predict.c (strip_builtin_expect): Don't strip all the time
+	when the target provides builtin_expect.
+	* gcc/config/spu/spu.c (spu_expect_op0, spu_expect_op1): Define.
+	(spu_emit_branch_or_set): Handle spu_expect_op0 and spu_expect_op1.
+	(spu_emit_branch_hint): Handle dynamic hints.
+	(get_branch_target): Handle expect_then and expect_else rtl.
+	(insn_clobbers_hbr): Return true for hints.
+	(spu_machine_dependent_reorg): Reposition lables for hints.
+	* gcc/config/spu/spu.h (spu_expect_op0, spu_expect_op1): Declare.
+	* gcc/config/spu/spu.md (expect_then, expect_else, builtin_expect):
+	Declare.
+
+2008-08-27  Trevor Smigiel <Trevor_Smigiel@playstation.sony.com>
+
+	Fix a bug caused by the previous change.
+
+	* gcc/config/spu/spu.c (spu_init_expanders) : Pregenerate
+	a couple of pseudo-registers.
+
+2008-08-27  Trevor Smigiel <Trevor_Smigiel@playstation.sony.com>
+	
+	Improve code generated for loads and stores on SPU.
+
+	* gcc/doc/tm.texi (SPLIT_BEFORE_CSE2) : Document.
+	* gcc/tree-pass.h (pass_split_before_cse2) : Declare.
+	* gcc/final.c (rest_of_clean_state) : Initialize split0_completed.
+	* gcc/testsuite/gcc.target/spu/split0-1.c : Add test.
+	* gcc/recog.c (split0_completed) : Define.
+	(gate_handle_split_before_cse2, rest_of_handle_split_before_cse2) :
+	New functions.
+	(pass_split_before_cse2) : New pass.
+	* gcc/rtl.h (split0_completed) : Declare.
+	* gcc/passes.c (init_optimization_passes) : Add
+	pass_split_before_cse2 before pass_cse2 .
+	* gcc/config/spu/spu-protos.h (spu_legitimate_address) : Add
+	for_split argument.
+	(aligned_mem_p, spu_valid_move) : Remove prototypes.
+	(spu_split_load, spu_split_store) : Change return type to int.
+	* gcc/config/spu/predicates.md (spu_mem_operand) : Remove.
+	(spu_dest_operand) : Add.
+	* gcc/config/spu/spu-builtins.md (spu_lqd, spu_lqx, spu_lqa,
+	spu_lqr, spu_stqd, spu_stqx, spu_stqa, spu_stqr) : Remove AND
+	operation.
+	* gcc/config/spu/spu.c (regno_aligned_for_load) : Remove.
+	(reg_aligned_for_addr, address_needs_split) : New functions.
+	(spu_legitimate_address, spu_expand_mov, spu_split_load,
+	spu_split_store) : Update.
+	* gcc/config/spu/spu.h (REG_ALIGN, SPLIT_BEFORE_CSE2) : Define.
+	(GO_IF_LEGITIMATE_ADDRESS) : Update for spu_legitimate_address.
+	* gcc/config/spu/spu.md ("_mov<mode>", "_movdi", "_movti") : Update
+	predicates.
+	("load", "store") : Change to define_split.
+
+2008-08-27  Ben Elliston  <bje@au.ibm.com>
+
+	* gcc/tree.h (check_qualified_type): Use CONST_CAST_TREE as
+	needed.
+	* gcc/langhooks.c (lhd_tree_dump_type_quals): Likewise.
+	* gcc/c-typeck.c (comptypes_internal): Likewise.
+	* gcc/cp/typeck.c (cp_type_quals): Likewise.
+
+2008-08-26  Ben Elliston  <bje@au.ibm.com>
+
+	* gcc/config/spu/spu.c (spu_valid_addr_space): Take a
+	const_tree argument.
+
+2008-08-26  Ben Elliston  <bje@au.ibm.com>
+	    Alan Modra  <amodra@au.ibm.com>
+
+	* gcc/c-decl.c (start_decl): Set DECL_SECTION_NAME to "._ea" for
+	__ea variable definitions.
+	(grokdeclarator): Allow static and global __ea variable
+	definitions.
+	* gcc/config/spu/spu.c (TARGET_SECTION_TYPE_FLAGS): Define.
+	(spu_section_type_flags): New function.
+	(ea_symbol_ref): Convert to for_each_rtx predicate.
+	(spu_legitimate_constant_p): Reject __ea symbol refs wrapped in
+	const.
+	(spu_legitimate_address): Likewise.
+	* gcc/config/spu/spu.h (TEXT_SECTION_ASM_OP): Add a tab.
+	(DATA_SECTION_ASM_OP): Likewise.
+	* gcc/config/spu/spu.opt: Add -mall-ea.
+	* gcc/c-typeck.c (build_array_ref): Do not strip qualifiers from
+	the array element type.  (PR 33726)
+
+	* gcc/c-common.h (strip_array_types): Move declaration to..
+	* gcc/tree.h: ..here.
+	(OTHER_ADDR_SPACE_POINTER_TYPE_P,
+	GENERIC_ADDR_SPACE_POINTER_TYPE_P, TYPE_QUALS): Invoke
+	strip_array_types on TYPE_ADDR_SPACE argument.
+	* gcc/c-objc-common.c (c_types_compatible_p): Likewise.
+	* gcc/config/spu/spu.h (ASM_OUTPUT_SYMBOL_REF): Likewise.
+	* gcc/config/spu/spu.c (ea_symbol_ref): Likewise.
+	* gcc/convert.c (convert_to_integer): Likewise.
+	* gcc/c-parser.c (c_parser_postfix_expression_after_paren_type): Ditto.
+	* gcc/dwarf2out.c (modified_type_die): Likewise.
+	* gcc/emit-rtl.c (set_mem_attributes_minus_bitpos): Likewise.
+	* gcc/varasm (make_decl_rtl): Likewise.
+	* gcc/tree.c (build_pointer_type): Likewise.
+	(strip_array_types): Move from here ..
+	* gcc/c-common.c: .. to here.
+	* gcc/testsuite/gcc.target/spu/ea/compile.c: Update test.
+	* gcc/testsuite/gcc.target/spu/ea/errors.c: Likewise.
+	* gcc/testsuite/gcc.target/spu/cache.c: Likewise.
+
+2008-08-25  Ulrich Weigand  <uweigand@de.ibm.com>
+
+	* gcc/config/spu/cachemgr.c: Make interrupt safe and respect tag
+	mask policy.
+
+2008-08-20  Ben Elliston  <bje@au.ibm.com>
+
+	Backport from FSF mainline:
+	2008-08-20  Ben Elliston  <bje@au.ibm.com>
+
+	* c-decl.c: Include targhooks.h.
+	(shadow_tag_warned): Include declspecs->address_space.
+	(quals_from_declspecs): Encode address space number into quals.
+	(grokdeclarator): Warn about duplicate address space qualifiers.
+	Issue various diagnostics as specified by N1169.
+	(build_null_declspecs): Clear ret->address_space.
+	(declspecs_add_addrspace): New function.
+	* c-objc-common.c (c_types_compatible_p): Two types in different
+	address spaces are not compatible.
+	* c-parser.c (enum c_id_kind): Add C_ID_ADDRSPACE.
+	(c_lex_one_token): Set token->id_kind to C_ID_ADDRSPACE if the
+	token is a recognised address space.
+	(c_token_starts_typename): Return true for C_ID_ADDRSPACE.
+	(c_token_starts_declspecs): Likewise.
+	(c_parser_declspecs): Handle C_ID_ADDRSPACE.
+	(c_parser_postfix_expression_after_paren_type): Reject compound
+	literals qualified by an address space qualifier.	
+	* c-pretty-print.c: Include target.h and target-def.h.
+	(pp_c_type_qualifier_list): Print address space if non-zero.
+	* c-tree.h (struct c_declspecs): Add address_space field.
+	* c-typeck.c (build_binary_op): If an operand is a pointer into
+	another address space, make the result of the comparison such a
+	pointer also.
+	* convert.c: Include target.h.
+	(convert_to_pointer): Use targetm.addr_space_pointer_mode to
+	calculate the width of a pointer.
+	(convert_to_integer): Likewise.
+	* dwarf2out.c (modified_type_die): Set the DW_AT_address_class
+	attribute to the address space number for pointer and reference
+	types, if the type is in a non-generic address space.
+	* emit-rtl.c (get_mem_attrs): Add address space parameter.
+	(set_mem_attributes_minus_bitpos, set_mem_attrs_from_reg,
+	set_mem_alias_set, set_mem_align, set_mem_expr, set_mem_offset,
+	set_mem_size, change_address, adjust_address_1, offset_address,
+	widen_memory_access): Update all callers.
+	(set_mem_addr_space): New function.
+	* emit-rtl.h (set_mem_addr_space): Declare.
+	* explow.c (memory_address): Only convert memory addresses to
+	Pmode if they are not valid pointer modes.
+	* expr.c (expand_expr_addr_expr): Do not assume the target mode is
+	Pmode.
+	(expand_expr_real_1): Handle casts of pointers to/from non-generic
+	address spaces.
+	* fold-const.c (fit_double_type): Do not assume the type precision
+	of a pointer is POINTER_SIZE.
+	(fold_convert_const): Return NULL_TREE for non-generic pointers.
+	* output.h (default_addr_space_pointer_mode): Declare.
+	* print-rtl.c (print_rtx): Output the address space number, if
+	non-zero.
+	* rtl.h (struct mem_attrs): Add addrspace field.
+	(MEM_ADDR_SPACE): New macro.
+	* target-def.h (TARGET_ADDR_SPACE_POINTER_MODE): New target hook.
+	(TARGET_ADDR_SPACE_NAME): Likewise.
+	(TARGET_ADDR_SPACE_NUMBER): Likewise.
+	(TARGET_ADDR_SPACE_CONVERSION_RTL): Likewise.
+	(TARGET_VALID_ADDR_SPACE): Likewise.
+	(TARGET_INITIALIZER): Incorporate the hooks above.
+	* target.h (struct gcc_target): Add addr_space_pointer_mode,
+	addr_space_name, addr_space_number, addr_space_conversion_rtl,
+	valid_addr_space callbacks.
+	* targhooks.h (default_addr_space_name): Declare.
+	(default_addr_space_number): Likewise.
+	(default_addr_space_conversion_rtl): Likewise.
+	* targhooks.c (default_addr_space_name): New.
+	(default_addr_space_conversion_rtl): Likewise.
+	(default_addr_space_number): Likewise.
+	* tree-pretty-print.c: Include target.h and target-def.h.
+	(dump_generic_node): Output address space information.
+	* tree-ssa-loop-ivopts.c (generic_type_for): If the pointer
+	belongs to another address space, include that qualification in
+	the type for the pointer returned.
+	* tree-ssa.c (useless_type_conversion_p_1): Casts between pointers
+	in different address spaces are never useless.
+	(useless_type_conversion_p): Casts between two generic void
+	pointers are useless.
+	* tree.c (integer_pow2p): Handle non-generic pointer sizes.
+	(tree_log2): Likewise.
+	(tree_floor_log2): Likewise.
+	(set_type_quals): Set TYPE_ADDR_SPACE.
+	(build_pointer_type): Do not assume pointers are ptr_mode.
+	* tree.h (OTHER_ADDR_SPACE_POINTER): New macro.
+	(GENERIC_ADDR_SPACE_POINTER): Likewise.
+	(TYPE_ADDR_SPACE): Likewise.
+	(ENCODE_QUAL_ADDR_SPACE): Likewise.
+	(DECODE_QUAL_ADDR_SPACE): Likewise.
+	(TYPE_QUALS): Encode the address space in the qualifiers.
+	(struct tree_type): Add address_space field.
+	* varasm.c (make_decl_rtl): Use the address space pointer mode,
+	not necessarily Pmode.
+	(default_addr_space_pointer_mode): New function.
+	* doc/extend.texi (Named Address Spaces): New node.
+	* doc/rtl.texi (Special Accessors): Document MEM_ADDR_SPACE.
+	* doc/tm.texi (Misc): Document these new target hooks.
+
+	* config.gcc (spu-*-elf*): Add spu_cache.h to extra_headers.
+	* config/spu/spu-c.c (spu_cpu_cpp_builtins): Define __EA32__ or
+	__EA64__, depending on the ea pointer size.  *
+	* config/spu/spu-elf.h (DRIVER_SELF_SPECS): Link the right
+	gcc_cache library depending on the -mcache-size and
+	-matomic-updates option given.
+	(LIB_SPEC): Link gcc_cachemgr library.
+	* config/spu/spu.c (struct spu_address_space): New.
+	(spu_address_spaces): New table.
+	(TARGET_ADDR_SPACE_POINTER_MODE): Define.
+	(TARGET_ADDR_SPACE_NUMBER): Likewise.
+	(TARGET_ADDR_SPACE_CONVERSION_RTL): Likewise.
+	(TARGET_VALID_POINTER_MODE): Likewise.
+	(TARGET_VALID_ADDR_SPACE): Likewise.
+	(TARGET_ASM_UNALIGNED_DI_OP): Remove.
+	(TARGET_ASM_ALIGNED_DI_OP): Define instead.
+	(ea_symbol_ref): New.
+	(spu_legitimate_constant_p): Reject __ea qualified references.
+	(spu_legitimate_address): Keep __ea references until reload.
+	(EAmode): Define.
+	(cache_fetch, cache_fetch_dirty, ea_alias_set): New variables.
+	(ea_load_store): New function.
+	(ea_load_store_inline): Likewise.
+	(expand_ea_mem): Likewise.
+	(spu_expand_mov): Handle __ea memory operands.
+	(spu_ea_pointer_mode): New function.
+	(spu_valid_pointer_mode): Likewise.
+	(spu_addr_space_name): Likewise.
+	(spu_addr_space_conversion_rtl): Likewise.
+	(spu_valid_addr_space): Likewise.
+	(spu_addr_space_number): Likewise.
+	* config/spu/spu.h (ASM_OUTPUT_SYMBOL_REF): New macro.
+	* config/spu/spu.md (to_ea): New expander.
+	(from_ea): Likewise.
+	* config/spu/spu.opt (mea32, mea64): New options.
+	* config/spu/spu_mfcio.h: New typedef.
+	* config/spu/t-spu-elf (MULTILIB_OPTIONS): Add mea64.
+	(EXTRA_MULTILIB_PARTS): Add cache libraries.
+	(cachemgr.o, %/cachemgr.o): New targets.
+	(cachemgr_nonatomic.o, %/cachemgr_nonatomic.o): Likewise.
+	(libgcc_%.a, %/libgcc_%.a): Likewise.
+	(cache8k.o, cache16k.o, cache32k.o, etc): Likewise.
+	(%/cache8k.o, %/cache16k.o, %/cache32k.o, etc): Likewise.
+	* config/spu/cache.S: New file.
+	* config/spu/cachemgr.c: Likewise.
+	* config/spu/spu_cache.h: Likewise.
+	* doc/invoke.texi (SPU Options): Document -mea32, -mea64,
+	-mcache-size and -matomic-updates options.
+
+2008-07-24  Ben Elliston  <bje@au.ibm.com>
+
+	* gcc/config/spu/spu-c.c: Move GTY markers to match the precedent.
+
+2008-07-23  Ben Elliston  <bje@au.ibm.com>
+
+	Backport from mainline:
+
+	2008-07-23  Ben Elliston  <bje@au.ibm.com>
+
+	* gcc/config/spu/spu-c.c (__vector_keyword): New variable.
+	(vector_keyword): Likewise.
+	(spu_categorize_keyword): New function.
+	(spu_macro_to_expand): Likewise.
+	(spu_cpu_cpp_builtins): Enable context-sensitive macros if not
+	compiling an ISO C dialect.
+
+	2008-07-23  Ben Elliston  <bje@au.ibm.com>
+
+	* gcc/testsuite/gcc.target/spu/vector.c: New test.
+	* gcc/testsuite/gcc.target/spu/vector-ansi.c: Likewise.
+
+2008-07-21  Ben Elliston  <bje@au.ibm.com>
+
+	Backport from mainline:
+
+	2008-07-14  Ben Elliston  <bje@au.ibm.com>
+
+	* gcc/c-common.h (C_CPP_HASHNODE): New macro.
+	* gcc/coretypes.h (struct cpp_token): Forward declare.
+	* gcc/doc/extend.texi (PowerPC AltiVec Built-in Functions): Document
+	the context-sensitive keyword method.
+	* gcc/config/rs6000/rs6000-c.c (__vector_keyword, vector_keyword,
+	__pixel_keyword, pixel_keyword, __bool_keyword, bool_keyword,
+	expand_bool_pixel): New.
+	(altivec_categorize_keyword): New function.
+	(init_vector_keywords): New function.
+	(rs6000_macro_to_expand): Likewise.
+	(rs6000_cpu_cpp_builtins): Enable context-sensitive macros if not
+	compiling an ISO C dialect.
+
+	2008-07-14  Ben Elliston  <bje@au.ibm.com>
+
+	* gcc/testsuite/gcc.target/powerpc/altivec-macros.c: New test.
+	* gcc/testsuite/gcc.target/powerpc/altviec-26.c: Likewise.
+	* gcc/testsuite/gcc.dg/vmx/1b-06.c: Remove bool variable.
+	* gcc/testsuite/gcc.dg/vmx/1b-07.c: Likewise.
+	* gcc/testsuite/gcc.dg/vmx/1b-06-ansi.c: New test for the pre-define
+	method.
+	* gcc/testsuite/gcc.dg/vmx/1b-07-ansi.c: Likewise.
+
+	2008-07-14  Ben Elliston  <bje@au.ibm.com>
+
+	* libcpp/include/cpplib.h (NODE_CONDITIONAL): New.
+	(struct cpp_callbacks): New macro_to_expand field.
+	(struct cpp_hashnode): Adjust size of flags and type fields.
+	(cpp_peek_token): Prototype.
+	* libcpp/lex.c (cpp_peek_token): New function.
+	(_cpp_temp_token): Protect pre-existing lookaheads.
+	* libcpp/macro.c (cpp_get_token): Expand any conditional macros.
+	(_cpp_backup_tokens_direct): New.
+	(_cpp_backup_tokens): Call _cpp_backup_tokens_direct.
+	(warn_of_redefinition): Silently allow redefined conditional
+	macros.
+	(_cpp_create_definition): Remove the conditional flag when a user
+	defines one of the conditional macros.
+	* libcpp/internal.h (_cpp_backup_tokens_direct): New prototype.
+
+2008-05-20  Ulrich Weigand  <Ulrich.Weigand@de.ibm.com>
+
+	* Created "cell-4_3-branch".
+
Index: libcpp/macro.c
===================================================================
--- libcpp/macro.c	(.../gcc-4_3-branch)	(revision 156795)
+++ libcpp/macro.c	(.../cell-4_3-branch)	(revision 156810)
@@ -1224,16 +1224,21 @@
 
       if (!(node->flags & NODE_DISABLED))
 	{
-	  int ret;
+	  int ret = 0;
 	  /* If not in a macro context, and we're going to start an
 	     expansion, record the location.  */
 	  if (can_set && !context->macro)
 	    pfile->invocation_location = result->src_loc;
 	  if (pfile->state.prevent_expansion)
 	    break;
-	  ret = enter_macro_context (pfile, node, result);
-	  if (ret)
-	    {
+
+	  /* Conditional macros require that a predicate be evaluated
+	     first.  */
+	  if (((!(node->flags & NODE_CONDITIONAL))
+	       || (pfile->cb.macro_to_expand
+		   && (node = pfile->cb.macro_to_expand (pfile, result))))
+	      && (ret = enter_macro_context (pfile, node, result)))
+ 	    {
 	      if (pfile->state.in_directive || ret == 2)
 		continue;
 	      return padding_token (pfile, result);
@@ -1311,26 +1316,31 @@
   pfile->state.prevent_expansion--;
 }
 
+/* Step back one or more tokens obtained from the lexer.  */
+void
+_cpp_backup_tokens_direct (cpp_reader *pfile, unsigned int count)
+{
+  pfile->lookaheads += count;
+  while (count--)
+    {
+      pfile->cur_token--;
+      if (pfile->cur_token == pfile->cur_run->base
+          /* Possible with -fpreprocessed and no leading #line.  */
+          && pfile->cur_run->prev != NULL)
+        {
+          pfile->cur_run = pfile->cur_run->prev;
+          pfile->cur_token = pfile->cur_run->limit;
+        }
+    }
+}
+
 /* Step back one (or more) tokens.  Can only step back more than 1 if
    they are from the lexer, and not from macro expansion.  */
 void
 _cpp_backup_tokens (cpp_reader *pfile, unsigned int count)
 {
   if (pfile->context->prev == NULL)
-    {
-      pfile->lookaheads += count;
-      while (count--)
-	{
-	  pfile->cur_token--;
-	  if (pfile->cur_token == pfile->cur_run->base
-	      /* Possible with -fpreprocessed and no leading #line.  */
-	      && pfile->cur_run->prev != NULL)
-	    {
-	      pfile->cur_run = pfile->cur_run->prev;
-	      pfile->cur_token = pfile->cur_run->limit;
-	    }
-	}
-    }
+    _cpp_backup_tokens_direct (pfile, count);
   else
     {
       if (count != 1)
@@ -1356,6 +1366,11 @@
   if (node->flags & NODE_WARN)
     return true;
 
+  /* Redefinitions of conditional (context-sensitive) macros, on
+     the other hand, must be allowed silently.  */
+  if (node->flags & NODE_CONDITIONAL)
+    return false;
+
   /* Redefinition of a macro is allowed if and only if the old and new
      definitions are the same.  (6.10.3 paragraph 2).  */
   macro1 = node->value.macro;
@@ -1788,6 +1803,10 @@
       && ustrcmp (NODE_NAME (node), (const uchar *) "__STDC_CONSTANT_MACROS"))
     node->flags |= NODE_WARN;
 
+  /* If user defines one of the conditional macros, remove the
+     conditional flag */
+  node->flags &= ~NODE_CONDITIONAL;
+
   return ok;
 }
 
Index: libcpp/include/cpplib.h
===================================================================
--- libcpp/include/cpplib.h	(.../gcc-4_3-branch)	(revision 156795)
+++ libcpp/include/cpplib.h	(.../cell-4_3-branch)	(revision 156810)
@@ -476,6 +476,10 @@
   void (*read_pch) (cpp_reader *, const char *, int, const char *);
   missing_header_cb missing_header;
 
+  /* Context-sensitive macro support.  Returns macro (if any) that should
+     be expanded.  */
+  cpp_hashnode * (*macro_to_expand) (cpp_reader *, const cpp_token *);
+
   /* Called to emit a diagnostic if client_diagnostic option is true.
      This callback receives the translated message.  */
   void (*error) (cpp_reader *, int, const char *, va_list *)
@@ -537,6 +541,7 @@
 #define NODE_WARN	(1 << 4)	/* Warn if redefined or undefined.  */
 #define NODE_DISABLED	(1 << 5)	/* A disabled macro.  */
 #define NODE_MACRO_ARG	(1 << 6)	/* Used during #define processing.  */
+#define NODE_CONDITIONAL (1 << 7)	/* Conditional macro */
 
 /* Different flavors of hash node.  */
 enum node_type
@@ -696,6 +701,7 @@
 extern const unsigned char *cpp_macro_definition (cpp_reader *,
 						  const cpp_hashnode *);
 extern void _cpp_backup_tokens (cpp_reader *, unsigned int);
+extern const cpp_token *cpp_peek_token (cpp_reader *, int);
 
 /* Evaluate a CPP_CHAR or CPP_WCHAR token.  */
 extern cppchar_t cpp_interpret_charconst (cpp_reader *, const cpp_token *,
Index: libcpp/internal.h
===================================================================
--- libcpp/internal.h	(.../gcc-4_3-branch)	(revision 156795)
+++ libcpp/internal.h	(.../cell-4_3-branch)	(revision 156810)
@@ -527,6 +527,7 @@
 extern int _cpp_warn_if_unused_macro (cpp_reader *, cpp_hashnode *, void *);
 extern void _cpp_push_token_context (cpp_reader *, cpp_hashnode *,
 				     const cpp_token *, unsigned int);
+extern void _cpp_backup_tokens_direct (cpp_reader *, unsigned int);
 
 /* In identifiers.c */
 extern void _cpp_init_hashtable (cpp_reader *, hash_table *);
Index: libcpp/lex.c
===================================================================
--- libcpp/lex.c	(.../gcc-4_3-branch)	(revision 156795)
+++ libcpp/lex.c	(.../cell-4_3-branch)	(revision 156810)
@@ -730,6 +730,49 @@
   return run->next;
 }
 
+/* Look ahead in the input stream.  */
+const cpp_token *
+cpp_peek_token (cpp_reader *pfile, int index)
+{
+  cpp_context *context = pfile->context;
+  const cpp_token *peektok;
+  int count;
+
+  /* First, scan through any pending cpp_context objects.  */
+  while (context->prev)
+    {
+      ptrdiff_t sz = (context->direct_p
+                      ? LAST (context).token - FIRST (context).token
+                      : LAST (context).ptoken - FIRST (context).ptoken);
+
+      if (index < (int) sz)
+        return (context->direct_p
+                ? FIRST (context).token + index
+                : *(FIRST (context).ptoken + index));
+
+      index -= (int) sz;
+      context = context->prev;
+    }
+
+  /* We will have to read some new tokens after all (and do so
+     without invalidating preceding tokens).  */
+  count = index;
+  pfile->keep_tokens++;
+
+  do
+    {
+      peektok = _cpp_lex_token (pfile);
+      if (peektok->type == CPP_EOF)
+	return peektok;
+    }
+  while (index--);
+
+  _cpp_backup_tokens_direct (pfile, count + 1);
+  pfile->keep_tokens--;
+
+  return peektok;
+}
+
 /* Allocate a single token that is invalidated at the same time as the
    rest of the tokens on the line.  Has its line and col set to the
    same as the last lexed token, so that diagnostics appear in the
@@ -738,10 +781,31 @@
 _cpp_temp_token (cpp_reader *pfile)
 {
   cpp_token *old, *result;
+  ptrdiff_t sz = pfile->cur_run->limit - pfile->cur_token;
+  ptrdiff_t la = (ptrdiff_t) pfile->lookaheads;
 
   old = pfile->cur_token - 1;
-  if (pfile->cur_token == pfile->cur_run->limit)
+  /* Any pre-existing lookaheads must not be clobbered.  */
+  if (la)
     {
+      if (sz <= la)
+        {
+          tokenrun *next = next_tokenrun (pfile->cur_run);
+
+          if (sz < la)
+            memmove (next->base + 1, next->base,
+                     (la - sz) * sizeof (cpp_token));
+
+          next->base[0] = pfile->cur_run->limit[-1];
+        }
+
+      if (sz > 1)
+        memmove (pfile->cur_token + 1, pfile->cur_token,
+                 MIN (la, sz - 1) * sizeof (cpp_token));
+    }
+
+  if (!sz && pfile->cur_token == pfile->cur_run->limit)
+    {
       pfile->cur_run = next_tokenrun (pfile->cur_run);
       pfile->cur_token = pfile->cur_run->base;
     }

